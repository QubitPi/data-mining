\lecture{deep}{deep}

\date{Chapter 26: Deep Learning}

\begin{frame}
\titlepage
\end{frame}

%\chapter{Deep Learning}
%\label{ch:reg:deep}
%
\begin{frame}{Overview}

Recurrent Neural Networks (RNNs) include feedback from one layer to another and
they can typically be trained by unfolding (or unrolling) the recurrent
connections, resulting in deep networks whose parameters can be learned
via the backpropagation algorithm. 

\medskip

Since RNNs are prone to the vanishing and exploding gradients
problem, we next consider {\em gated RNNs} that introduce a new type of
layer that endows the network with the ability to selectively read,
store, and write the hidden state via an internal {\em memory layer}.
Gated RNNs are highly effective for prediction tasks on sequence inputs.

\medskip

%Finally, we consider 
Convolutional Neural Networks (CNNs) that are
deep MLPs that exploit spatial or temporal relationships between
different elements of each layer to construct a hierarchy of {\em
features} that can be used for regression or classification tasks.
Unlike regular MLPs whose layers are fully connected, CNNs have layers
that are localized and sparse. In particular, CNNs are highly effective
for image inputs.
\end{frame}
%
%% Deep MLPs can be trained via the 
%% backpropagation algorithm which computes the net gradients at the output
%% layer and then each of the previous hidden layers in a backward
%% direction. These net gradients are used to update the weight matrices
%% and the biases.
%
%
%\section{Recurrent Neural Networks}
\begin{frame}{Recurrent Neural Networks}
%\label{sec:reg:deep:rnn}
%
%\index{recurrent neural networks}
%\index{deep learning!recurrent neural networks}
%\index{deep learning!RNN}
%\index{RNN|see{recurrent neural networks}}
%
Multilayer perceptrons are feed-forward networks in which the
information flows in only one direction, namely from the input layer to
the output layer via the hidden layers. 

	\medskip

In contrast, recurrent neural
networks (RNNs) are dynamically driven (e.g., temporal), with a {\em
feedback} loop between two (or more) layers, which makes them %such networks
ideal for learning from  sequence data. %For example,
%\cref{fig:reg:deep:rnn} shows a simple RNN where there is a feedback
%loop from the hidden layer $\bh_{t}$ to itself via a temporal delay of
%one time unit, denoted by the $-1$ on the loop.
%
%
%\begin{figure}[!t]
%\vspace{0.2in}
%\caption{Recurrent neural network. Recurrent connection shown in gray.}
%\label{fig:reg:deep:rnn}
%\end{figure}
%
%Let $\cX = \tup{\bx_1, \bx_2, \cdots, \bx_\tau}$ denote a sequence of vectors, where
%$\bx_t \in \setR^{d}$ is a $d$-dimensional vector ($t=1,2,\cdots,\tau$).
%Thus, $\cX$ is an input sequence of length $\tau$, with $\bx_t$ denoting
%the input at time step $t$.  Let $\cY = \tup{\by_1, \by_2, \cdots,
%\by_\tau}$
%denote a sequence of vectors, with $\by_t \in \setR^{p}$ a $p$-dimensional
%vector.  Here, $\cY$ is the desired target or response sequence, with $\by_t$
%denoting the response vector at time $t$.  Finally, let $\cO = \tup{\bo_1, \bo_2,
%\cdots, \bo_\tau}$ denote the predicted or output sequence from
%the RNN. Here $\bo_t \in \setR^p$ is also a $p$-dimensional vector to match
%the corresponding true response $\by_t$.  

\medskip

The task of an RNN is to learn a
function that predicts the target sequence $\cY$ given the input sequence
$\cX$. That is, the predicted output $\bo_t$ on input $\bx_t$ should be
similar or close to the target response $\by_t$, for each time point $t$.

\end{frame}

\begin{frame}{Recurrent Neural Network}
To learn dependencies between elements of the input sequence, an RNN maintains
a sequence of $m$-dimensional hidden state vectors $\bh_t \in \setR^m$, where
$\bh_t$ captures the essential features of the input sequences up to time
$t$.%, as illustrated in \cref{fig:reg:deep:rnn}.  
The hidden vector $\bh_t$
at time $t$ depends on the input vector $\bx_t$ at time $t$ and the previous
hidden state vector $\bh_{t-1}$ from time $t-1$, and it is computed as follows:
\begin{align*}
   \tcbhighmath{
   \bh_t = f^h(\bW_{\!i}^T \bx_t + \bW_{\!h}^T
\bh_{t-1} + \bb_{h})}
   %\label{eq:reg:deep:hidden_t}
\end{align*}
Here, $f^h$ is the hidden state activation function, typically tanh or ReLU. 
%Also,
%we need an initial hidden state vector $\bh_0$ that serves as the prior
%state to compute $\bh_1$. This is usually set to the zero vector, or
%seeded from a prior RNN prediction step.  The matrix $\bW_{\!i} \in \setR^{d
%\times m}$ specifies the weights between the input vectors and the hidden
%state vectors. The matrix $\bW_{\!h} \in \setR^{m \times m}$ specifies the
%weight matrix between the hidden state vectors at time $t-1$ and $t$, with
%$\bb_{h} \in \setR^m$ specifying the bias terms associated with the
%hidden states. Note that we need only one bias vector $\bb_{h}$
% associated with the
% hidden state neurons; we do not need a separate bias vector between the
% input and hidden neurons. 
% 
%Given the hidden state vector at time $t$, the output vector $\bo_t$ at
%time $t$ is
%computed as follows:
%\begin{align}
%    \tcbhighmath{
%    \bo_t = f^o(\bW_{\!o}^T \bh_t + \bb_{o})}
%    \label{eq:reg:deep:output_t}
%\end{align}
%Here, $\bW_{\!o} \in \setR^{m \times p}$ specifies the weights between the
%hidden state and output vectors, with bias vector $\bb_{o}$. The
%output activation function $f^o$ typically uses linear or identity
%activation, or a softmax activation for one-hot encoded categorical
%output values. 

\psset{tnpos=l,tnsep=2pt,colsep=2,rowsep=1.5,mcol=c, arrowscale=1.5, arrows=->}
\vspace{0.7in}
\centerline{
\scalebox{0.8}{
\psmatrix
\TR[name=x]{\myfbox{$\bx_t$}} &
\TR[name=h]{\myfbox{$\bh_t$}} &
\TR[name=o]{\myfbox{$\bo_t$}}
%bias to next layer
\psset{linewidth=2pt}
\ncline{x}{h}\nbput[npos=0.5]{$\bW_{\!i}$}
\ncline{h}{o}\nbput[npos=0.5]{$\bW_{\!o}, \bb_{o}$}
% \ncloop[loopsize=1.5,angleA=15,angleB=160,arm=0.5,linearc=0.3]{->}{h}{h}{0.3in}\nbput[npos=0.5]{$\bW_h, \bb_h$}
\nccircle[linecolor=gray]{->}{h}{0.3in}\nbput[npos=0.5]{$\bW_{\!h},\bb_{h}$}%
\ncput*{\scalebox{0.7}{$-1$}}
\endpsmatrix
}}
\end{frame}
%
%\index{recurrent neural networks!parameter sharing}
%\index{recurrent neural networks!weight tying}
%\index{parameter sharing!recurrent neural networks}
%\index{weight tying|see{parameter sharing}}
\begin{frame}{Recurrent Neural Networks}
It is important to note that all the weight matrices and bias vectors
are {\em independent} of the time $t$. For example, for the hidden
layer,
the same weight matrix  $\bW_{\!h}$ and bias vector $\bb_{h}$ is used and updated while training
the model,
over all time steps $t$. 


\medskip

This is an example of {\em parameter
sharing} or {\em weight tying} 
between different layers or components of a neural network. 
Likewise, the input weight matrix $\bW_{\!i}$, the output weight
matrix $\bW_{\!o}$ and the bias vector $\bb_{o}$ are all shared across
time.


\medskip

This greatly reduces the number of parameters that
need to be learned by the RNN, but it also relies on the assumption that
all relevant sequential features can be captured by the shared
parameters.
\end{frame}
%
%\cref{fig:reg:deep:rnn} shows an illustration of an RNN,
%with a recurrent hidden layer $\bh_{t}$, denoted by the feed-back loop with
%a time delay of $-1$ noted on the loop. The figure also shows the shared
%parameters between the layers.  \cref{fig:reg:deep:rnn_vert} shows the
%same RNN in vertical format, where the layers are stacked vertically, and
%\cref{fig:reg:deep:rnn_unfolded_t} shows the RNN with the layers unfolded in
%time; it shows the input ($\bx_t$), hidden ($\bh_t$), and output
%($\bo_t$) layers at each time step $t$. We can observe that the feed-back loop
%has been unfolded  for $\tau$ time steps, starting with $t=1$ and ending at
%$t=\tau$. The time step $t=0$ is used to denote the previous or initial hidden
%state $\bh_0$. We can also explicitly observe the parameter sharing across
%time, since all weight matrices and bias vectors are independent of $t$.
%\cref{fig:reg:deep:rnn_unfolded_t} makes it clear that an RNN is a deep
%neural network with $\tau$ layers, where $\tau$ is the maximum 
%input sequence length.
%
%\begin{figure}[!t]
%    \large
% \captionsetup[subfloat]{captionskip=12pt}
%    \hspace{0.25in}
\begin{frame}{RNN unfolded in time.}
We can observe that the feed-back loop has been unfolded  for $\tau$ time steps.
We can also explicitly observe the parameter sharing across
time, since all weight matrices and bias vectors are independent of $t$.

\medskip

    \psset{tnpos=l,tnsep=2pt,colsep=1.5,rowsep=1.5,mcol=c,
    arrowscale=1.5, arrows=->}
\centerline{
\hspace{-0.5in}
%    \subfloat[RNN]{%
%        \label{fig:reg:deep:rnn_vert}
\scalebox{0.6}{%
\begin{minipage}{0.6in}
\psmatrix
\\[-3em]
\TR[name=o]{\myfbox{$\bo_t$}}\\
\TR[name=h]{\myfbox{$\bh_t$}}\\
\TR[name=x]{\myfbox{$\bx_t$}}
%%bias to next layer
\psset{linewidth=2pt}
\ncline{x}{h}\nbput[npos=0.2]{$\bW_{\!i}$}
\ncline{h}{o}\nbput[npos=0.7]{$\bW_{\!o}, \bb_{o}$}
%% \nccircle[angleA=90]{->}{h}{0.3in}\nbput[npos=0.5]{$\bW_{\!h},\bb_{h}$}%
\ncloop[linecolor=gray,loopsize=1.2,angleA=0,angleB=180,arm=0.6,linearc=0.2]{->}{h}{h}%
\nbput[npos=3.5]{$\bW_h, \bb_h$}%
\ncput*[npos=1.5]{\scalebox{0.7}{$-1$}}
\endpsmatrix
\end{minipage}
}%}
\hspace{0.3in}
%\subfloat[RNN: unfolded in time]{%
%    \label{fig:reg:deep:rnn_unfolded_t}
%\hspace*{-1in}
\scalebox{0.6}{%
\begin{minipage}{4.9in}
\psmatrix
[mnode=none]{$t=0$} 
    & [mnode=none]{$t=1$} 
    & [mnode=none]{$t=2$} 
    & [mnode=none]{$\ldots$} 
    & [mnode=none]{$t=\tau-1$} 
    & [mnode=none]{$t=\tau$}\\[-3em]
&   \TR[name=o1]{\myfbox{$\bo_1$}}
    & \TR[name=o2]{\myfbox{$\bo_2$}}
    & \TR[name=oi]{\myfboxB[linecolor=gray]{$\cdots\!$}}
    & \TR[name=ot1]{\myfbox{$\bo_{\tau-1}$}}
    & \TR[name=ot]{\myfbox{$\bo_{\tau}$}}\\
\TR[name=h0]{\myfbox{$\bh_0$}}
    & \TR[name=h1]{\myfbox{$\bh_1$}}
    & \TR[name=h2]{\myfbox{$\bh_2$}}
    & \TR[name=hi]{\myfboxB[linecolor=gray]{$\cdots\!$}}
    %\Tcircle[name=hi,linecolor=gray]{$\cdots$}
    & \TR[name=ht1]{\myfbox{$\bh_{\tau-1}$}}
    & \TR[name=ht]{\myfbox{$\bh_{\tau}$}}\\
&   \TR[name=x1]{\myfbox{$\bx_1$}}
    & \TR[name=x2]{\myfbox{$\bx_2$}}
    & \TR[name=xi]{\myfboxB[linecolor=gray]{$\cdots\!$}}
    & \TR[name=xt1]{\myfbox{$\bx_{\tau-1}$}}
    & \TR[name=xt]{\myfbox{$\bx_{\tau}$}}
%bias to next layer
    \psset{linewidth=2pt}
    \ncline{h0}{h1}\nbput[npos=0.5]{$\bW_{\!h}, \bb_{h}$}
    \ncline{h1}{h2}\nbput[npos=0.5]{$\bW_{\!h}, \bb_{h}$}
    \ncline{ht1}{ht}\nbput[npos=0.5]{$\bW_{\!h}, \bb_{h}$}
    \ncline{h1}{o1}\nbput[npos=0.5]{$\bW_{\!o}, \bb_{o}$}
    \ncline{h2}{o2}\nbput[npos=0.5]{$\bW_{\!o}, \bb_{o}$}
    \ncline{ht1}{ot1}\nbput[npos=0.5]{$\bW_{\!o}, \bb_{o}$}
    \ncline{ht}{ot}\nbput[npos=0.5]{$\bW_{\!o}, \bb_{o}$}
    \ncline{x1}{h1}\nbput[npos=0.5]{$\bW_{\!i}$}
    \ncline{x2}{h2}\nbput[npos=0.5]{$\bW_{\!i}$}
    \ncline{xt1}{ht1}\nbput[npos=0.5]{$\bW_{\!i}$}
    \ncline{xt}{ht}\nbput[npos=0.5]{$\bW_{\!i}$}
\psset{linewidth=1pt, linestyle=dashed}
\ncline{h2}{hi} 
\ncline{hi}{ht1}
\ncline{xi}{hi}
\ncline{hi}{oi}
\endpsmatrix
\end{minipage}
}%}
}
\end{frame}
%\vspace{0.1in}
%\caption{RNN unfolded in time.}
%\label{fig:reg:deep:rnn_unfolded}
%\end{figure}
%
%%\subsection{Training an RNN}
%On training input of length $\tau$ we first unfold the RNN for $\tau$ steps,
%following which the parameters can be learned via the standard feed-forward
%and backpropagation steps, keeping in mind the connections between the layers.
%
%\subsection{Feed-forward in Time}
\begin{frame}{Feed-forward in Time}
%\index{recurrent neural networks!feed-forward phase}
%\index{recurrent neural networks!feed-forward in time}
%
The feed-forward process starts at time $t=0$, taking as input the initial
hidden state vector $\bh_0$, which is usually set to $\bzero$ or it can be
user-specified, say from a previous prediction step. 

	\medskip

Given the current set
of parameters, we predict the output $\bo_t$ at each time step $t=1,2,\cdots,
\tau$.% via \cref{eq:reg:deep:hidden_t} and \cref{eq:reg:deep:output_t}:
%%\begin{samepage}
\begin{align*}
    \bo_t & = f^o \bigl( \bW_{\!o}^T \bh_t + \bb_{o} \bigr)\\
    & = f^o \Bigl( \bW_{\!o}^T 
        \underbrace{f^h \bigl(\bW_{\!i}^T \bx_t + \bW_{\!h}^T
        \bh_{t-1} + \bb_{h}\bigr)}_{\bh_t}
    + \bb_{o} \Bigr)\\
    & = \qquad \qquad \vdots\\
    & = f^o \Bigl( \bW_{\!o}^T 
        f^h \Bigl(\bW_{\!i}^T \bx_t + \bW_{\!h}^T
         f^h\bigl( \cdots 
           \underbrace{f^h\bigl(\bW_{\!i}^T \bx_{1} + \bW_{\!h}^T
               \bh_{0} + \bb_{h}}_{\bh_1} 
            \bigr) 
        +\cdots \bigr)
    + \bb_{h} \Bigr)
    + \bb_{o} \Bigr)
\end{align*}
%%\end{samepage}
We can observe that the RNN implicitly makes a prediction for every prefix of
the input sequence, since $\bo_t$ depends on all the previous input vectors
$\bx_1, \bx_2, \cdots, \bx_t$, but not on any future inputs $\bx_{t+1},
\cdots, \bx_\tau$.
\end{frame}

\begin{frame}{RNN: Feed-forward step}
\centerline{
    \psset{tnpos=l,tnsep=2pt,colsep=1.5,rowsep=1.75,mcol=c,
    arrowscale=2, arrows=->}
%    \subfloat[Feed-forward step]{%
%\label{fig:reg:deep:rnn_unfolded_l_ff}
    \scalebox{0.65}{%
\psmatrix
[mnode=none,name=l0]{$l=0$} 
    & [mnode=none]{$l=1$} 
    & [mnode=none]{$l=2$} 
    & [mnode=none]{$l=3$} 
    & [mnode=none]{$\ldots$} 
    & [mnode=none]{$l=\tau$} 
    & [mnode=none]{$l=\tau+1$}\\[-4em]
\Tp[name=om1]
    & \Tp[name=o0] 
    &  \TR[name=o1]{\myfbox{$\large\bo_1$}}
    & \TR[name=o2]{\myfbox{$\large\bo_2$}}
& \TR[name=oi]{\myfboxB[linecolor=gray]{$\cdots\!$}}
    & \TR[name=ot1]{\myfbox{$\large\bo_{\tau-1}$}}
    & \TR[name=ot]{\myfbox{$\large\bo_{\tau}$}}\\
\TR[name=h0]{\myfbox{$\large\bh_0$}}
    & \TR[name=h1]{\myfbox{$\large\bh_1$}}
    & \TR[name=h2]{\myfbox{$\large\bh_2$}}
    & \TR[name=hi]{\myfboxB[linecolor=gray]{$\cdots\!$}}
    %\Tcircle[name=hi,linecolor=gray]{$\cdots$}
    & \TR[name=ht1]{\myfbox{$\large\bh_{\tau-1}$}}
    & \TR[name=ht]{\myfbox{$\large\bh_{\tau}$}}\\
\TR[name=x1]{\myfbox{$\large\bx_1$}}
    & \TR[name=x2]{\myfbox{$\large\bx_2$}}
    & \TR[name=xi]{\myfboxB[linecolor=gray]{$\cdots\!$}}
    & \TR[name=xt1]{\myfbox{$\large\bx_{\tau-1}$}}
    & \TR[name=xt]{\myfbox{$\large\bx_{\tau}$}}
    & \Tp[name=xp1] 
    & \Tp[name=xp2]
%bias to next layer
    \psset{linewidth=2pt, nrot=:U}
    \ncline{h0}{h1}\nbput[npos=0.5]{$\bW_{\!h}, \bb_{h}$}
    \ncline{h1}{h2}\nbput[npos=0.5]{$\bW_{\!h}, \bb_{h}$}
    \ncline{ht1}{ht}\nbput[npos=0.5]{$\bW_{\!h}, \bb_{h}$}
    \ncline{h1}{o1}\nbput[npos=0.5]{$\bW_{\!o}, \bb_{o}$}
    \ncline{h2}{o2}\nbput[npos=0.5]{$\bW_{\!o}, \bb_{o}$}
    \ncline{ht1}{ot1}\nbput[npos=0.5]{$\bW_{\!o}, \bb_{o}$}
    \ncline{ht}{ot}\nbput[npos=0.5]{$\bW_{\!o}, \bb_{o}$}
    \ncline{x1}{h1}\nbput[npos=0.5]{$\bW_{\!i}$}
    \ncline{x2}{h2}\nbput[npos=0.5]{$\bW_{\!i}$}
    \ncline{xt1}{ht1}\nbput[npos=0.5]{$\bW_{\!i}$}
    \ncline{xt}{ht}\nbput[npos=0.5]{$\bW_{\!i}$}
\psset{linewidth=1pt, linestyle=dashed}
\ncline{h2}{hi} 
\ncline{hi}{ht1}
\ncline{xi}{hi}
\ncline{hi}{oi}
% \psset{linestyle=dashed, boxsize=0.8, nodesep=0.2cm}
% \ncbox[]{om1}{x1}
% \ncbox[]{o0}{x2}
% \ncbox[]{o1}{xi}
% \ncbox[]{o2}{xt1}
% \ncbox[]{oi}{xt}
% \ncbox[]{ot1}{xp1}
% \ncbox[]{ot}{xp2}
\endpsmatrix
}}%}
\end{frame}

\begin{frame}{Training an RNN}
%
%The training data for the RNN is given as $\bD = \{\cX_i, \cY_i\}_{i=1}^{n}$,
%comprising $n$ input sequences $\cX_i$ and the corresponding target response
%sequences $\cY_i$, with sequence length $\tau_i$.  Given each pair $(\cX, \cY)
%\in \bD$, with $\cX = \tup{\bx_1, \bx_2, \cdots, \bx_\tau}$ and 
%$\cY = \tup{\by_1, \by_2,
%\cdots, \by_\tau}$, the RNN has to update the model parameters $\bW_{\!i},
% \bW_{\!h}, \bb_{h}, \bW_{\!o}, \bb_{o}$ for the
% input, hidden and output layers, to learn the corresponding output sequence
%$\cO=\tup{\bo_1, \bo_2, \cdots, \bo_\tau}$.  
For training the network, we compute
the error or {\em loss} between the predicted and response vectors over
all time steps. For example, the squared error loss is given as
\begin{align*}
    \cE_{\!\cX} = \sum_{t=1}^\tau \cE_{\bx_t} = \frac{1}{2} \cdot
    \sum^{\tau}_{t=1} \norm{\by_t - \bo_t}^2
\end{align*}
On the other hand, if we use a softmax activation at the output layer,
then we use the cross-entropy loss, given as
\begin{align*}
    \cE_{\!\cX} = \sum_{t=1}^\tau \cE_{\bx_t} =
    - \sum^{\tau}_{t=1} \sum^{p}_{i=1} y_{ti} \cdot \ln(o_{ti})
\end{align*}
where $\by_t = (y_{t1}, y_{t2}, \cdots, y_{tp})^T \in \setR^p$ and
$\bo_t = (o_{t1}, o_{t2}, \cdots, o_{tp})^T \in \setR^p$.
\end{frame}
%
%
%\subsection{Backpropagation in Time}
\begin{frame}{Backpropagation in Time}
%\index{recurrent neural networks!backpropagation phase}
%\index{backpropagation!RNN}
%\index{backpropagation!recurrent neural networks}
%\label{sec:reg:deep:rnn_bpt}
Once the the output sequence $\cO = \tup{\bo_1,
\bo_2, \cdots, \bo_\tau}$ is generated, we can compute the error in the predictions 
using the squared error
(or cross-entropy) loss function, which can in turn be used to compute the
net gradient vectors that are backpropagated from the output layers to the
input layers for each time step.


\medskip

%For the backpropagation step it is easier to view the RNN in terms
%of the distinct layers based on the dependencies, as opposed to
%unfolding in time. %\cref{fig:reg:deep:rnn_unfolded_l_ff} shows the RNN
%unfolded using this layer view as opposed to the time view shown in
%\cref{fig:reg:deep:rnn_unfolded_t}. The first layer is $l=0$ comprising the
%hidden states $\bh_0$ and the input vector $\bx_1$, both of which are required
%to compute $\bh_1$ in layer $l=1$, which also includes the input vector
%$\bx_2$. In turn, both $\bh_1$ and $\bx_2$ are required to compute
%$\bh_2$ in layer $l=2$, and so on for other layers. Note
%also that $\bo_1$ is not output until layer $l=2$ since we can compute the
%output only once $\bh_1$ has been computed in the previous layer. The layer
%view is essentially indexed by the hidden state vector index, except the
%final layer $l=\tau+1$ that outputs $\bo_{\tau}$ that depends on $\bh_\tau$
%from layer $l=\tau$. 

Let $\cE_{\bx_t}$ denote the loss on $\bx_t$ from the input
sequence $\cX = \tup{\bx_1, \bx_2, \cdots, \bx_\tau}$.
%The unfolded feed-forward RNN for $\cX$  has $l=\tau+1$ layers.%, as
%shown in \cref{fig:reg:deep:rnn_unfolded_l_ff}. 


\medskip

Define $\bdelta^o_t$ as the net gradient vector for the output vector
$\bo_t$, i.e., the
derivative of the error function $\cE_{\bx_t}$
with respect to the net value at each neuron in $\bo_t$, given as
\begin{align*}
    \bdelta^o_t = \lB(
        \frac{\partial \cE_{\bx_t}}{\partial \net^{\;o}_{t1}},
        \frac{\partial \cE_{\bx_t}}{\partial \net^{\;o}_{t2}},
                \cdots,
                \frac{\partial \cE_{\bx_t}}{\partial \net^{\;o}_{tp}}
    \rB)^T
\end{align*}
where $\bo_t = (o_{t1}, o_{t2}, \cdots, o_{tp})^T \in \setR^p$ is the
$p$-dimensional output vector at time $t$, and $\net^{\;o}_{ti}$ is
the net value at output neuron $o_{ti}$ at time $t$.
\end{frame}

\begin{frame}{Backpropagation in Time}
Likewise, let $\bdelta^h_t$ denote the net gradient vector for the
hidden state neurons $\bh_t$ at time $t$

\begin{align*}
    \bdelta^h_t = \lB(
        \frac{\partial \cE_{\bx_t}}{\partial \net^{\;h}_{t1}},
        \frac{\partial \cE_{\bx_t}}{\partial \net^{\;h}_{t2}},
                \cdots,
                \frac{\partial \cE_{\bx_t}}{\partial \net^{\;h}_{tm}}
    \rB)^T
\end{align*}
where $\bh_t = (h_{t1}, h_{t2}, \cdots, h_{tm})^T \in \setR^m$ is the
$m$-dimensional hidden state vector at time $t$, and $\net^{\;h}_{ti}$ is
the net value at hidden neuron $h_{ti}$ at time $t$.

\medskip

The key step in backpropagation is to compute the net gradients in
reverse order, starting from the output neurons to the input neurons via
the hidden neurons. 
\end{frame}
%Let $f^h$ and $f^o$ denote the
%activation functions for the hidden state and output neurons, and
%let $\partial\bff^{\;h}_t$ and $\partial\bff^{\;o}_t$ 
%denote the vector of the derivatives of the activation
%function with respect to the net signal (that is, its argument) 
%for the hidden and output
%neurons at time $t$, given as
%\begin{align*}
%    \partial\bff^{\;h}_t & = \lB(
%        \frac{\partial f^h(\net^{\;h}_{t1})}{\partial\net^{\;h}_{t1}},
%        \frac{\partial f^h(\net^{\;h}_{t2})}{\partial\net^{\;h}_{t2}},
%        \cdots, \frac{\partial f^h(\net^{\;h}_{tm})}{\partial
%    \net^{\;h}_{tm}}\rB)^T\\
%\partial\bff^{\;o}_t & = \lB(
%    \frac{\partial f^o(\net^{\;o}_{t1})}{\partial\net^{\;o}_{t1}},
%    \frac{\partial f^o(\net^{\;o}_{t2})}{\partial\net^{\;o}_{t2}},
%\cdots, \frac{\partial f^o(\net^{\;o}_{tp})}{\partial \net^{\;o}_{tp}}\rB)^T
%\end{align*}
%Finally, let $\partial\bcE_{\bx_t}$ denote the vector of partial
%derivatives of the error function with respect to $\bo_t$:
%\begin{align*}
%    \partial\bcE_{\bx_t} = 
%    \lB(\frac{\partial \cE_{\bx_t}}{\partial o_{t1}},
%        \frac{\partial \cE_{\bx_t}}{\partial o_{t2}},
%    \;\cdots,\; \frac{\partial \cE_{\bx_t}}{\partial o_{tp}}\rB)^T
%\end{align*}
%
%\begin{figure}[!t]
%    \large
%\captionsetup[subfloat]{captionskip=12pt}

\begin{frame}{RNN:Backpropagation step}
%\vspace{0.2in}
\centerline{
    \psset{tnpos=l,tnsep=2pt,colsep=1.5,rowsep=1.75,mcol=c,
    arrowscale=2, arrows=<-}
%    \subfloat[Backpropagation step]{%
%\label{fig:reg:deep:rnn_unfolded_l_bp}
    \scalebox{0.65}{%
\psmatrix
[mnode=none,name=l0]{$l=0$} 
    & [mnode=none]{$l=1$} 
    & [mnode=none]{$l=2$} 
    & [mnode=none]{$l=3$} 
    & [mnode=none]{$\ldots$} 
    & [mnode=none]{$l=\tau$} 
    & [mnode=none]{$l=\tau+1$}\\[-4em]
\Tp[name=om1]
    & \Tp[name=o0] 
    &  \TR[name=o1]{\myfboxB{$\bo_1$\\~\\$\Large\bdelta^o_1$}}
    & \TR[name=o2]{\myfboxB{$\bo_2$\\~\\$\Large\bdelta^o_2$}}
    & \TR[name=oi]{\myfboxB[linecolor=gray]{$\cdots\!$}}
    & \TR[name=ot1]{\myfboxB{$\bo_{\tau-1}$\\~\\$\Large\bdelta^o_{\tau-1}$}}
    & \TR[name=ot]{\myfboxB{$\bo_\tau$\\~\\$\Large\bdelta^o_{\tau}$}}\\
\TR[name=h0]{\myfbox{$\bh_0$}}
    & \TR[name=h1]{\myfboxB{$\bh_1$\\~\\$\Large\bdelta^h_1$}}
    & \TR[name=h2]{\myfboxB{$\bh_2$\\~\\$\Large\bdelta^h_2$}}
    & \TR[name=hi]{\myfboxB[linecolor=gray]{$\cdots\!$}}
    & \TR[name=ht1]{\myfboxB{$\bh_{\tau-1}$\\~\\$\Large\bdelta^h_{\tau-1}$}}
    & \TR[name=ht]{\myfboxB{$\bh_\tau$\\~\\$\Large\bdelta^h_{\tau}$}}\\
\TR[name=x1]{\myfbox{$\bx_1$}}
    & \TR[name=x2]{\myfbox{$\bx_2$}}
    & \TR[name=xi]{\myfboxB[linecolor=gray]{$\cdots\!$}}
    & \TR[name=xt1]{\myfbox{$\bx_{\tau-1}$}}
    & \TR[name=xt]{\myfbox{$\bx_{\tau}$}}
    & \Tp[name=xp1] 
    & \Tp[name=xp2]
%bias to next layer
    \psset{linewidth=2pt, nrot=:U}
\ncline{h0}{h1}\nbput[npos=0.5]{$\bW_h\cdot\bdelta^h_1$}
\ncline{h1}{h2}\nbput[npos=0.5]{$\bW_h\cdot\bdelta^h_2$}
\ncline{ht1}{ht}\nbput[npos=0.5]{$\bW_h\cdot\bdelta^h_\tau$}
\ncline{h1}{o1}\nbput[npos=0.5]{$\bW_o\cdot\bdelta^o_1$}
\ncline{h2}{o2}\nbput[npos=0.5]{$\bW_o\cdot\bdelta^o_2$}
\ncline{ht1}{ot1}\nbput[npos=0.5]{$\bW_o\cdot\bdelta^o_{\tau-1}$}
\ncline{ht}{ot}\nbput[npos=0.5]{$\bW_o\cdot\bdelta^o_\tau$}
\ncline{x1}{h1}%\nbput[npos=0.5]{$\bW_i\cdot\bdelta^h_1$}
\ncline{x2}{h2}%\nbput[npos=0.5]{$\bW_i\cdot\bdelta^h_2$}
\ncline{xt1}{ht1}%\nbput[npos=0.5]{$\bW_i\cdot\bdelta^h_{\tau-1}$}
\ncline{xt}{ht}%\nbput[npos=0.5]{$\bW_i\cdot\bdelta^h_\tau$}
\psset{linewidth=1pt, linestyle=dashed}
\ncline{h2}{hi} 
\ncline{hi}{ht1}
\ncline{xi}{hi}
\ncline{hi}{oi}
% \psset{linestyle=dashed, boxsize=0.8, nodesep=0.2cm}
% \ncbox[]{om1}{x1}
% \ncbox[]{o0}{x2}
% \ncbox[]{o1}{xi}
% \ncbox[]{o2}{xt1}
% \ncbox[]{oi}{xt}
% \ncbox[]{ot1}{xp1}
% \ncbox[]{ot}{xp2}
\endpsmatrix
}}%}
\end{frame}
%\vspace{0.2in}
%\caption{RNN unfolded as layers.}
%\label{fig:reg:deep:rnn_unfolded_l}
%\vspace{-0.2in}
%\end{figure}
%
%
%\subsubsection{Computing Net Gradients}
\begin{frame}{Computing Net Gradients}
%\index{recurrent neural networks!net gradients}
%The key step in backpropagation is to compute the net gradients in
%reverse order, starting from the output neurons to the input neurons via
%the hidden neurons. %Given the layer view in
%\cref{fig:reg:deep:rnn_unfolded_l_ff}, 
The backpropagation step reverses
the flow direction for computing the net gradients $\bdelta^o_t$ and
$\bdelta^h_t$.%, as shown in the backpropagation graph.% in 
%\cref{fig:reg:deep:rnn_unfolded_l_bp}.
%
%In particular, 
The net gradient vector at the output $\bo_t$ can be
computed as follows:
\begin{align*}
    \tcbhighmath{
\bdelta^o_t = \partial\bff^{\;o}_t \;\odot\; \partial\bcE_{\bx_t}}
\end{align*}
where $\odot$ is the element-wise or Hadamard product.
%For example, if $\cE_{\bx_t}$ is the squared error function, and the output
%layer uses the identity function, then via
%\cref{eq:reg:neural:deriv_identity} and \cref{eq:reg:neural:deriv_SE} we have
%\begin{align*}
%    \bdelta^o_t = \bone \;\odot\; (\bo_t - \by_t)
%\end{align*}

On the other hand, the net gradients at each of the hidden layers
need to account for the incoming net gradients from $\bo_t$ and from
$\bh_{t+1}$.% as seen in \cref{fig:reg:deep:rnn_unfolded_l_bp}. 
%Thus,
%generalizing \cref{eq:reg:neural:bdelta_l}, 
%the net gradient vector for $\bh_t$ (for $t=1,2,\ldots,\tau-1$) is given as
\begin{align*}
    \tcbhighmath{
    \bdelta^h_t = \partial\bff^{\;h}_t \;\odot\; 
    \Bigl( \bigl(\bW_{\!o} \cdot \bdelta^o_t \bigr) + 
        \bigl(\bW_{\!h} \cdot \bdelta^h_{t+1}  \bigr)
\Bigr)}
\end{align*}
Note that for $\bh_\tau$, it depends only on $\bo_\tau$.% (see
%\cref{fig:reg:deep:rnn_unfolded_l_bp}), therefore
%\begin{align*}
%    \bdelta^h_\tau = \partial\bff^{\;h}_\tau \;\odot\; 
%    \bigl(\bW_{\!o} \cdot \bdelta^o_\tau \bigr)
%\end{align*}
%For the tanh activation, which is commonly used in RNNs, the derivative
%of the activation function (see \cref{eq:reg:neural:deriv_tanh}) 
%with respect to the net values at $\bh_t$ is
%given as
%\begin{align*}
%    \partial\bff^{\;h}_t = (\bone - \bh_t \odot \bh_t)
%\end{align*}
%Finally, note that 
The net gradients do not have to be computed for $\bh_0$ or for any
of the input neurons $\bx_t$, since these are leaf nodes in the
backpropagation graph.%, and thus do not backpropagate the gradients beyond
%those neurons.
\end{frame}
%
%\subsubsection{Stochastic Gradient Descent}
%\index{stochastic gradient descent}
%\index{recurrent neural networks!stochastic gradient descent}
%
%The net gradients for the output $\bdelta_{t}^o$ and hidden neurons
%$\bdelta_t^h$ at time $t$ can be
%used to compute the gradients for the weight matrices and bias vectors
%at each time point. 
%However, since an RNN uses parameter sharing across time,
%the gradients are obtained by summing up all of the
%contributions from each time step $t$. 
%Define
%$\bgrad^t_{\bW_{\!o}}$ and $\bgrad^t_{\bb_{o}}$ as the gradients
%of the weights and biases between
%the hidden neurons $\bh_t$ and output neurons $\bo_t$ for time $t$.
%Using the backpropagation equations,
%[\cref{eq:reg:neural:gradW_deep}] and [\cref{eq:reg:neural:gradB_deep}],
%for deep multilayer perceptrons,
%these gradients are
%computed as follows:
%\begin{align*}
%    \bgrad_{\!\bb_{o}} & = \sum^{\tau}_{t=1} \bgrad^t_{\bb_{o}} = 
%    \sum^{\tau}_{t=1} \bdelta^o_t &
%    \bgrad_{\bW_{\!o}} & = \sum^{\tau}_{t=1}\bgrad^t_{\bW_{\!o}} = 
%    \sum^{\tau}_{t=1} \bh_t \cdot \lB(\bdelta^o_t\rB)^T
%\end{align*}
%%where $\bgrad^t_{\bW_{\!o}}$ and $\bgrad^t_{\bb_{o}}$
%%are the gradient
%%contributions from time $t$ for the weights and biases for the output
%%neurons.
%Likewise, the gradients of the other shared parameters between hidden
%layers $\bh_{t-1}$ and $\bh_t$, and between the input layer $\bx_t$ and
%hidden layer $\bh_t$, are obtained as follows:
%\begin{align*}
%    \bgrad_{\!\bb_{h}} & = \sum^{\tau}_{t=1} \bgrad^t_{\bb_{h}} = 
%    \sum^{\tau}_{t=1} \bdelta_t^h &
%    \bgrad_{\bW_{\!h}} & = \sum^{\tau}_{t=1}\bgrad^t_{\bW_{\!h}} =
%    \sum^{\tau}_{t=1} \bh_{t-1} \cdot \lB(\bdelta_t^h\rB)^T\\
%    \bgrad_{\bW_{\!i}} & = \sum^{\tau}_{t=1}\bgrad^t_{\bW_{\!i}} = 
%    \sum^{\tau}_{t=1} \bx_{t} \cdot \lB(\bdelta_t^h\rB)^T
%\end{align*}
%where $\bgrad^t_{\bW_{\!h}}$ and $\bgrad^t_{\bb_{h}}$
%are the gradient
%contributions from time $t$ for the weights and biases for the hidden
%neurons, and $\bgrad^t_{\bW_{\!i}}$ the gradient contributions for the
%weights for the input neurons.
%Finally, we update all the weight matrices and bias vectors as follows
%\begin{empheq}[box=\tcbhighmath]{equation}
%    \begin{aligned}
%    \bW_{\!i} & = \bW_{\!i} - \eta \cdot \bgrad_{\bW_{\!i}} & 
%    \bW_{\!h} & = \bW_{\!h} - \eta \cdot \bgrad_{\bW_{\!h}} &
%    \bb_{h} & = \bb_{h} - \eta \cdot
%    \bgrad_{\bb_{h}}\\
%    \bW_{\!o} & = \bW_{\!o} - \eta \cdot \bgrad_{\bW_{\!o}} &
%    \bb_{o} & = \bb_{o} - \eta \cdot \bgrad_{\bb_{o}}
%\end{aligned}
%\end{empheq}
%where $\eta$ is the gradient step size (or learning rate).
%
%
%
%% \vspace*{-0.1in}
%\subsection{Training RNNs}
%
%\cref{alg:reg:deep:rnn} shows the pseudo-code for learning the weights
%and biases for an RNN.  The inputs comprise the dataset $\bD = \{\cX_i,
%\cY_i\}_{i=1,\cdots,n}$, the step size for gradient descent $\eta$, an integer
%threshold {\tt maxiter} denoting the number of iterations for training, the
%size of the hidden state vectors $m$, and the activation functions $f^o$
%and $f^h$ for the output and hidden layers.  The size of the input ($d$)
%and output ($p$) layers is determined directly from $\bD$.
%For simplicity we assume that all inputs
%$\cX_{i}$ have the same length $\tau$, which determines the number of
%layers in the RNN. It is relatively easy to handle variable length input
%sequences by unrolling the RNN for different input lengths $\tau_{i}$.
%
%The RNN first initializes the weight matrices and bias vectors with random
%values drawn uniformly from a small range, e.g., $[-0.01, 0.01]$.  The RNN considers each input pair
%$(\cX, \cY) \in \bD$, and computes the predicted output $\bo_t$ for each
%time step via the feed-forward process. The backpropagation phase begins by
%computing the error between $\bo_t$ and true response $\by_t$, and then
%the net gradient vector $\bdelta^{o}_t$ at the output layer for each time
%step $t$. These net gradients are backpropagated from the output layers at
%time $t$ to the hidden layers at time $t$, which are in turn used to compute
%the net gradients $\bdelta_t^h$ at the hidden layers for all time steps $t=1, 2, \cdots,
%\tau$.  
%Note that \cref{alg:reg:deep:rnn:bdo} shows 
%the case where the output layer neurons are independent; if they are not
%independent we can replace it by 
%$\partial\bF^{\;o} \cdot \partial\bcE_{\bx_t}$ (see
%\cref{eq:reg:neural:FE_dependent}). 
%Next, we compute the weight gradient matrices,
%$\bgrad_{\bW_{\!i}}$, $\bgrad_{\bW_{\!h}}$, and $\bgrad_{\bW_{\!o}}$,
%and bias gradient vectors, $\bgrad_{\bb_{h}}$
%and $\bgrad_{\bb_{o}}$. These gradients are used to update the weights
%and biases via stochastic gradient descent.  After each point has been used
%to update the weights, that completes one epoch of training.  The training
%stops when {\tt maxiter} epochs have been reached.
%
%\begin{tightalgo}[!t]{\textwidth-18pt}
%    \SetKwInOut{Algorithm}{\textsc{RNN-Training} ($\bD, \eta, \texttt{maxiter}, m, f^o, f^h$)}
%\Algorithm{}
%\tcp{Initialize bias vectors}
%$\bb_{h} \assign\; $ random $m$-dimensional vector with small values\;
%$\bb_{o} \assign\; $ random $p$-dimensional vector with small values\;
%\tcp{Initialize weight matrix}
%$\bW_{\!i} \assign\; $ random $d \times m$ matrix with small values\;
%$\bW_{\!h} \assign\; $ random $m \times m$ matrix with small values\;
%$\bW_{\!o} \assign\; $ random $m \times p$ matrix with small values\;
%$r \assign 0$ \tcp*[h]{iteration counter}\;
%\Repeat{$r \ge \text{\tt maxiter}$}
%{%
%    \ForEach{$(\cX,\cY) \in \bD$ in random order}{%
%        $\tau \assign |\cX|$ \tcp*[h]{length of training sequence}\;
%        \tcp{Feed-Forward Phase}
%        $\bh_0 \assign \bzero \in \setR^m$ \tcp*[h]{initialize hidden
%        state}\;
%        \For{$t=1,2,\ldots,\tau$}{
%        $\bh_t \assign f^h(\bW_{\!i}^T \bx_t + 
%            \bW_{\!h}^T \bh_{t-1} + \bb_{h})$\;
%        $\bo_t \assign f^o(\bW_{\!o}^T \bh_t + \bb_{o})$\;
%        }
%        \tcp{Backpropagation Phase}
%        \For{$t=\tau,\tau-1,\ldots,1$}{
%            $\bdelta^o_t \assign \partial\bff^{\;o}_t \;\odot\;
%        \partial\bcE_{\bx_t}$
%        \tcp*[h]{net gradients at output}\label{alg:reg:deep:rnn:bdo} \;
%        }
%        $\bdelta^h_\tau \assign \partial\bff^{\;h}_t \;\odot\; 
%            \bigl(\bW_{\!o} \cdot \bdelta^o_t \bigr)$
%            \tcp*[h]{net gradients at $\bh_\tau$}\;
%            \For{$t=\tau-1,\tau-2,\cdots,1$}{
%            $\bdelta^h_t \assign \partial\bff^{\;h}_t \;\odot\; 
%            \Bigl( \bigl(\bW_{\!o} \cdot \bdelta^o_t \bigr) + 
%            \bigl(\bW_{\!h} \cdot \bdelta^h_{t+1}  \bigr)\Bigr)$
%            \tcp*[h]{net gradients at $\bh_t$}%
%            }
%        \tcp{Gradients of weight matrices and bias vectors}
%        $\bgrad_{\bb_{o}} \assign \sum^{\tau}_{t=1}
%        \bdelta^o_t; \quad
%        \bgrad_{\bW_{\!o}} \assign \sum^{\tau}_{t=1} \bh_t \cdot
%        \lB(\bdelta^o_t\rB)^T$\;
%        $\bgrad_{\bb_{h}} \assign \sum^{\tau}_{t=1}
%        \bdelta_t^h; \quad
%        \bgrad_{\bW_{\!h}} \assign \sum^{\tau}_{t=1} \bh_{t-1} \cdot
%        \lB(\bdelta_t^h\rB)^T; \quad
%        \bgrad_{\bW_{\!i}} \assign 
%        \sum^{\tau}_{t=1} \bx_{t} \cdot \lB(\bdelta_t^h\rB)^T$\;
%        \tcp{Gradient Descent Step}
%        $\bb_{o} \assign \bb_{o} - \eta \cdot
%        \bgrad_{\bb_{o}}; \quad
%        \bW_{\!o} \assign \bW_{\!o} - \eta \cdot \bgrad_{\bW_{\!o}}$\; 
%        $\bb_{h} \assign \bb_{h} - \eta \cdot
%        \bgrad_{\bb_{h}}; \quad
%        \bW_{\!h} \assign \bW_{\!h} - \eta \cdot \bgrad_{\bW_{\!h}}; \quad
%        \bW_{\!i} \assign \bW_{\!i} - \eta \cdot \bgrad_{\bW_{\!i}}$\;
%    }
%    $r \assign r+1$\;
%}%
%\caption{RNN Training: Stochastic Gradient Descent}
%\label{alg:reg:deep:rnn}
%\end{tightalgo}
%
%% \enlargethispage{18pt}
%\index{recurrent neural networks!minibatch learning}
%\index{minibatch learning}
%Note that, whereas \cref{alg:reg:deep:rnn} shows the pseudo-code for
%stochastic gradient descent, in practice, RNNs are trained using 
%subsets or {\em minibatches} of input sequences instead of single
%sequences. 
%This helps to speed up the
%computation and convergence of gradient descent, since 
%minibatches provide better
%estimates of the bias and weight gradients and allow the use of
%vectorized operations.
%
%\begin{figure}[t!]
%\vspace{0.1in}
%\caption{Reber grammar automata.}
%\label{fig:reg:deep:reber}
%\vspace{-0.1in}
%\end{figure}
%
%%see rnn-output.txt for results
%
%%\vspace*{-0.1in}
%%\enlargethispage{3\baselineskip}
\begin{frame}{RNN}
\framesubtitle{Reber grammar automata}
%\begin{example}[RNN]
%    \label{ex:reg:deep:rnn}
%    \enlargethispage{0.2in}
    We use an RNN to learn the Reber grammar, which is generated
    according to the automata.% shown in \cref{fig:reg:deep:reber}. 
Let
    $\Sigma = \{\texttt{B}, \texttt{E}, \texttt{P}, \texttt{S},
    \texttt{T}, \texttt{V}, \texttt{X}\}$ denote the alphabet comprising
    the seven symbols. Further, let \texttt{\$} denote a terminal symbol.

\medskip

    Starting from the initial node, we can generate strings that follow
    the Reber grammar by emitting the symbols on the edges. If there are
    two transitions out of a node, each one can be chosen with equal
    probability. 

\medskip

    The task of the RNN is to learn to predict the next symbol for each
    of the positions in a given Reber sequence. For training, we
    generate Reber sequences from the automata.

\end{frame}

\begin{frame}{RNN}
\framesubtitle{Reber grammar automata.}
The sequence $\tup{\texttt{B}, \texttt{T}, \texttt{S},
        \texttt{S}, \texttt{X}, \texttt{X}, \texttt{T}, \texttt{V},
    \texttt{V}, \texttt{E}}$ is a valid Reber sequence (with the
    corresponding state sequence $\tup{0,1,2,2,2,4,3,3,5,6,7}$). On the
    other hand, the sequence
    $\tup{\texttt{B},\texttt{P},\texttt{T},\texttt{X},\texttt{S},\texttt{E}}$
    is not a valid Reber sequence, since there is no edge out of state
    $3$ with the symbol \texttt{X}.

\special{ps: /XC@. {0 setgray } def }
%\vspace{-0.2in}
\centering
\scalebox{0.85}{
    \tikzset{initial text={}}
    % \tikzsetnextfilename{reg_deep_reber}
\begin{tikzpicture}[->,>=Stealth,shorten >=1pt,auto,node distance=2.8cm,
                    semithick]
  \tikzstyle{every state}=[fill=lightgray]

  \node[state] (s)                    {$0$};
  \node[state]         (B) [right of=s]       {$1$};
  \node[state]         (BT) [above right of=B] {$2$};
  \node[state]         (BP) [below right of=B] {$3$};
  \node[state]         (BTX) [right of=BT] {$4$};
  \node[state]         (BPV) [right of=BP] {$5$};
  \node[state]         (BTXS) [below right of=BTX]       {$6$};
  \node[state]         (e) [right of=BTXS]       {$7$};

  \path 
  (s) edge[above] node {\texttt{B}} (B)
  (B) edge[above] node {\texttt{T}} (BT)
  edge[below] node {\texttt{P}} (BP)
  (BT) edge[above] node {\texttt{X}} (BTX)
  edge [loop above]  node {\texttt{S}} (BT)
  (BP) edge[below] node {\texttt{V}} (BPV)
  edge [loop below]  node {\texttt{T}} (BP)
  (BTX) edge[above] node {\texttt{S}} (BTXS)
  edge[below] node {\texttt{X}} (BP)
  (BPV) edge[below] node {\texttt{V}} (BTXS)
  edge node {\texttt{P}} (BTX)
  (BTXS) edge node {\texttt{E}} (e);
\end{tikzpicture}
}
\end{frame}

\begin{frame}{RNN}
\framesubtitle{Reber grammar automata}

\medskip

    Let $S_\cX = \tup{s_1, s_2, \cdots, s_{\tau}}$ be a Reber
    sequence. The corresponding true output $\cY$ is then given as the
    set of next symbols from each of the edges leaving the state
    corresponding to each position in $S_\cX$. 

\medskip

For example, consider the
    Reber sequence $S_\cX = \tup{\texttt{B}, \texttt{P}, \texttt{T},
    \texttt{V}, \texttt{V}, \texttt{E}}$, with the state sequence $\pi =
    \tup{0,1,3,3,5,6,7}$. The desired output sequence is then given as
    $S_\cY = \{\texttt{P|T}, \texttt{T|V}, \texttt{T|V}, \texttt{P|V},
    \texttt{E}, \texttt{\$} \}$, where $\texttt{\$}$ is the terminal
    symbol. 


\medskip

    Here, $\texttt{P|T}$ denotes that the next symbol can be either
    $\texttt{P}$ or $\texttt{T}$.
    We can see that $S_\cY$ comprises the sequence of possible
    next symbols from each of the states in $\pi$ (excluding the start
    state $0$).
\end{frame}

\begin{frame}{RNN}
\framesubtitle{Reber grammar automata}

    To generate the training data for the RNN, we have to convert the
    symbolic Reber strings into numeric vectors through%. We do this via a 
binary encoding:% of the symbols, as follows:

\medskip

    \begin{center}
        \vspace*{-0.1in}
        \begin{tabular}{|c|c|}
            \hline
            \texttt{B}  & $(1,0,0,0,0,0,0)^T$\\
            \texttt{E}  & $(0,1,0,0,0,0,0)^T$\\
            \texttt{P}  & $(0,0,1,0,0,0,0)^T$\\
            \texttt{S}  & $(0,0,0,1,0,0,0)^T$\\
            \texttt{T}  & $(0,0,0,0,1,0,0)^T$\\
            \texttt{V}  & $(0,0,0,0,0,1,0)^T$\\
            \texttt{X}  & $(0,0,0,0,0,0,1)^T$\\
            \texttt{\$} & $(0,0,0,0,0,0,0)^T$\\
            \hline
        \end{tabular}
    \end{center} 
%    That is, each symbol is encoded by a 7-dimensional binary vector,
%    with a $1$ in the column corresponding to its position in the
%    ordering of symbols in $\Sigma$. 

	\medskip

	The terminal symbol $\texttt{\$}$
    is not part of the alphabet, and its encoding is all 0's.

	\medskip

    Finally, to encode the possible next symbols, we follow a similar
    binary encoding with a $1$ in the column corresponding to the
    allowed symbols. 

\end{frame}

\begin{frame}{RNN}
\framesubtitle{Reber grammar automata}

For example, the choice $\texttt{P|T}$ is encoded
    as $(0,0,1,0,1,0,0)^T$. Thus, the Reber sequence $S_\cX$ and the
    desired output sequence $S_\cY$ are encoded as:
    \begin{center}
        \begin{tabular}{|c|cccccc||cccccc|}
            \hline
        & \multicolumn{6}{c||}{$\cX$} & \multicolumn{6}{c|}{$\cY$}\\
            \hline
            & $\bx_1$ & $\bx_2$ & $\bx_3$ & $\bx_4$ & $\bx_5$ & $\bx_6$
            & $\by_1$ & $\by_2$ & $\by_3$ & $\by_4$ & $\by_5$ & $\by_6$\\
            \hline
            $\Sigma$ & \texttt{B} & \texttt{P} & \texttt{T} & \texttt{V} &
            \texttt{V} & \texttt{E} & 
            \texttt{P|T} & \texttt{T|V} & \texttt{T|V} & \texttt{P|V} &
            \texttt{E}& \texttt{\$} \\
            \hline
            \texttt{B} & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
            \texttt{E} & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0\\
            \texttt{P} & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0\\
            \texttt{S} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
            \texttt{T} & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0\\
            \texttt{V} & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 1 & 0 & 0\\
            \texttt{X} & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
            \hline
        \end{tabular}
    \end{center} 

\end{frame}

\begin{frame}{RNN}
\framesubtitle{Reber grammar automata}

    For training, we generate $n=400$ Reber sequences with a minimum
    length of $30$. The maximum sequence length is $\tau=52$. Each of these
    Reber sequences is used to create a training pair $(\cX, \cY)$ as
    described above. 

\medskip
Next, we train an RNN with $m=4$ hidden neurons
    using tanh activation. The input and ouput layer sizes are
    determined by the dimensionality of the encoding, namely $d= 7$ and
    $p=7$. We use a sigmoid activation at the output layer, treating
    each neuron as independent. We use  the binary cross
    entropy error function. 

\medskip

The RNN is trained for $r=10000$ epochs,
    using gradient step size $\eta=1$ and the entire set of 400
    input sequences as the batch size.
    The RNN model 
    learns the training data perfectly, making no errors in the
    prediction of the set of possible next symbols. 


\end{frame}

\begin{frame}{RNN}
\framesubtitle{Reber grammar automata}


    We test the RNN model on $100$ previously unseen Reber
    sequences (with minimum length 30, as before). The RNN makes no
    errors on the test sequences. 

	\medskip

	On the other hand, we also trained an
    MLP with a single hidden layer, with size $m$ varying between $4$
    and $100$. 

\medskip


Even after $r=10000$ epochs, the MLP is not able to
    correctly predict any of the output sequences perfectly.

	\medskip

	It makes
    2.62 mistakes on average per sequence for both the training and
    testing data.

	\medskip

	Increasing the number of epochs or the number of
    hidden layers does not improve the MLP performance.
\end{frame}
%\end{example}
%
%% \vspace*{-0.2in}
%\subsection{Bidirectional RNNs}
\begin{frame}{Bidirectional RNNs}
%\index{bidirectional recurrent neural networks}
%\index{recurrent neural networks!bidirectional}
An RNN makes use of a hidden state $\bh_t$ that depends on the
previous hidden state $\bh_{t-1}$ and the current input $\bx_{t}$ at
time $t$. In other words, it only looks at information from the past. 

\medskip


A bidirectional RNN (BRNN) %, as shown in \cref{fig:reg:deep:brnn}, 
extends the RNN model to also include
information from the future. 

\medskip

In particular, a BRNN maintains a backward
hidden state vector $\bb_t \in \setR^m$ that depends on the next backward hidden
state $\bb_{t+1}$ and the current input $\bx_t$. The output at time $t$
is a function of both $\bh_t$ and $\bb_t$. In particular, we compute the
forward and backward hidden state vectors as follows:
\begin{empheq}[box=\tcbhighmath]{equation*}
    \begin{aligned}    
    \bh_t  & = f^h(\bW_{\!ih}^T \bx_t + \bW_{\!h}^T
    \bh_{t-1} + \bb_{h})\\
    \bb_t  & = f^b(\bW_{\!ib}^T \bx_t + \bW_{\!b}^T
    \bb_{t+1} + \bb_{b})
    \end{aligned}
\end{empheq}
%Also, a BRNN needs two initial state vectors $\bh_0$ and $\bb_{\tau+1}$ to
%compute $\bh_1$ and $\bb_\tau$, respectively. These are usually set to
%$\bzero \in \setR^m$. 
%The forward and backward hidden states are computed independently, with
%the forward hidden states computed by considering the input sequence
%in the forward direction $\bx_1, \bx_2, \cdots, \bx_\tau$, and with the
%backward hidden
%states computed by considering the sequence in reverse order $\bx_\tau,
%\bx_{\tau-1}, \cdots, \bx_1$.  
\end{frame}

\begin{frame}{Bidirectional RNNs}
The output at time $t$ is computed only when both $\bh_t$ and $\bb_t$
are available, and is given as
\begin{align*}
    \bo_t & = f^o(\bW_{\!ho}^T \bh_t + 
    \bW_{\!bo}^T \bb_{t} + \bb_{o})
\end{align*}
It is clear that BRNNs need the complete input before they can compute
the output. 

\medskip

We can also view a BRNN as having two sets of input sequences, namely
the forward input sequence $\cX = \tup{\bx_1, \bx_2, \cdots, \bx_\tau}$ and the
reversed input sequence $\cX^r = \tup{\bx_\tau, \bx_{\tau-1}, \ldots,
\bx_1}$, with the corresponding hidden states $\bh_t$ and $\bb_t$, which
together determine the output $\bo_t$. 

\medskip

Thus, a BRNN is comprised of two
``stacked'' RNNs with independent hidden layers 
that jointly determine the output. 
\end{frame}
%
%\begin{figure}[t!]
%    \large
% \captionsetup[subfloat]{captionskip=12pt}
\begin{frame}{Bidirectional RNN: Unfolded in time.}
    \psset{tnpos=l,tnsep=2pt,colsep=1.5,rowsep=1.5,mcol=c,
    arrowscale=1.5, arrows=->}
\centerline{
\scalebox{0.55}{%
\psmatrix
[mnode=none]{$t=0$} 
    & [mnode=none]{$t=1$} 
    & [mnode=none]{$t=2$} 
    & [mnode=none]{$\ldots$} 
    & [mnode=none]{$t=\tau-1$} 
    & [mnode=none]{$t=\tau$}
    & [mnode=none]{$t=\tau+1$}\\[-3em]
&   \TR[name=o1]{\myfbox{$\bo_1$}}
    & \TR[name=o2]{\myfbox{$\bo_2$}}
    & \TR[name=oi]{\myfboxB[linecolor=gray]{$\cdots\!$}}
    & \TR[name=ot1]{\myfbox{$\bo_{\tau-1}$}}
    & \TR[name=ot]{\myfbox{$\bo_{\tau}$}}\\
\TR[name=h0]{\myfbox{$\bh_0$}}
& 
    %\TR[name=h1]{\myfbox{$\bh_1$\\\hline$\bb_{\tau}$}}
    \TR[name=h1]{\myfboxD{$\bh_1$}{$\bb_{1}$}}
    & \TR[name=h2]{\myfboxD{$\bh_{2}$}{$\bb_{2}$}}
    & \TR[name=hi]{\myfboxD[linecolor=gray]{$\cdots\!$}{$\cdots\!$}}
    %\Tcircle[name=hi,linecolor=gray]{$\cdots$}
    & \TR[name=ht1]{\myfboxD{$\bh_{\tau-1}$}{$\bb_{\tau-1}$}}
    & \TR[name=ht]{\myfboxD{$\bh_{\tau}$}{$\bb_{\tau}$}}
    & \TR[name=b0]{\myfbox{$\bb_{\tau+1}$}}\\
&   \TR[name=x1]{\myfbox{$\bx_1$}}
    & \TR[name=x2]{\myfbox{$\bx_2$}}
    & \TR[name=xi]{\myfboxB[linecolor=gray]{$\cdots\!$}}
    & \TR[name=xt1]{\myfbox{$\bx_{\tau-1}$}}
    & \TR[name=xt]{\myfbox{$\bx_{\tau}$}}
%bias to next layer
    \psset{linewidth=2pt, arcangle=20}
    \ncline{h0}{h1}\naput[npos=0.5]{$\bW_{\!h}, \bb_{h}$}
    \ncarc{h1}{h2}\naput[npos=0.5]{$\bW_{\!h}, \bb_{h}$}
    \ncarc{ht1}{ht}\naput[npos=0.5]{$\bW_{\!h}, \bb_{h}$}
    \ncline{h1}{o1}\nbput[npos=0.6]{$\bW_{\!ho}, \bb_{o}$}
    \naput[npos=0.6]{$\bW_{\!bo}$}
    \ncline{h2}{o2}\nbput[npos=0.6]{$\bW_{\!ho}, \bb_{o}$}
    \naput[npos=0.6]{$\bW_{\!bo}$}
    \ncline{ht1}{ot1}\nbput[npos=0.6]{$\bW_{\!ho}, \bb_{o}$}
    \naput[npos=0.6]{$\bW_{\!bo}$}
    \ncline{ht}{ot}\nbput[npos=0.6]{$\bW_{\!ho}, \bb_{o}$}
    \naput[npos=0.6]{$\bW_{\!bo}$}
    \ncline{x1}{h1}\naput[npos=0.25]{$\bW_{\!ih}$}\nbput[npos=0.25]{$\bW_{\!ib}$}
    \ncline{x2}{h2}\naput[npos=0.25]{$\bW_{\!ih}$}\nbput[npos=0.25]{$\bW_{\!ib}$}
    \ncline{xt1}{ht1}\naput[npos=0.25]{$\bW_{\!ih}$}\nbput[npos=0.25]{$\bW_{\!ib}$}
    \ncline{xt}{ht}\naput[npos=0.25]{$\bW_{\!ih}$}\nbput[npos=0.25]{$\bW_{\!ib}$}
\psset{linewidth=1pt, linestyle=dashed}
\ncarc{h2}{hi} 
\ncarc{hi}{ht1}
\ncline{xi}{hi}
\ncline{hi}{oi}
%backward direction
\psset{linewidth=1pt, linestyle=dashed, linecolor=black}
\ncarc{hi}{h2} 
\ncarc{ht1}{hi}
\psset{linewidth=2pt, linestyle=solid, linecolor=black}
\ncarc{h2}{h1}\naput[npos=0.5]{$\bW_{\!b}, \bb_{b}$}
\ncarc{ht}{ht1}\naput[npos=0.5]{$\bW_{\!b}, \bb_{b}$}
\ncline{b0}{ht}\naput[npos=0.5]{$\bW_{\!b}, \bb_{b}$}
\endpsmatrix
}}
\end{frame}
%\vspace{0.2in}
%\caption{Bidirectional RNN: Unfolded in time.}
%\label{fig:reg:deep:brnn}
%\end{figure}
%
%\section{Gated RNN\textsc{s}: Long Short-Term Memory Networks}

\ifdefined\wox \begin{frame} \titlepage \end{frame} \fi



\begin{frame}{Gated RNN\textsc{s}: Long Short-Term Memory Networks}
%\label{sec:reg:deep:LSTM} 
%
%\index{gated RNN}
%\index{LSTM|see{long short-term memory networks}}
%\index{long short-term memory networks}
%\index{deep learning!gated RNN}
%\index{deep learning!LSTM}
%\index{deep learning!long short-term memory networks}
One of the problems in training RNNs is their susceptibility 
to either the {\em vanishing gradient} or the {\em exploding gradient} problem. For example,
consider the task of computing the net gradient vector $\bdelta_t^h$ for
the hidden layer at time $t$, given as
%\index{vanishing gradient}
%\index{exploding gradient}
%\index{deep learning!vanishing gradient}
%\index{deep learning!exploding gradient}
\begin{align*}
    \bdelta_t^h = \partial\bff^{\;h}_t \odot \Bigl(\bigl(\bW_{\!o} \cdot
        \bdelta_t^o \bigr) + \bigl(\bW_{\!h} \cdot \bdelta_{t+1}^h
    \bigr) \Bigr)
\end{align*}
Assume for simplicity that we use a linear
activation function, i.e., $\partial \bff^{\;h}_t = \bone$, 
and let us ignore the net gradient vector for the
output layer, focusing only on the dependence on the hidden layers.
Then for an input sequence of
length $\tau$, we have
\begin{align*}
    \bdelta_t^h & = \bW_{\!h} \cdot \bdelta_{t+1}^h
     = \bW_{\!h} (\bW_{\!h} \cdot \bdelta_{t+2}^h) = \bW_{\!h}^2 \cdot
    \bdelta_{t+2}^h
    = \cdots = \bW_{\!h}^{\tau-t} \cdot \bdelta_{\tau}^h
\end{align*}
We can observe that the net gradient from time $\tau$ affects the
net gradient vector at time $t$ as a function of $\bW_{\!h}^{\tau-t}$,
i.e., as powers of the hidden weight matrix $\bW_{\!h}$. 
%In particular, for example, at  time $t=1$, we have $\bdelta^h_1 =
%\bW_{\!h}^{\tau-1} \cdot \bdelta_\tau^h$.
\end{frame}

\begin{frame}{Gated RNN\textsc{s}: Long Short-Term Memory Networks}
Let the {\em
spectral radius} of $\bW_{\!h}$, defined as the absolute value of its largest
eigenvalue, be
given as $\abs{\lambda_1}$. 

\medskip

It turns out that if $\abs{\lambda_1} < 1$,
then $\norm{\bW_{\!h}^{k}} \to 0$ as $k \to
\infty$, that is, the gradients vanish as we train on
long sequences. 


\medskip

On the other hand, if $\abs{\lambda_1} > 1$, then at
least one element of $\bW_{\!h}^{k}$ becomes unbounded and thus 
$\norm{\bW_{\!h}^{k}} \to \infty$ as $k \to \infty$, 
that is, the gradients
explode as we train on long sequences. 
%To see this more clearly, let the eigendecomposition of the square
%$m\times m$
%matrix $\bW_{\!h}$ be given as
%\begin{align*}
%    \bW_{\!h} = \bU \cdot 
%            \matr{\lambda_1 & 0 & \cdots & 0\\
%                    0 & \lambda_2 & \cdots & 0\\
%                    \vdots &\vdots & \ddots & \vdots\\
%                0 & 0 & \cdots & \lambda_m} \cdot \bU^{-1}
%\end{align*}
%where $|\lambda_1| \ge |\lambda_2| \ge \cdots \ge |\lambda_m|$ are the
%eigenvalues of $\bW_{\!h}$, and $\bU$ is the matrix comprising the corresponding
%eigenvectors, $\bu_1, \bu_2, \cdots, \bu_m$, as columns. Thus, we have
%\begin{align*}
%    \bW_{\!h}^{k} = \bU \cdot 
%    \matr{\lambda_1^{k} & 0 & \cdots & 0\\
%        0 & \lambda_2^{k} & \cdots & 0\\
%                    \vdots &\vdots & \ddots & \vdots\\
%                0 & 0 & \cdots & \lambda_m^{k}} \cdot \bU^{-1}
%\end{align*}

\medskip

It is clear that the net gradients scale according to the eigenvalues of
$\bW_{\!h}$. Therefore, if $|\lambda_1| < 1$, then $|\lambda_1|^{k} \to 0$
as $k \to \infty$, and since $|\lambda_1| \ge |\lambda_i|$ for all
$i=1,2,\cdots,m$, then necessarily $|\lambda_i|^{k} \to 0$ as well.
That is, the gradients vanish.

\medskip

On the other hand, if $|\lambda_1| > 1$, then $|\lambda_1|^{k} \to
\infty$
as $k \to \infty$, and the gradients explode.


\medskip

Therefore, for the error to
neither vanish nor explode, the spectral radius of $\bW_{\!h}$ should
remain $1$ or very close to it.
\end{frame}
%
\begin{frame}{Gated RNN\textsc{s}: Long Short-Term Memory Networks}
Long short-term memory (LSTM) networks alleviate the vanishing gradients
problem by using {\em
gate neurons} to control access to the hidden states.

\medskip

Consider the $m$-dimensional hidden state vector $\bh_{t} \in \setR^m$ 
at time $t$. In a regular RNN, we update the hidden state as follows: % (as
%per \cref{eq:reg:deep:hidden_t}):
%\index{gate neurons}
%\index{long short-term memory networks!gate neurons}
\begin{align*}
    \bh_t = f^h(\bW_{\!i}^T \bx_t + \bW_{\!h}^T
    \bh_{t-1} + \bb_{h})
\end{align*}
Let $\bg \in \{0,1\}^m$ be a binary vector. If we take the element-wise
product of $\bg$ and $\bh_t$, namely, $\bg \odot \bh_{t}$, 
then elements of $\bg$ act as gates that 
either allow the corresponding element of $\bh_t$ to be retained or set
to zero, that is, select elements of $\bh_t$ to be remembered or forgotten. 

\medskip

%The vector $\bg$ thus acts as logical gate that allows selected
%elements of $\bh_t$ to be remembered or forgotten. 

However, for
backpropagation we need {\em differentiable gates}, for which we use
sigmoid activation on the gate neurons so that their value lies in the
range $[0,1]$. 


\medskip

Like a logical gate, such neurons allow the inputs to be
completely remembered if the value is $1$, or forgotten if the value is
$0$. 

\medskip

In addition, they allow a weighted memory, allowing partial
remembrance of the elements of $\bh_t$, for values between $0$
and $1$.
\end{frame}
%
%\begin{example}[Differentiable Gates]
\begin{frame}{Gated RNNs: Differentiable Gates}

    Consider a hidden state vector
    \begin{align*}
        \bh_t = \matr{-0.94 & 1.05 & 0.39 & 0.97 & 0.90}^T
    \end{align*}
    and a logical gate vector $\bg$:
    \begin{align*}
        \bg = \matr{0 & 1 & 1 & 0 & 1}^T
    \end{align*}
    Their element-wise product gives
    \begin{align*}
        \bg \odot \bh_{t} = \matr{0 & 1.05 & 0.39 & 0 & 0.90}^T
    \end{align*}
    The first and fourth elements have been
    ``forgotten.''
    Now consider a differentiable gate vector
    \begin{align*}
        \bg = \matr{0.1 & 0 & 1 & 0.9 & 0.5}^T
    \end{align*}
    The element-wise product of $\bg$ and $\bh_t$ gives
    \begin{align*}
        \bg \odot \bh_{t} = \matr{-0.094 & 0 & 0.39 & 0.873 & 0.45}^T
    \end{align*}
    Only a fraction specified by an element of $\bg$ is
    retained as a memory.
\end{frame}
%\end{example}
%
%
%\subsection{Forget Gate}
%\index{recurrent neural networks!forget gate}
\begin{frame}{Gated RNN\textsc{s}: Long Short-Term Memory Networks}
	\framesubtitle{Forget Gate}
To see how gated neurons work, we consider an RNN with a {\em forget
gate}. 
%Let $\bh_t \in \setR^m$ be the hidden state vector, and let
%$\bphi_t \in \setR^m$ be a
%forget gate vector. 
%Both these vectors have the same number
%of neurons, $m$. 
%
%

\medskip

In a regular RNN, assuming tanh activation, the hidden state vector is updated unconditionally, as
follows:
\begin{align*}
    \bh_t & = \tanh\lB( \bW_{\!i}^T \bx_t + \bW_{\!h}^T \bh_{t-1} +
    \bb_{h} \rB)
\end{align*}
Instead of directly updating $\bh_t$, we will employ the 
forget gate neurons to control how much of
the previous hidden state vector to forget when computing its new value, 
and also to control how to update it in
light of the new input $\bx_t$.

%
%\cref{fig:reg:deep:forgetRNN} shows the architecture of an RNN with a
%forget gate. 

\medskip

Given input $\bx_t$ and previous hidden state
$\bh_{t-1}$, we first compute a candidate update
vector $\bu_t$, as follows:
\begin{align*}
    \tcbhighmath{
    \bu_t  = \tanh\lB( \bW_{\!u}^T \bx_t + \bW_{\!hu}^T \bh_{t-1} +
\bb_{u} \rB)}
\end{align*}
%The candidate update vector 
$\bu_t$ is essentially the unmodified hidden state vector, as in a regular RNN.
%
\end{frame}

\begin{frame}{Gated RNN\textsc{s}: Long Short-Term Memory Networks}
	\framesubtitle{Forget Gate}
Using the forget gate, we can compute the new hidden state vector as follows:
\begin{align*}
    \tcbhighmath{
    \bh_t  = \bphi_t \odot \bh_{t-1} + (\bone -\bphi_t) \odot
\bu_t}
\end{align*}
%Here $\odot$ is the element-wise product operation. 
We can
see that the new hidden state vector retains a fraction of the previous
hidden state values, and a (complementary) fraction of
the candidate update values.

\medskip

Observe that if $\bphi_t = \bzero$, i.e., if we want to entirely
forget the previous hidden state, then $\bone-\bphi_t = \bone$, which
means that the hidden state will be updated completely at each time step
just like in a regular RNN. 
\end{frame}
%Finally, given the hidden state $\bh_t$ we can compute the output vector $\bo_t$
%as follows
%\begin{align*}
%    \bo_t  = f^{\!o}\lB(\bW_{\!o}^T \bh_t + \bb_o\rB)
%\end{align*}
%
%
%\begin{figure}[!t]
%    \large
%    \hspace{-0.1in}
\begin{frame}{RNN with a forget gate $\bphi_t$}

	\begin{minipage}{3.5cm}
Recurrent connections shown in gray,
forget gate shown doublelined. 

\medskip


		$\odot$ denotes element wise product.

\end{minipage}
\hspace{0.7in}
\begin{minipage}{5cm}
\centerline{
\scalebox{0.75}{%
    \psset{tnpos=l,tnsep=2pt,colsep=0.5,rowsep=1,mcol=c,
    arrowscale=1.5, arrows=->}
\psmatrix
\\[-3em]
& & \TR[name=o]{\myfbox{$\bo_t$}}\\
& & \TR[name=h]{\myfbox{$\bh_t$}}\\[0.4em]
\TR[name=f]{\myfbox[doubleline=true]{$\bphi_t$}} &
% \TR[name=fh]{\psframebox{\scalebox{1.5}{$\odot$}}} & & 
% \TR[name=zf]{\psframebox{\scalebox{1.5}{$\odot$}}} &
\TR[name=fh]{\scalebox{2}{$\odot$}} & & 
\TR[name=zf]{\scalebox{2}{$\odot$}} &
\TR[name=z]{\myfbox{$\bu_t$}}\\[-2.5em]
& & \TR[name=x]{\myfbox{$\bx_t$}}
%bias to next layer
\psset{linewidth=2pt}
\ncline{h}{o}\nbput[npos=0.7]{$\bW_{\!o}, \bb_{o}$}
\ncline{f}{fh}
\ncline{z}{zf}
%\ncline{f}{zf}
\ncangle[angleA=90,angleB=90,armB=1.2,linearc=0.2]{f}{zf}
%\ncarc[arcangle=-20]{f}{zf}
\ncput*[npos=2.4]{\scalebox{0.8}{$\bone-\bphi_t$}}
\ncline{zh}{h}
\ncangle[angleA=0,angleB=270,armB=0,linearc=0.3]{fh}{h}
\ncangle[angleA=180,angleB=270,armB=0,linearc=0.3]{zf}{h}
%from x
\ncangle[angleA=180,angleB=270,armB=0,linearc=0.2]{x}{f}
\naput[npos=1.2]{$\bW_{\!\phi}, \bb_{\phi}$}
\ncangle[angleA=0,angleB=270,armB=0,linearc=0.2]{x}{z}
\nbput[npos=1.2]{$\bW_{\!u}, \bb_{u}$}
%from h
\psset{linewidth=2pt, linecolor=gray}
\ncangle[angleA=180,angleB=180,armB=0.75,linearc=0.2]{h}{f}
\ncput*[npos=1.2]{\scalebox{0.7}{$-1$}}
\nbput*[npos=1.3]{\scalebox{1}{$\bW_{\!h\phi}$}}
\ncangle[angleA=0,angleB=0,armB=0.75,linearc=0.2]{h}{z}
\ncput*[npos=1.2]{\scalebox{0.7}{$-1$}}
\naput*[npos=1.3]{\scalebox{1}{$\bW_{\!hu}$}}
\ncangle[angleA=180,angleB=90,armB=0,linearc=0.2]{h}{fh}
\ncput*[npos=1.2]{\scalebox{0.7}{$-1$}}
\endpsmatrix
}}
\end{minipage}
\end{frame}
%\vspace{0.2in}
%\caption{RNN with a forget gate $\bphi_t$. Recurrent connections shown in gray,
%forget gate shown doublelined. $\odot$ denotes element-wise product.}
%\label{fig:reg:deep:forgetRNN}
%\end{figure}
%
%How should we compute the forget gate vector $\bphi_t$? It makes sense
%to base it on 
%both the previous hidden state and the new input value, so we compute it
%as follows:
%\begin{align}
%    \tcbhighmath{
%    \bphi_t  = \sigma\lB( \bW_{\!\phi}^T \bx_t + \bW_{\!h\phi}^T \bh_{t-1} +
%\bb_{\phi} \rB)}
%\end{align}
%where we use a sigmoid
%activation function, denoted $\sigma$, 
%to ensure that all the neuron values are in the range
%$[0,1]$, denoting the extent to which the corresponding previous hidden state
%values should be forgotten.
%
%
\begin{frame}{Gated RNN\textsc{s}: Long Short-Term Memory Networks}
%To summarize, 
$\bphi_t$ is a layer that
$\bh_{t-1}$ and 
$\bx_t$;
these connections are fully connected, and are specified by the
corresponding  $\bW_{h\phi}$ and $\bW_{\phi}$, 
and  $\bb_\phi$. %On the other hand, the output of the
%forget gate layer 

\medskip


$\bphi_t$ needs to modify the previous hidden state layer $\bh_{t-1}$, and
therefore, both $\bphi_t$ and $\bh_{t-1}$ feed into what is essentially a new {\em
element-wise} product layer, denoted by $\odot$.% in
%\cref{fig:reg:deep:forgetRNN}. 

\medskip


Finally, the output of this element-wise
product layer is used as input to the new hidden layer $\bh_t$ that
also takes input from another element-wise gate that computes the output
from $\bu_t$ and  $\bone-\bphi_t$.


\medskip

Thus, unlike regular layers
that are fully connected and have a weight matrix and bias vector
between the layers, the connections between $\bphi_t$ and $\bh_t$ via
the element-wise layer are all
one-to-one, and the weights are fixed at the value $1$ with bias
$0$. Likewise the connections between $\bu_t$ and $\bh_{t}$ via the
other element-wise layer are also one-to-one, with weights fixed at 1 and
bias at 0.
\end{frame}
%
\begin{frame}{Gated RNNs}
\framesubtitle{Example}
%\begin{example}
    Let $m=5$. Assume that the previous hidden state vector and the 
    candidate update vector are given as follows:
    \begin{align*}
	    \bh_{t-1} &= \matr{-0.94 & 1.05 & 0.39 & 0.97 & 0.9}^T &
        \bu_t & = \matr{0.5 & 2.5 & -1.0 & -0.5 & 0.8}^T
    \end{align*}
    Let the forget gate and its complement be given as follows:
    \begin{align*}
	    \bphi_t& =  \matr{0.9 & 1& 0 & 0.1 & 0.5}^T & 
        \bone-\bphi_t& =  \matr{0.1 & 0& 1 & 0.9 & 0.5}^T
    \end{align*}
    The new hidden state vector is then computed as the weighted sum of
    the previous hidden state vector and the candidate update vector:
    \begin{align*}
    \bh_t & = \bphi_t \odot \bh_{t-1} + (\bone -\bphi_t) \odot \bu_t\\
              & = \matr{0.9 & 1& 0 & 0.1 & 0.5}^T \odot 
              \matr{-0.94 & 1.05  & 0.39 & 0.97 & 0.9}^T +\\
              & \quad \matr{0.1 & 0& 1 & 0.9 & 0.5}^T \odot 
              \matr{0.5 & 2.5 & -1.0 & -0.5 & 0.8}^T\\
                & = 
                \matr{-0.846 & 1.05 & 0 & 0.097 & 0.45}^T +
                \matr{0.05 & 0 & -1.0 & -0.45 & 0.40}^T\\
                & =\matr{-0.796 & 1.05 & -1.0 & -0.353 & 0.85}^T
    \end{align*}
%\end{example}
\end{frame}
%
%
%\subsubsection{Computing Net Gradients}
%It is instructive to compute the net gradients for an RNN with a forget
%gate, since a similar approach is used to compute the net gradients when
%training an LSTM network.
%An RNN with a forget gate has the following parameters it needs to learn,
%namely the weight matrices $\bW_{\!u}$, $\bW_{\!hu}$,
%$\bW_{\!\phi}$, $\bW_{\!h\phi}$, and $\bW_{\!o}$, 
%and the bias vectors $\bb_{u}$, $\bb_{\phi}$ and $\bb_{o}$. 
%%The output weight matrix $\bW_{\!h}$ and bias vector $b_{\!h}$
%%are computed from $\bh_t$ in the usual manner.
%%However, 
%The computation of the hidden state vector 
%$\bh_t$ adds together the
%inputs from the new element-wise layer that multiplies its incoming edges to compute the net input
%as opposed to computing a weighted sum. We will look at how to
%account for the element-wise layers 
%during backpropagation.
%
%%Let $\delta^h_{ti}$ denote the net input at hidden state neuron $i$ at
%%time $t$.
%\cref{fig:reg:deep:forgetRNN_unfold} shows a forget gate RNN unfolded
%in time for two time steps.
%Let $\bdelta^o_t$, $\bdelta^h_t$, $\bdelta^\phi_t$, and
%$\bdelta^{u}_t$ denote the net gradient vectors at the output,
%hidden, forget gate, and candidate update layers, respectively. 
%During backpropagation, we need to compute the net gradients 
%at each layer. 
\begin{frame}{Gated RNN\textsc{s}: Long Short-Term Memory Networks}
\framesubtitle{Computing Net Gradients}
The
net gradients at the outputs are computed by considering the
partial derivatives of the activation function ($\partial\bff^{\;o}_t$)
and the error function ($\partial\bcE_{\bx_t}$):
\begin{align*}
    \bdelta^o_t = \partial\bff^{\;o}_t \odot \partial\bcE_{\bx_t}
\end{align*}

For the other layers, we can reverse all the arrows to determine the
dependencies between the layers.
%Therefore, to compute the net gradient for the update layer 
%$\bdelta^{u}_{t}$, notice that in backpropagation it has
%only one incoming edge from $\bh_t$ via 
%the element-wise product $(\bone - \bphi_t) \odot \bu_t$.
The net gradient $\delta_{ti}^u$ at update layer neuron $i$ at time $t$ is:
\begin{align*}
    \delta_{ti}^u = 
    \frac{\partial \cE_\bx}{\partial \net^{\;u}_{ti}}  = 
    \frac{\partial \cE_\bx}{\partial \net^{\;h}_{ti}} \cdot 
    \frac{\partial \net^{\;h}_{ti}}{\partial u_{ti}} \cdot
    \frac{\partial u_{ti}}{\partial \net^{\;u}_{ti}}
     = \delta^h_{ti} \cdot (1 - \phi_{ti}) \cdot \lB(1-u_{ti}^2\rB)
\end{align*}
where 
$\frac{\partial \net^{\;h}_{ti}}{\partial u_{ti}} = 
     \frac{\partial}{\partial
    u_{ti}} \lB\{ \phi_{ti} \cdot h_{t-1,i} +  (1-\phi_{ti}) \cdot
    u_{ti}\rB\} = 1 - \phi_{ti}$, and 
the update layer uses a $\tanh$ activation function.

\medskip

Across all neurons, we obtain the net gradient at
$\bu_t$ as follows:
\begin{align*}
    \bdelta^{u}_{t} = \bdelta^h_t \odot (\bone -\bphi_t) 
    \odot (\bone - \bu_t  \odot \bu_t )
\end{align*}
\end{frame}

%To compute the net gradient vector for the forget gate, we
%observe from \cref{fig:reg:deep:forgetRNN_unfold} that there are two
%incoming flows into $\bphi_t$ during backpropagation ---
%one from $\bh_t$ via the element-wise product $\bphi_t
%\odot \bh_{t-1}$, and the other also from $\bh_t$ via the element-wise product 
%$(\bone - \bphi_t) \odot \bu_t$. 
%Therefore, 
\begin{frame}{Gated RNN\textsc{s}: Long Short-Term Memory Networks}
The net gradient $\delta_{ti}^\phi$ 
at forget gate neuron $i$ at time $t$ is given as
\begin{align*}
    \delta_{ti}^\phi = 
    \frac{\partial \cE_\bx}{\partial \net^{\;\phi}_{ti}} & = 
    \frac{\partial \cE_\bx}{\partial \net^{\;h}_{ti}} \cdot
    \frac{\partial \net^{\;h}_{ti}}{\partial \phi_{ti}} \cdot
    \frac{\partial \phi_{ti}}{\partial \net^{\;\phi}_{ti}}
    = \delta^h_{ti} \cdot (h_{t-1,i} - u_{ti}) \cdot \phi_{ti}
    (1-\phi_{ti})
\end{align*}
%where 
%$\frac{\partial \net^{\;h}_{ti}}{\partial \phi_{ti}} = 
%     \frac{\partial}{\partial
%    \phi_{ti}} \lB\{ \phi_{ti} \cdot h_{t-1,i} +  (1-\phi_{ti}) \cdot
%    u_{ti}\rB\} = h_{t-1,i} - u_{ti}$, and 
%    we use the fact that the forget gate uses a sigmoid activation
%function.
Across all neurons, we obtain the net gradient at
$\bphi_t$ as follows:
\begin{align*}
    \bdelta^\phi_t = \bdelta^h_{t} \odot (\bh_{t-1} - \bu_t) 
    \odot \bphi_t \odot (\bone - \bphi_t)
\end{align*}
%
%Finally, let us consider how to compute $\bdelta^h_t$, the net gradient
%at the hidden layer at time $t$.
%In \cref{fig:reg:deep:forgetRNN_unfold} we can observe that if we
%reverse the arrows, $\bdelta^h_t$ depends on the gradients at the 
%output layer $\bo_t$, the forget gate layer
%$\bphi_{t+1}$, the update layer $\bu_{t+1}$, and on the hidden layer
%$\bh_{t+1}$ via the element-wise product $\bh_{t+1} \odot \bphi_{t+1}$. 
%The output, forget and update layers are treated as in a regular RNN.
%However, due to the element-wise layer, the flow from $\bh_{t+1}$ is
%handled as follows:
%\begin{align*}
%    \frac{\partial \cE_{\bx_t}}{\partial
%    \net^{\;h}_{t+1,i}} 
%    \cdot \frac{\partial \net^{\;h}_{t+1,i}}{\partial h_{ti}}
%    \cdot \frac{\partial h_{ti}}{\partial \net^{\;h}_{ti}}
%    = \delta^h_{t+1,i} \cdot \phi_{t+1,i} \cdot 1
%     = \delta^h_{t+1,i} \cdot \phi_{t+1,i}
%\end{align*}
%where $\frac{\partial \net^{\;h}_{t+1,i}}{\partial h_{ti}} = 
%     \frac{\partial}{\partial
%    h_{ti}} \lB\{ \phi_{t+1,i} \cdot h_{ti} +  (1-\phi_{t+1,i}) \cdot
%    u_{t+1,i}\rB\} = \phi_{t+1,i}$, and we used the fact that $\bh_t$
%    implicitly uses an identity activation function.
%Across all the hidden neurons at time $t$, the net gradient vector component from $\bh_{t+1}$ is given
%as $\bdelta^h_{t+1} \odot \bphi_{t+1}$. 
Considering all the layers,
including the output, forget, update and element-wise layers, 
the complete net gradient vector
at the hidden layer at time $t$ is given as:
\begin{align*}
    \bdelta^h_t = \bW_{\!o} \bdelta^o_t + \bW_{\!h\phi}
    \bdelta^\phi_{t+1} + \bW_{\!hu} \bdelta^{u}_{t+1} + 
    \lB(\bdelta^h_{t+1} \odot \bphi_{t+1}\rB)
\end{align*}
\end{frame}
%
%Given the net gradients, we can compute the gradients for all the weight
%matrices and bias vectors in a manner similar to that outlined for a
%regular RNN in \cref{sec:reg:deep:rnn_bpt}. Likewise, stochastic
%gradient descent can be used to train the network.
%\begin{figure}[!t]
%    \large
%    \hspace{-0.3in}
\begin{frame}{RNN with a forget gate unfolded in time}
\framesubtitle{Recurrent connectionsin gray}
\centerline{
\scalebox{0.6}{%
    \hspace{0.9in}
    \psset{tnpos=l,tnsep=2pt,colsep=0.75,rowsep=1,mcol=c,
    arrowscale=1.5, arrows=->}
\psmatrix
\\[-3em]
& & & & \TR[name=o1]{\myfbox{$\bo_1$}} & &
& & \TR[name=o2]{\myfbox{$\bo_2$}}\\
\TR[name=h0]{\myfbox{$\bh_0$}} & &
& & \TR[name=h1]{\myfbox{$\bh_1$}} & &
& & \TR[name=h2]{\myfbox{$\bh_2$}} & &\TR[name=h3]{~}\\[-1.5em]
& & \TR[name=fh1]{\scalebox{1.5}{$\odot$}}
& & & &
\TR[name=fh2]{\scalebox{1.5}{$\odot$}}\\[-1.5em]
& & & & \TR[name=zf1]{\scalebox{1.5}{$\odot$}} &
& & &
\TR[name=zf2]{\scalebox{1.5}{$\odot$}}\\[-1.5em]
& & \TR[name=f1]{\myfbox[doubleline=true]{$\bphi_1$}} & &
\TR[name=z1]{\myfbox{$\bu_1$}} & &
\TR[name=f2]{\myfbox[doubleline=true]{$\bphi_2$}} &
 & 
 \TR[name=z2]{\myfbox{$\bu_2$}}\\[-2em]
& & &\TR[name=x1]{\myfbox{$\bx_1$}} & & &
& \TR[name=x2]{\myfbox{$\bx_2$}}
%bias to next layer
\psset{linewidth=2pt}
\ncline{h1}{o1}\nbput[npos=0.7]{$\bW_{\!o}, \bb_{o}$}
\ncline{h2}{o2}\nbput[npos=0.7]{$\bW_{\!o}, \bb_{o}$}
\ncline{f1}{fh1}
\ncline{z1}{zf1}
\ncline{f2}{fh2}
\ncline{z2}{zf2}
\ncangle[angleA=0,angleB=180,armB=2,linearc=0.2]{f1}{zf1}
\ncput*[npos=1.4]{\scalebox{0.8}{$\bone-\bphi_1$}}
\ncangle[angleA=0,angleB=180,armB=2,linearc=0.2]{f2}{zf2}
\ncput*[npos=1.4]{\scalebox{0.8}{$\bone-\bphi_2$}}
\ncangle[angleA=0,angleB=270,armB=0,linearc=0.2]{fh1}{h1}
\ncline{zf1}{h1}
\ncangle[angleA=0,angleB=270,armB=0,linearc=0.2]{fh2}{h2}
\ncline{zf2}{h2}
%from x
\ncangle[angleA=180,angleB=270,armB=0,linearc=0.2]{x1}{f1}
\naput[npos=0.8]{$\bW_{\!\phi}, \bb_{\phi}$}
\ncangle[angleA=0,angleB=270,armB=0,linearc=0.2]{x1}{z1}
\nbput[npos=0.8]{$\bW_{\!u}, \bb_{u}$}
\ncangle[angleA=180,angleB=270,armB=0,linearc=0.2]{x2}{f2}
\naput[npos=0.8]{$\bW_{\!\phi}, \bb_{\phi}$}
\ncangle[angleA=0,angleB=270,armB=0,linearc=0.2]{x2}{z2}
\nbput[npos=0.8]{$\bW_{\!u}, \bb_{u}$}
%from h
\psset{linewidth=2pt, linecolor=gray}
\ncangle[angleA=0,angleB=180,armB=0.75,linearc=0.2]{h0}{f1}
\nbput*[npos=2]{\scalebox{1}{$\bW_{\!h\phi}$}}
\ncangle[angleA=0,angleB=180,armB=0.75,linearc=0.2]{h0}{z1}
\nbput*[npos=2]{\scalebox{1}{$\bW_{\!hu}$}}
\ncangle[angleA=0,angleB=90,armB=0,linearc=0.2]{h0}{fh1}
\psset{linewidth=2pt, linecolor=gray}
\ncangle[angleA=0,angleB=180,armB=0.75,linearc=0.2]{h1}{f2}
\nbput*[npos=2]{\scalebox{1}{$\bW_{\!h\phi}$}}
\ncangle[angleA=0,angleB=180,armB=0.75,linearc=0.2]{h1}{z2}
\nbput*[npos=2]{\scalebox{1}{$\bW_{\!hu}$}}
\ncangle[angleA=0,angleB=90,armB=0,linearc=0.2]{h1}{fh2}
\ncline{h2}{h3}
\endpsmatrix
}}
\end{frame}
%\vspace{0.2in}
%\caption{RNN with a forget gate unfolded in time (recurrent connections
%in gray).}
%\label{fig:reg:deep:forgetRNN_unfold}
%\end{figure}
%
%
%% \paragraph{Input Gate}
%% Instead of using the complement of the forget gate $\bphi_t$ for the
%% candidate update vector, we can use a separate input gate vector
%% $\bkappa \in \setR^m$ to control how much of the update vector to use
%% for the new hidden state. That is, the 
%
%%\begin{figure}[!ht]
%%    \hspace{-0.3in}
%%\centerline{
%%\scalebox{0.7}{%
%%    \hspace{0.9in}
%%    \psset{tnpos=l,tnsep=2pt,colsep=1,rowsep=1,mcol=c,
%%    arrowscale=1.5, arrows=->}
%%\psmatrix
%%\\[-3em]
%%& \TR[name=o]{\myfbox{$\bo_t$}}\\
%%& \TR[name=h]{\myfbox{$\bh_t$}}\\[-1.5em]
%%\TR[name=fh]{\scalebox{1.5}{$\odot$}} & 
%%\TR[name=zh]{\scalebox{1.5}{$\oplus$}}\\[-1.5em]
%%\TR[name=f]{\myfboxC{$\bphi_t$}{doubleline=true}} &
%%\TR[name=zf]{\scalebox{1.5}{$\odot$}} & 
%%\TR[name=z]{\myfbox{$\bz_t$}}\\[-2em]
%%& \TR[name=x]{\myfbox{$\bx_t$}}
%%%bias to next layer
%%\psset{linewidth=2pt}
%%\ncline{h}{o}\nbput[npos=0.7]{$\bW_{\!o}, \bb_{o}$}
%%\ncline{f}{fh}
%%\ncline{fh}{zh}
%%\ncline{z}{zf}
%%\ncline{f}{zf}\nbput[npos=0.5]{$\bone-\bphi_t$}
%%\ncline{zf}{zh}
%%\ncline{zh}{h}
%%%from x
%%\ncangle[angleA=180,angleB=270,armB=0,linearc=0.2]{x}{f}
%%\naput[npos=1.2]{$\bW_{\!\phi}, \bb_{\phi}$}
%%\ncangle[angleA=0,angleB=270,armB=0,linearc=0.2]{x}{z}
%%\nbput[npos=1.2]{$\bW_{\!u}, \bb_{u}$}
%%%from h
%%\psset{linewidth=2pt, linecolor=gray}
%%\ncangle[angleA=180,angleB=180,armB=0.75,linearc=0.2]{h}{f}
%%\ncput*[npos=1.2]{\scalebox{0.7}{$-1$}}
%%\nbput*[npos=1.3]{\scalebox{1}{$\bW_{\!h\phi}$}}
%%\ncangle[angleA=0,angleB=0,armB=0.75,linearc=0.2]{h}{z}
%%\ncput*[npos=1.2]{\scalebox{0.7}{$-1$}}
%%\naput*[npos=1.3]{\scalebox{1}{$\bW_{\!hz}$}}
%%\ncangle[angleA=180,angleB=90,armB=0,linearc=0.2]{h}{fh}
%%\ncput*[npos=1.4]{\scalebox{0.7}{$-1$}}
%%\endpsmatrix
%%}}
%%\caption{RNN with a forget gate $\bphi_t$. Recurrent connections shown in gray,
%%gate layers shown doublelined. $\odot$ denotes element-wise product and
%%$\oplus$ denotes (element-wise) sum of the incoming vectors.}
%%\label{fig:reg:deep:forgetRNN}
%%\end{figure}
%
%
%
%\subsection{Long Short-Term Memory (LSTM) Networks}
\begin{frame}{Long Short-Term Memory (LSTM) Networks}
%\index{long short-term memory networks}
%\index{deep learning!long short-term memory networks}
%\index{deep learning!LSTM}
%\index{long short-term memory networks!internal memory}
%\index{long short-term memory networks!forget gate}
%\index{long short-term memory networks!input gate}
%\index{long short-term memory networks!output gate}
%We now describe 
LSTMs %, which 
use differentiable gate vectors to control the hidden
state vector $\bh_t$, as well as another
vector $\bc_t \in \setR^m$ called the
{\em internal memory} vector. 
 
\medskip

In particular, LSTMs utilize three {\em gate vectors}:
an input gate vector $\bkappa_{\!t} \in \setR^m$, 
a forget gate vector $\bphi_t \in \setR^m$, 
and an output gate vector $\bomega_t \in \setR^m$.
%as illustrated in 
%\cref{fig:reg:deep:lstm}, which shows the architecture of an LSTM network.
 
\medskip

Like a regular RNN, an LSTM also maintains a hidden state vector for each
time step. However, the content of the hidden vector is selectively
copied from the internal memory vector via the output gate, with the
internal memory being updated via the input gate and parts of
it forgotten via the forget gate.

\medskip

Each of the gate vectors conceptually plays a different role in an LSTM network. The
input gate vector $\bkappa_{\!t}$ controls how much of the input vector,
via the candidate update vector $\bu_t$, is allowed to
influence the memory vector $\bc_t$. 

\medskip

The forget gate vector $\bphi_t$
controls how much of the previous memory vector to forget, and finally
the output gate vector $\bomega_t$ controls how much of the memory state
is retained for the hidden state. 
\end{frame}
%
%\begin{figure}[!t]
%    \Large
%    \hspace{-0.3in}
\begin{frame}{LSTM neural network}
\framesubtitle{Recurrent connections shown in gray, gate layers shown doublelined.}
\begin{minipage}{5cm}
At each time step $t$, the three gate vectors are updated as follows:
\begin{empheq}[box=\tcbhighmath]{equation*}
    \begin{aligned}
        \bkappa_{\!t} & = \sigma\lB( \bW_{\!\kappa}^T \bx_t + \bW_{\!h\kappa}^T \bh_{t-1} +
    \bb_{\kappa} \rB)\\
    \bphi_t & = \sigma\lB( \bW_{\!\phi}^T \bx_t + \bW_{\!h\phi}^T \bh_{t-1} +
    \bb_{\phi} \rB)\\
    \bomega_t & = \sigma\lB( \bW_{\!\omega}^T \bx_t + \bW_{\!h\omega}^T \bh_{t-1} +
    \bb_{\omega} \rB)
\end{aligned}
\label{eq:reg:deep:lstm_ff_gates}
\end{empheq}
\end{minipage}
\hspace*{0.1cm}
\begin{minipage}{6cm}
\centerline{
\scalebox{0.50}{%
    \hspace{0.9in}
    \psset{tnpos=l,tnsep=2pt,colsep=1,rowsep=1,mcol=c,
    arrowscale=1.5, arrows=->}
\psmatrix
\\[-3em]
& & \TR[name=o]{\myfbox{$\bo_t$}}\\
& & \TR[name=h]{\myfbox{$\bh_t$}}\\[-0.5em]
& &
%\TR[name=ch]{\scalebox{1.5}{$\underset{\scalebox{0.5}{\fbox{tanh}}}{\large\odot}$}}\\[-0.5em]
% \TR[name=ch]{\psframebox{\scalebox{1.5}{$\odot$}}}\\[2em]
% & \Rnode[vref=0]{fc}{\psframebox{\scalebox{1.5}{$\odot$}}} &
\TR[name=ch]{\scalebox{1.5}{$\odot$}}\\[2em]
& \TR[name=fc]{\scalebox{1.5}{$\odot$}} &
\TR[name=c]{\myfbox{$\bc_t$}}\\[-0.5em]
& & \TR[name=zc]{\scalebox{1.5}{$\odot$}}\\[-0.5em]
%& & \TR[name=zc]{\psframebox{\scalebox{1.5}{$\odot$}}}\\[-0.5em]
\TR[name=i]{\myfbox[doubleline=true]{$\bkappa_{\!t}$}} & 
\TR[name=f]{\myfbox[doubleline=true]{$\bphi_t$}} &
\TR[name=z]{\myfbox{$\bu_t$}} & 
\TR[name=oo]{\myfbox[doubleline=true]{$\bomega_t$}}\\
& & \TR[name=x]{\myfbox{$\bx_t$}}
%bias to next layer
\psset{linewidth=2pt}
\ncline{h}{o}\nbput[npos=0.7]{$\bW_{\!o}, \bb_{o}$}
\ncline{c}{ch}\ncput*[npos=0.5]{tanh}
\ncline{ch}{h}
\ncline[linecolor=gray]{fc}{c}
\ncline{z}{zc}
\ncline{zc}{c}
\ncline{f}{fc}
\ncangle[angleA=90,angleB=180,armB=0,linearc=0.2]{i}{zc}
\ncangle[angleA=90,angleB=0,armB=0,linearc=0.2]{oo}{ch}
%from x
\ncangle[angleA=180,angleB=270,armB=0,linearc=0.2]{x}{i}
\naput[npos=1.2]{$\bW_{\!\kappa}, \bb_{\kappa}$}
\ncangle[angleA=180,angleB=270,armB=0,linearc=0.2]{x}{f}
\naput[npos=1.2]{$\bW_{\!\phi}, \bb_{\phi}$}
\ncangle[angleA=0,angleB=270,armB=0,linearc=0.2]{x}{oo}
\nbput[npos=1.2]{$\bW_{\!\omega}, \bb_{\omega}$}
\ncline{x}{z}
\nbput[npos=0.2]{$\bW_{\!u}, \bb_{u}$}
%from h
\psset{linewidth=2pt, linecolor=gray}
\ncangle[angleA=180,angleB=180,armB=0.75,linearc=0.2]{h}{i}
\ncput*[npos=1.1]{\scalebox{0.7}{$-1$}}
\nbput*[npos=1.2]{\scalebox{1}{$\bW_{\!h\kappa}$}}
\ncangle[angleA=180,angleB=180,armB=0.75,linearc=0.2]{h}{f}
\ncput*[npos=1.1]{\scalebox{0.7}{$-1$}}
\nbput*[npos=1.2]{\scalebox{1}{$\bW_{\!h\phi}$}}
\ncangle[angleA=180,angleB=180,armB=0.75,linearc=0.2]{h}{z}
\ncput*[npos=1.1]{\scalebox{0.7}{$-1$}}
\nbput*[npos=1.2]{\scalebox{1}{$\bW_{\!hu}$}}
\ncangle[angleA=0,angleB=0,armB=0.75,linearc=0.2]{h}{oo}
\ncput*[npos=1.1]{\scalebox{0.7}{$-1$}}
\naput*[npos=1.2]{\scalebox{1}{$\bW_{\!h\omega}$}}
%from c recurrent
\ncloop[loopsize=1.1,angleA=0,angleB=90,arm=0.6,linearc=0.2]{->}{c}{fc}%
\ncput*[npos=1.5]{\scalebox{0.7}{$-1$}}
\endpsmatrix
}}
\end{minipage}
\end{frame}
%\vspace{0.2in}
%\caption{LSTM neural network. Recurrent connections shown in gray,
%gate layers shown doublelined.}
%\label{fig:reg:deep:lstm}
%\vspace{-0.2in}
%\end{figure}
%
%Let $\cX = \tup{\bx_1, \bx_2, \cdots, \bx_\tau}$ denote a sequence
%of $d$-dimensional input vectors of length $\tau$, 
%$\cY = \tup{\by_1, \by_2, \cdots,
%\by_\tau}$
%the sequence of $p$-dimensional response vectors, 
%and $\cO = \tup{\bo_1, \bo_2,
%\cdots, \bo_\tau}$ the $p$-dimensional output sequence from
%an LSTM. 
%At each time step $t$, the three gate vectors are updated as follows:
%\begin{empheq}[box=\tcbhighmath]{equation}
    %\begin{aligned}
        %\bkappa_{\!t} & = \sigma\lB( \bW_{\!\kappa}^T \bx_t + \bW_{\!h\kappa}^T \bh_{t-1} +
    %\bb_{\kappa} \rB)\\
    %\bphi_t & = \sigma\lB( \bW_{\!\phi}^T \bx_t + \bW_{\!h\phi}^T \bh_{t-1} +
    %\bb_{\phi} \rB)\\
    %\bomega_t & = \sigma\lB( \bW_{\!\omega}^T \bx_t + \bW_{\!h\omega}^T \bh_{t-1} +
    %\bb_{\omega} \rB)
%\end{aligned}
%\label{eq:reg:deep:lstm_ff_gates}
%\end{empheq}
%Here $\sigma(\cdot)$ denotes the sigmoid activation function.
%We can observe that each gate is a function of the input vector $\bx_t$
%at time $t$, as well as the hidden state $\bh_{t-1}$ from the previous
%time step. Each gate vector has a corresponding weight matrix from the input
%neurons to the gate neurons, and from the hidden state neurons to the
%gate neurons, as well as a corresponding bias vector.

%
\begin{frame}{LSTM neural network}
Given $\bx_t$ and 
$\bh_{t-1}$, an LSTM computes $\bu_t$
after applying the $\tanh$ activation:
\begin{align*}
    \tcbhighmath{
    \bu_t  = \tanh\lB( \bW_{\!u}^T \bx_t + \bW_{\!hu}^T \bh_{t-1} +
\bb_{u} \rB)}
\end{align*}
It 
then computes the internal memory and
hidden state vectors:
\begin{empheq}[box=\tcbhighmath]{equation*}
    \begin{aligned}
    \bc_t & = \bkappa_{\!t} \odot \bu_t + \bphi_t \odot \bc_{t-1}\\
    \bh_t & = \bomega_t \odot \tanh(\bc_t)
\end{aligned}
\label{eq:reg:deep:lstm_ff}
\end{empheq}
$\bc_t$ depends on $\bu_t$ and 
$\bc_{t-1}$. However, $\bkappa_{\!t}$ controls the extent to
which $\bu_t$ influences $\bc_t$, and the forget gate $\bphi_t$ controls
how much of the previous memory is forgotten.

\medskip

$\bh_t$ depends on a tanh activated internal memory vector $\bc_{t}$, but 
$\bomega_{t}$ controls how much of the internal memory is
reflected in the hidden state. 
%%Note that, implicitly the biases elements are zero and weight matrix
%%entries are one, whenever these are not not specified. 
%Besides the input vectors $\bx_t$, the LSTM also needs an initial hidden
%state vector $\bh_0$ and an initial memory state vector $\bc_0$, both
%typically set to $\bzero \in \setR^m$.
\end{frame}

\begin{frame}{LSTM neural network}
Finally, the
output of the network $\bo_t$ is obtained by applying the output
activation function $f^o$ to an affine combination of the hidden state
neuron values:
\begin{align*}
    \bo_t & = f^o(\bW_{\!o}^T\bh_t + \bb_{o})
\end{align*}
%
LSTMs can typically handle long sequences since the net gradients for
the internal memory states do not vanish over long time steps. This is
because, by design, the memory state $\bc_{t-1}$ at time $t-1$ is linked
to the memory state $\bc_t$ at time $t$ via implicit weights fixed at 1
and biases fixed at 0, with linear activation. 

\medskip

This allows the error to flow across
time steps without vanishing or exploding.
\end{frame}
%
%
%\index{backpropagation!LSTM}
%LSTMs can be trained just like regular RNNs by unfolding the layers in
%time, as illustrated in \cref{fig:reg:deep:lstm_unfolded}, which shows the
%unfolded layers for two time steps. 
%The first step in training is to use the feed-forward steps
%%\cref{eq:reg:deep:lstm_ff_gates} and \cref{eq:reg:deep:lstm_ff} 
%to compute the error, 
%followed by the backpropagation of the gradients as a second step. 
%The latter has to be modified to accommodate the element-wise operations
%used to update the memory state $\bc_t$ and the hidden state $\bh_t$.
%The connections from $\bc_{t-1}$ to $\bc_t$ starting from $\bc_0$ to
%$\bc_\tau$, which can be thought of as using unit weight matrices and
%zero biases, appear as a straight
%line in the figure indicating that the internal memory state can flow across
%longer periods
%of time without the gradients vanishing or exploding.
%
%\begin{figure}[!t]
%    \large
%    \hspace{-0.3in}
\begin{frame}{LSTM neural network unfolded in time}
\framesubtitle{Recurrent connections in gray}
\centerline{
\scalebox{0.465}{%
    \hspace{0.9in}
    \psset{tnpos=l,tnsep=2pt,colsep=1,rowsep=1,mcol=c,
    arrowscale=1.5, arrows=->}
\psmatrix
\\[-3em]
& & & & & \TR[name=o]{\myfbox{$\bo_1$}} & & &
& & \TR[name=o2]{\myfbox{$\bo_2$}} &\\
\TR[name=h0]{\myfbox{$\bh_0$}} & & & & & \TR[name=h]{\myfbox{$\bh_1$}} & & &
& & \TR[name=h2]{\myfbox{$\bh_2$}} & & \TR[name=h3]{~}\\[-0.5em]
% & & & & & \TR[name=ch]{\psframebox{\scalebox{1.5}{$\odot$}}}& & &
% & & \TR[name=ch2]{\psframebox{\scalebox{1.5}{$\odot$}}}& \\[0.5em]
& & & & & \TR[name=ch]{\scalebox{1.5}{$\odot$}}& & &
& & \TR[name=ch2]{\scalebox{1.5}{$\odot$}}& \\[0.5em]
& & \TR[name=tmp1]{} & & & & & \TR[name=tmp2]{}\\[-0.5em]
\TR[name=c0]{\myfbox{$\bc_0$}}& & & &
%\TR[name=fc]{\psframebox{\scalebox{1.5}{$\odot$}}} 
\TR[name=fc]{\scalebox{1.5}{$\odot$}} 
& \TR[name=c]{\myfbox{$\bc_1$}} & & &
%& \TR[name=fc2]{\psframebox{\scalebox{1.5}{$\odot$}}} &
& \TR[name=fc2]{\scalebox{1.5}{$\odot$}} &
\TR[name=c2]{\myfbox{$\bc_2$}} & & \TR[name=c3]{~}\\[-0.5em]
% & & & &  & \TR[name=zc]{\psframebox{\scalebox{1.5}{$\odot$}}} & & &
%     & & \TR[name=zc2]{\psframebox{\scalebox{1.5}{$\odot$}}} &\\[-0.5em]
& & & &  & \TR[name=zc]{\scalebox{1.5}{$\odot$}} & & &
    & & \TR[name=zc2]{\scalebox{1.5}{$\odot$}} &\\[-0.5em]
& & & \TR[name=i]{\myfbox[doubleline=true]{$\bkappa_{\!1}$}} & 
\TR[name=f]{\myfbox[doubleline=true]{$\bphi_1$}}
& \TR[name=z]{\myfbox{$\bu_1$}} & 
\TR[name=oo]{\myfbox[doubleline=true]{$\bomega_1$}} & &
\TR[name=i2]{\myfbox[doubleline=true]{$\bkappa_{\!2}$}} & 
\TR[name=f2]{\myfbox[doubleline=true]{$\bphi_2$}} &
\TR[name=z2]{\myfbox{$\bu_2$}} & 
\TR[name=oo2]{\myfbox[doubleline=true]{$\bomega_2$}}\\
& & & & & \TR[name=x]{\myfbox{$\bx_1$}} & & &
& & \TR[name=x2]{\myfbox{$\bx_2$}} &
%bias to next layer
\psset{linewidth=2pt}
%internal
\ncline{h}{o}\nbput[npos=0.7]{$\bW_{\!o}, \bb_{o}$}
\ncline{c}{ch}\ncput*[npos=0.7]{tanh}
\ncline{ch}{h}
\ncline[linecolor=gray]{fc}{c}
\ncline{z}{zc}
\ncline{zc}{c}
\ncline{f}{fc}
\ncangle[angleA=90,angleB=180,armB=0,linearc=0.2]{i}{zc}
%internal2
\ncline{h2}{o2}\nbput[npos=0.7]{$\bW_{\!o}, \bb_{o}$}
\ncline{c2}{ch2}\ncput*[npos=0.7]{tanh}
\ncline{ch2}{h2}
\ncline[linecolor=gray]{fc2}{c2}
\ncline{z2}{zc2}
\ncline{zc2}{c2}
\ncline{f2}{fc2}
\ncangle[angleA=90,angleB=180,armB=0,linearc=0.2]{i2}{zc2}
%out
\ncline{c}{fc2}
%from oo
\ncangle[angleA=90,angleB=0,armB=0,linearc=0.2]{oo}{ch}
\ncangle[angleA=90,angleB=0,armB=0,linearc=0.2]{oo2}{ch2}
%from x
\ncangle[angleA=180,angleB=270,armB=0,linearc=0.2]{x}{i}
\nbput[npos=1.6]{$\bW_{\!\kappa}, \bb_{\kappa}$}
\ncangle[angleA=180,angleB=270,armB=0,linearc=0.2]{x}{f}
\nbput[npos=1.6]{$\bW_{\!\phi}, \bb_{\phi}$}
\ncangle[angleA=0,angleB=270,armB=0,linearc=0.2]{x}{oo}
\nbput[npos=1.6]{$\bW_{\!\omega}, \bb_{\omega}$}
\ncline{x}{z}
\nbput[npos=0.2]{$\bW_{\!u}, \bb_{u}$}
%from x2
\ncangle[angleA=180,angleB=270,armB=0,linearc=0.2]{x2}{i2}
\nbput[npos=1.6]{$\bW_{\!\kappa}, \bb_{\kappa}$}
\ncangle[angleA=180,angleB=270,armB=0,linearc=0.2]{x2}{f2}
\nbput[npos=1.6]{$\bW_{\!\phi}, \bb_{\phi}$}
\ncangle[angleA=0,angleB=270,armB=0,linearc=0.2]{x2}{oo2}
\nbput[npos=1.6]{$\bW_{\!\omega}, \bb_{\omega}$}
\ncline{x2}{z2}
\nbput[npos=0.2]{$\bW_{\!u}, \bb_{u}$}
\psset{linewidth=2pt, linecolor=gray}
%from h0
\ncangle[angleA=0,angleB=180,armB=0.75,linearc=0.2]{-}{h0}{tmp1}
\ncangle[angleA=0,angleB=180,armB=0.75,linearc=0.2]{tmp1}{i}
\nbput*[npos=1.1]{\scalebox{1}{$\bW_{\!h\kappa}$}}
\ncangle[angleA=0,angleB=180,armB=0.75,linearc=0.2]{tmp1}{f}
\nbput*[npos=1.1]{\scalebox{1}{$\bW_{\!h\phi}$}}
\ncangle[angleA=0,angleB=180,armB=0.75,linearc=0.2]{tmp1}{z}
\nbput*[npos=1.1]{\scalebox{1}{$\bW_{\!hu}$}}
\ncangle[angleA=0,angleB=180,armB=0.75,linearc=0.2]{->}{tmp1}{oo}
\naput*[npos=1.1]{\scalebox{1}{$\bW_{\!h\omega}$}}
%from h
\ncangle[angleA=0,angleB=180,armB=0.75,linearc=0.2]{-}{h}{tmp2}
\ncangle[angleA=0,angleB=180,armB=0.75,linearc=0.2]{tmp2}{i2}
\nbput*[npos=1.1]{\scalebox{1}{$\bW_{\!h\kappa}$}}
\ncangle[angleA=0,angleB=180,armB=0.75,linearc=0.2]{tmp2}{f2}
\nbput*[npos=1.1]{\scalebox{1}{$\bW_{\!h\phi}$}}
\ncangle[angleA=0,angleB=180,armB=0.75,linearc=0.2]{tmp2}{z2}
\nbput*[npos=1.1]{\scalebox{1}{$\bW_{\!hu}$}}
\ncangle[angleA=0,angleB=180,armB=0.75,linearc=0.2]{->}{tmp2}{oo2}
\naput*[npos=1.1]{\scalebox{1}{$\bW_{\!h\omega}$}}
%from c recurrent
\ncline{c0}{fc}%
\ncline{c}{fc2}%
\ncline{c2}{c3}
\ncline{h2}{h3}
%tmp nodes
\endpsmatrix
}}
\end{frame}
%\vspace{0.2in}
%\caption{LSTM neural network unfolded in time (recurrent connections in
%gray).}
%\label{fig:reg:deep:lstm_unfolded}
%\end{figure}
%
%\subsection{Training LSTMs}
\begin{frame}{Training LSTMs}
%Consider the unfolded LSTM in \cref{fig:reg:deep:lstm_unfolded}.
During backpropagation the net gradient vector at the output layer at
time $t$ is computed by considering the
partial derivatives of the activation function, $\partial\bff^{\;o}_t$,
and the error function, $\partial\bcE_{\bx_t}$, as follows:
\begin{align*}
    \bdelta^o_t = \partial\bff^{\;o}_t \odot \partial\bcE_{\bx_t}
\end{align*}
where we assume that the output neurons are independent.
%
%In backpropagation there are two incoming connections to
%the internal memory vector $\bc_t$, one from $\bh_t$ and the other from
%$\bc_{t+1}$.
%Therefore, the net gradient $\delta_{ti}^c$ 
%at the internal memory neuron $i$ at time $t$ is given as
%\begin{align*}
%    \delta_{ti}^c = 
%    \frac{\partial \cE_\bx}{\partial \net^{\;c}_{ti}} & = 
%    \frac{\partial \cE_\bx}{\partial \net^{\;h}_{ti}} \cdot
%    \frac{\partial \net^{\;h}_{ti}}{\partial c_{ti}} \cdot
%    \frac{\partial c_{ti}}{\partial \net^{\;c}_{ti}} + 
%    \frac{\partial \cE_\bx}{\partial \net^{\;c}_{t+1,i}} \cdot
%    \frac{\partial \net^{\;c}_{t+1,i}}{\partial c_{ti}} \cdot
%    \frac{\partial c_{ti}}{\partial \net^{\;c}_{ti}}\\
%& = \delta^h_{ti} \cdot \omega_{ti}(1 - c_{ti}^2)  +
%    \delta^c_{t+1,i} \cdot \phi_{t+1,i}
%\end{align*}
%where 
%    we use the fact that the internal memory vector implicitly uses an
%    identity activation function, and furthermore
%\begin{align*}
%    \frac{\partial \net^{\;h}_{ti}}{\partial c_{ti}} & = 
%     \frac{\partial}{\partial
%     c_{ti}} \lB\{ \omega_{ti} \cdot  \tanh(c_{ti})\rB\} =
%     \omega_{ti} (1-c_{ti}^2)\\
%     \frac{\partial \net^{\;c}_{t+1,i}}{\partial c_{ti}} & = 
%     \frac{\partial}{\partial
%    c_{ti}} \lB\{ \kappa_{t+1,i} \cdot u_{t+1,i} +  \phi_{t+1,i} \cdot
%    c_{ti}\rB\} = \phi_{t+1,i}
%\end{align*}
%
%% First consider the element-wise, net activation 
%% $\bh_t = \bomega_t \odot \tanh(\bc_t)$. We have
%% \begin{align*}
%%     \frac{\partial \bh_t}{\partial \bc_t} = \bomega_t \odot (\bone -
%%     \bc_t \odot \bc_t)
%% \end{align*}
%% Next, from the $\bc_{t+1} \odot \bphi_{t+1}$ term, we have
%% \begin{align*}
%%     \frac{\partial \bc_{t+1}\odot \bphi_{t+1}}{\partial \bc_t} = 
%%     \bphi_{t+1}
%% \end{align*}
The net gradient vector $\bdelta^c_t$ at $\bc_t$ is therefore given as:
\begin{align*}
    \bdelta_t^c = \bdelta_t^h \odot \bomega_t \odot (\bone - \bc_t
    \odot \bc_t) \; +\; \bdelta_{t+1}^c \odot \bphi_{t+1}
\end{align*}
%
%The forget gate has only one incoming edge in backpropagation, from
%$\bc_t$, via the element-wise multiplication $\bphi_t \odot
%\bc_{t-1}$, with sigmoid activation, therefore the net gradient is: 
%\begin{align*}
%    \delta^\phi_{ti} = 
%    \frac{\partial \cE_\bx}{\partial \net^{\;\phi}_{ti}} & = 
%    \frac{\partial \cE_\bx}{\partial \net^{\;c}_{ti}} \cdot
%    \frac{\partial \net^{\;c}_{ti}}{\partial \phi_{ti}} \cdot
%    \frac{\partial \phi_{ti}}{\partial \net^{\;\phi}_{ti}}
%    = \delta^c_{ti} \cdot c_{t-1,i} \cdot \phi_{ti} (1 - \phi_{ti})
%\end{align*}
%where we used the fact that the forget gate uses sigmoid activation and
%\begin{align*}
%    \frac{\partial \net^{\;c}_{ti}}{\partial \phi_{ti}} & = 
%     \frac{\partial}{\partial
%    \phi_{ti}} \lB\{ \kappa_{ti} \cdot u_{ti} +  \phi_{ti} \cdot
%    c_{t-1,i}\rB\} = c_{t-1,i}
%\end{align*}
Across all forget gate neurons, the net gradient vector is therefore
given as
\begin{align*}
    \bdelta_t^\phi = \bdelta_t^c \odot \bc_{t-1} \odot  (\bone - \bphi_t) \odot \bphi_t
\end{align*}

The input gate also has only one incoming edge in backpropagation, from
$\bc_t$, via the element-wise multiplication $\bkappa_{\!t} \odot
\bu_{t}$, with sigmoid activation. In a similar manner, as outlined above
for $\bdelta_t^\phi$, the net gradient $\bdelta_t^\kappa$ at the input
gate $\bkappa_{\!t}$ is:
\begin{align*}
    \bdelta_t^\kappa = \bdelta_t^c \odot \bu_t \odot  (\bone -
    \bkappa_{\!t}) \odot \bkappa_{\!t}
\end{align*}
\end{frame}

\begin{frame}{Training LSTMs}
The same reasoning applies to the update candidate $\bu_t$, which also has an
incoming edge from $\bc_t$ via $\bkappa_{\!t} \odot \bu_t$ and tanh
activation, so the net gradient vector $\bdelta_t^u$ at the update layer
is
\begin{align*}
    \bdelta_t^u = \bdelta_t^c \odot \bkappa_{\!t} \odot (\bone - \bu_t \odot
    \bu_t)
\end{align*}

Likewise, in backpropagation, there is one incoming connection to the
output gate from $\bh_t$ via $\bomega_t \odot \tanh(\bc_t)$ with
sigmoid activation, therefore
\begin{align*}
    \bdelta_t^\omega = \bdelta_t^h \odot \tanh(\bc_t) \odot (\bone - \bomega_t) \odot \bomega_t
\end{align*}

Finally, to compute the net gradients at the hidden layer, note that 
gradients flow back to $\bh_{t}$ from the
following layers: $\bu_{t+1}, \bkappa_{\!t+1}, \bphi_{t+1},
\bomega_{t+1}$ and $\bo_t$. Therefore, 
the net gradient vector at the hidden state vector $\bdelta_t^h$ is given as
    \begin{align*}
        \bdelta_{\!t}^h = \bW_{\!o} \bdelta_t^o 
        + \bW_{\!h\kappa} \bdelta_{t+1}^\kappa
        + \bW_{\!h\phi} \bdelta_{t+1}^\phi
        + \bW_{\!h\omega} \bdelta_{t+1}^\omega
        + \bW_{\!hu} \bdelta_{t+1}^u
    \end{align*}
\end{frame}

%The gradients for the weight matrix and bias vector at the output
%    layer are given as:
%    \begin{align*}
%    \bgrad_{\bb_{o}} & = \sum^{\tau}_{t=1} \bdelta^o_t &
%    \bgrad_{\bW_{\!o}} & = 
%    \sum^{\tau}_{t=1} \bh_t \cdot \lB(\bdelta^o_t\rB)^T
%\end{align*}
%Likewise, the gradients for the weight matrices and bias vectors for the
%other layers are given as
%    follows:
%\begin{align*}
%    \scalebox{1}{$\bgrad_{\bb_{\kappa}}$} & =
%    \scalebox{1}{$\sum^{\tau}_{t=1} \bdelta_t^\kappa$} &
%    \scalebox{1}{$\bgrad_{\bW_{\!\kappa}}$} & =
%    \scalebox{1}{$\sum^{\tau}_{t=1} \bx_t \cdot
%    \lB(\bdelta_t^\kappa\rB)^T$} &
%    \scalebox{1}{$\bgrad_{\bW_{\!h\kappa}}$} & =
%    \scalebox{1}{$\sum^{\tau}_{t=1} \bh_{t-1} \cdot
%    \lB(\bdelta_t^\kappa\rB)^T$}\\
%%
%    \scalebox{1}{$\bgrad_{\bb_{\phi}}$} & =
%    \scalebox{1}{$\sum^{\tau}_{t=1} \bdelta_t^\phi$} &
%    \scalebox{1}{$\bgrad_{\bW_{\!\phi}}$} & =
%    \scalebox{1}{$\sum^{\tau}_{t=1} \bx_t \cdot
%    \lB(\bdelta_t^\phi\rB)^T$} &
%    \scalebox{1}{$\bgrad_{\bW_{\!h\phi}}$} & =
%    \scalebox{1}{$\sum^{\tau}_{t=1} \bh_{t-1} \cdot
%    \lB(\bdelta_t^\phi\rB)^T$}\\
%%
%    \scalebox{1}{$\bgrad_{\bb_{\omega}}$} & =
%    \scalebox{1}{$\sum^{\tau}_{t=1} \bdelta_t^\omega$} &
%    \scalebox{1}{$\bgrad_{\bW_{\!\omega}}$} & =
%    \scalebox{1}{$\sum^{\tau}_{t=1} \bx_t \cdot
%    \lB(\bdelta_t^\omega\rB)^T$} &
%    \scalebox{1}{$\bgrad_{\bW_{\!h\omega}}$} & =
%    \scalebox{1}{$\sum^{\tau}_{t=1} \bh_{t-1} \cdot
%    \lB(\bdelta_t^\omega\rB)^T$} \\
% %   
%    \scalebox{1}{$\bgrad_{\bb_{u}}$} & =
%    \scalebox{1}{$\sum^{\tau}_{t=1} \bdelta_t^u$} &
%    \scalebox{1}{$\bgrad_{\bW_{\!u}}$} & =
%    \scalebox{1}{$\sum^{\tau}_{t=1} \bx_t \cdot
%    \lB(\bdelta_t^u\rB)^T$} &
%    \scalebox{1}{$\bgrad_{\bW_{\!hu}}$} & =
%    \scalebox{1}{$\sum^{\tau}_{t=1} \bh_{t-1} \cdot
%    \lB(\bdelta_t^u\rB)^T$}
%\end{align*}
%
%Given these gradients, we can 
%%compute the gradients for all the weight
%%matrices and bias vectors in a manner similar to that outlined for a
%%regular RNN in \cref{sec:reg:deep:rnn_bpt}. Likewise, 
%use the stochastic
%gradient descent approach to train the network.
%
%
%\begin{figure}[!t]
%    \large
%\vspace{-0.3in}
	\begin{frame}{LSTM}
		\framesubtitle{Example -- Embedded Reber grammar automata}

    We use an LSTM to learn the embedded Reber grammar, which is generated
    according to the automata.% shown in \cref{fig:reg:deep:embedded_reber}. 
    This automata has two copies of the Reber automata% from

	\medskip

% \tikzsetnextfilename{reg_deep_embedded_reber}
\special{ps: /XC@. {0 setgray } def }
\centering
\scalebox{0.55}{
    \tikzset{initial text={}}
\begin{tikzpicture}[->,>=Stealth,shorten >=1pt,auto,%
    node distance=2.25cm and 2.5cm, semithick]
  \tikzstyle{every state}=[fill=lightgray]

  % \node[initial,state] (s) {$s$};
  \node[state] (ss) {$s_0$};
  \node[state] (s) [right of=ss] {$s_1$};
  \node[state, draw=none, fill=none] (s1p) [above right of=s] {\phantom{$sp1$}};
  \node[state, draw=none, fill=none] (s2p) [below right of=s] {\phantom{$sp2$}};
  % \node[initial,state] (ss) {$s_0$};
  % \node[state] (s) [above right of=ss] {$s_1$};
  % \node[state] (s1) [above right of=s] {$t_0$};
  \node[state] (s1) [above right of=s1p] {$t_0$};
  \node[state] (B1) [right of=s1] {$t_1$};
  \node[state] (BT1) [above right of=B1] {$t_2$};
  \node[state] (BP1) [below right of=B1] {$t_3$};
  \node[state] (BTX1) [right of=BT1] {$t_4$};
  \node[state] (BPV1) [right of=BP1] {$t_5$};
  \node[state] (BTXS1) [below right of=BTX1] {$t_6$};
  \node[state] (e1) [right of=BTXS1] {$t_7$};
  \node[state,draw=none, fill=none] (e1p) [below right of=e1]
  {\phantom{$ep1$}};
  % \node[state] (s2) [below right of=s] {$p_0$};
  \node[state] (s2) [below right of=s2p] {$p_0$};
  \node[state] (B2) [right of=s2] {$p_1$};
  \node[state] (BT2) [above right of=B2] {$p_2$};
  \node[state] (BP2) [below right of=B2] {$p_3$};
  \node[state] (BTX2) [right of=BT2] {$p_4$};
  \node[state] (BPV2) [right of=BP2] {$p_5$};
  \node[state] (BTXS2) [below right of=BTX2] {$p_6$};
  \node[state] (e2) [right of=BTXS2] {$p_7$};
  \node[state] (e) [below right of=e1p] {$e_0$};
  \node[state] (ee) [right of=e] {$e_1$};

  \path 
  (ss) edge node {\texttt{B}} (s)
  (s) edge node {\texttt{T}} (s1)
  (s) edge[below] node {\texttt{P}} (s2)
  %
  (s1) edge[above] node {\texttt{B}} (B1)
  (B1) edge[above] node {\texttt{T}} (BT1)
  edge[below] node {\texttt{P}} (BP1)
  (BT1) edge[above] node {\texttt{X}} (BTX1)
  edge [loop above]  node {\texttt{S}} (BT1)
  (BP1) edge[below] node {\texttt{V}} (BPV1)
  edge [loop below]  node {\texttt{T}} (BP1)
  (BTX1) edge[above] node {\texttt{S}} (BTXS1)
  edge[below] node {\texttt{X}} (BP1)
  (BPV1) edge[below] node {\texttt{V}} (BTXS1)
  edge node {\texttt{P}} (BTX1)
  (BTXS1) edge node {\texttt{E}} (e1)
  %
  (s2) edge[above] node {\texttt{B}} (B2)
  (B2) edge[above] node {\texttt{T}} (BT2)
  edge[below] node {\texttt{P}} (BP2)
  (BT2) edge[above] node {\texttt{X}} (BTX2)
  edge [loop above]  node {\texttt{S}} (BT2)
  (BP2) edge[below] node {\texttt{V}} (BPV2)
  edge [loop below]  node {\texttt{T}} (BP2)
  (BTX2) edge[above] node {\texttt{S}} (BTXS2)
  edge[below] node {\texttt{X}} (BP2)
  (BPV2) edge[below] node {\texttt{V}} (BTXS2)
  edge node {\texttt{P}} (BTX2)
  (BTXS2) edge node {\texttt{E}} (e2)
  %
  (e1) edge node {\texttt{T}} (e)
  (e2) edge[below] node {\texttt{P}} (e)
  (e) edge[above] node {\texttt{E}} (ee);
\end{tikzpicture}
}
\end{frame}
% \vspace{0.05in}
%\caption{Embedded Reber grammar automata.}
%\label{fig:reg:deep:embedded_reber}
%\vspace{-0.1in}
%\end{figure}
%
%%see rnn-output.txt for results
%% \vspace{-0.3in}
\begin{frame}{LSTM}
\framesubtitle{Example -- Embedded Reber grammar automata}
%\begin{example}[LSTM]
%    \cref{ex:reg:deep:rnn}. 

\medskip

From the state $s_1$, the top automata
    is reached by following the edge labeled \texttt{T}, whereas the
    bottom automata is reached via the edge labeled \texttt{P}. 
    The states of the top automata are labeled as $t_0, t_1,\cdots,t_7$,
    whereas the states of the bottom automata are labeled as $p_0, p_1,
    \cdots, p_7$. Finally, note that the state $e_0$ can be reached
    from either the top or the bottom automata by following the edges
    labeled \texttt{T} and \texttt{P}, respectively. 

\medskip

    The first symbol is always $\texttt{B}$ and the last symbol is
    always $\texttt{E}$. However, the important point is
    that the second symbol is always the same as the second last symbol,
    and thus any sequence learning model has to learn this long range
    dependency. For example, the following is a valid embedded Reber
    sequence:
    $S_\cX = \tup{\texttt{B}, \texttt{T}, \texttt{B}, \texttt{T}, 
        \texttt{S},
        \texttt{S}, \texttt{X}, \texttt{X}, \texttt{T}, \texttt{V},
    \texttt{V}, \texttt{E}, \texttt{T}, \texttt{E}}$.

	\medskip

    The task of the LSTM is to learn to predict the next symbol for each
    of the positions in a given embedded Reber sequence. 
\end{frame}

\begin{frame}{LSTM}
\framesubtitle{Example -- Embedded Reber grammar automata}
    
    For training, we
    generate $n=400$ embedded Reber sequences with a minimum length of
    40, and convert them into
    training pairs $(\cX, \cY)$ using the binary encoding.% described in
%    \cref{ex:reg:deep:rnn}. 
    The maximum sequence length is $\tau=64$. 

\medskip
    
    Given the long range dependency, we used an LSTM with $m=20$ hidden
    neurons (smaller values of $m$ either need more epochs to
    learn, or have trouble learning the grammar).
    The input and output layer sizes are
    determined by the dimensionality of encoding, namely $d= 7$ and
    $p=7$. We use sigmoid activation at the output layer, treating
    each neuron as independent. 

\medskip

Finally, we use  the binary cross
    entropy error function. The LSTM is trained for $r=10000$ epochs
    (using step size $\eta= 1$ and batch size 400); it 
    learns the training data perfectly, making no errors in the
    prediction of the set of possible next symbols. 
\end{frame}

\begin{frame}{LSTM}
\framesubtitle{Example -- Embedded Reber grammar automata}
%
%    % \enlargethispage{6pt}
    We test the LSTM model on $100$ previously unseen embedded Reber
    sequences (with minimum length 40, as before). The trained LSTM makes no
    errors on the test sequences. In particular, it is able to learn the
    long range dependency between the second symbol and the second last
    symbol, which must always match.

\medskip
    
    The embedded Reber grammar was chosen since an RNN has trouble
    learning the long range dependency. 
    Using an RNN with $m=60$ hidden neurons, using $r=25000$ epochs
    with a step size of $\eta=1$, the RNN can perfectly learn the training
    sequences. That is, it makes no errors on any of the 400 training
    sequences. 

\medskip

However, on the test data, this RNN makes a mistake in
    40
    out of the 100 test sequences. In fact, in each of these test
    sequences it makes exactly one error; it fails
    to correctly predict the second last symbol. 

\medskip

These results suggest that
    while the RNN is able to ``memorize'' the long range dependency in
    the training data, it is not able to generalize completely on unseen
    test sequences.
\end{frame}
%    % \label{ex:reg:deep:lstm}
%\end{example}
%
%
%\section{Convolutional Neural Networks}
\ifdefined\wox \begin{frame} \titlepage \end{frame} \fi

\begin{frame}{Convolutional Neural Networks}
%\label{sec:reg:deep:cnn}
%\index{convolutional neural networks}
%\index{deep learning!convolutional neural networks}
%\index{deep learning!CNN}
%\index{CNN|see{convolutional neural networks}}
A convolutional neural network (CNN) is essentially a {\em localized}
and sparse
feedforward MLP that is designed to exploit spatial and/or temporal 
structure in the
input data. 

\medskip

In a regular MLP all of the neurons in layer $l$ are
connected to all of the neurons in layer $l+1$. 

\medskip

In contrast, a CNN 
connects a contiguous or adjacent subset of neurons in layer
$l$ to a single neuron in the next layer $l+1$. 

\medskip

Different sliding
windows comprising contiguous subsets of neurons in layer $l$ connect to
different neurons in layer $l+1$. 

\medskip

Furthermore, all of these sliding
windows use
{\em parameter sharing}, that is, the same set of weights, called a {\em
filter}, is used for
all sliding windows. Finally, different filters are used to
automatically extract features from layer $l$ for use
by layer $l+1$.
\end{frame}

%\index{convolutional neural networks!parameter sharing}
%\index{parameter sharing}
%
%\subsection{Convolutions}
%\index{convolution}
%
%We begin by defining the convolution operation for one-way, two-way and
%three-way inputs. By one-way we mean data in the form of a single
%vector, by two-way we mean data in the form of a matrix, and by
%three-way we mean data in the form of a tensor. We also call
%them 1D, 2D or 3D inputs where the dimensionality refers to the number
%of axes in the input data.
%We will discuss the convolution operation in the context of the input
%layer and the first hidden layer, but the same approach can be applied
%to subsequent layers in the network.
%
%\index{convolution!1D}
%\subsubsection{1D Convolution}
\begin{frame}{1D Convolution}
%Let $\bx = (x_1, x_2, \cdots, x_n)^T$ be an input vector (a one-way or
%1D input) with
%$n$ points. It is assumed that the input points $x_i$ are not
%independent, but rather, there are dependencies between successive
%points. 
%Let $\bw  = (w_1, w_2, \cdots, w_k)^T$ be a vector of
%weights, called a {\em 1D filter}, with $k \le n$. Here $k$ is also called
%the {\em window size}.
%Let $\bx_k(i)$ denote the window of $\bx$ of length $k$ starting at
%position $i$, given as
%\begin{align*}
%    \bx_k(i) = \bigl(x_i, x_{i+1}, x_{i+2}, \cdots, x_{i+k-1}\bigr)^T
%\end{align*}
%with $1 \le i \le n-k+1$.
Given a vector $\ba \in \setR^k$, define the summation operator as one
that adds all the elements of the vector. That is,
\begin{align*}
    \summ(\ba) = \sum_{i=1}^k a_i
\end{align*}

A {\em 1D convolution} between $\bx$ and $\bw$, denoted by the asterisk symbol
$\ast$, is:
\begin{equation*}
    \bx \ast \bw = \matr{&&\\[-1em]
    \summ\bigl(\bx_k(1) \odot \bw\bigr) &
        \cdots &
        \summ\bigl(\bx_k({n-k+1}) \odot \bw\bigr)\\[-1em]
&&}^T
\end{equation*}
where $\odot$ is the element-wise product, so that
\begin{align*}
    \tcbhighmath{
    \summ\bigl(\bx_k(i) \odot \bw\bigr) = \sum_{j=1}^k x_{i+j-1} \cdot
w_j }
\end{align*}
for $i=1,2,\cdots,n-k+1$. 
The convolution of $\bx \in \setR^n$ and $\bw \in
\setR^k$ results in a vector of length $n-k+1$.
\end{frame}
%
%
%% \begin{align*}
%%     \begin{blockarray}{c|c|c|c|c|}
%%         \cline{2-5}
%%     \bx \ast \bw = &
%%     \summ\bigl(\bx_k(1) \odot \bw\bigr) &
%%         \summ\bigl(\bx_k(2) \odot \bw\bigr) &
%%         \cdots &
%%     \summ\bigl(\bx_k({n-k+1}) \odot \bw\bigr)\\
%%     \cline{2-5}
%% \end{blockarray}
%% \end{align*}
%% \begin{align*}
%%     \resizebox{0.8\hsize}{!}{
%%     \begin{blockarray}{c|c|c|c|c|}
%%         \cline{2-5}
%%     $\bx \ast \bw =$ &
%%     $\summ\bigl(\bx_k(1) \odot \bw\bigr)$ &
%%         $\summ\bigl(\bx_k(2) \odot \bw\bigr)$ &
%%         $\cdots$ &
%%     $\summ\bigl(\bx_k({n-k+1}) \odot \bw\bigr)$\\
%%     \cline{2-5}
%% \end{blockarray}
%% }
%% \end{align*}
%
%\begin{figure}[t!]
%    \vspace*{-0.2in}
\begin{frame}{1D Convolution}
Given $\bx$ with $n=7$ and a
	filter $\bw = (1,0,2)^T$ ($k=3$). The first window of
    $\bx$ is $\bx_3(1) = (1, 3, -1)^T$.% Therefore, %as seen in
%    \cref{fig:reg:deep:1dconv:a}, 
%we have
    \begin{align*}
        \summ\lB(\bx_3(1) \odot \bw\rB) = \summ\lB(
        (1,3,-1)^T \odot (1,0,2)^T \rB) = 
        \summ\lB( (1,0,-2)^T\rB)  = -1
    \end{align*}
    % \looseness=-1\relax
%    The convolution steps for different sliding windows of $\bx$ with the
%    filter $\bw$ are shown in  the figure.
%    \cref{fig:reg:deep:1dconv}\protect\subref{fig:reg:deep:1dconv:a}--\protect\subref{fig:reg:deep:1dconv:e}.
    The convolution $\bx \ast \bw$ has size $n-k+1 = 7-3+1 = 5$, and is
    given as 
    % \begin{align*}
    \[    \bx \ast \bw = (-1, 7, 5, 4 ,-1)^T \]
    % \end{align*}

	\medskip

    \centering
    \centerline{
%        \subfloat[\scriptsize $\summ(\bx_3(1)\odot\bw)$]{
%    \label{fig:reg:deep:1dconv:a}
    \scalebox{0.55}{
        \scantokens{
    % \tikzsetnextfilename{reg_deep_1dconv_a}
\begin{tikzpicture}[every node/.style={draw, minimum size=0.75cm}]
    \matrix [column sep=1mm, minimum height=6.2cm]
{
    \node[fill=lightgray, thick] (x) {1}; & & \node (w) {1};\\
    \node[fill=lightgray, thick] {3}; & \node[draw=none]{\scalebox{1.3}{$\ast$}}; &
    \node {0}; & \node[draw=none]{$=$}; & 
    \node [fill=gray, very thick] {-1};\\
    \node[fill=lightgray, thick] {-1}; & & \node {2}; & & \node{ };\\
    \node {2}; & & & & \node{ };\\
    \node{3};& & & & \node{ };\\
    \node {1};& & & &\node{ };\\
    \node {-2};\\
};
\node[draw=none] at ([yshift={2mm}]w.90) {$\bw$};
\node[draw=none] at ([yshift={2mm}]x.90) {$\bx$};
\end{tikzpicture}\endinput}}%}
%% \hspace{2mm}
%    \subfloat[\scriptsize $\summ(\bx_3(2)\odot\bw)$]{
        \scalebox{0.55}{
        \scantokens{
    % \tikzsetnextfilename{reg_deep_1dconv_b}
\begin{tikzpicture}[every node/.style={draw, minimum size=0.75cm}]
    \matrix [column sep=1mm, minimum height=6.2cm]
{
    \node (x) {1};\\
    \node[fill=lightgray, thick] {3}; & & \node (w) {1};& & \node{-1}; \\
    \node[fill=lightgray, thick] {-1}; & 
    \node[draw=none]{\scalebox{1.3}{$\ast$}}; & \node {0}; & 
    \node[draw=none]{=}; &
    \node [fill=gray, very thick] {7};\\
    \node[fill=lightgray, thick] {2}; & & \node {2}; & & \node{ };\\
    \node{3}; & & & & \node{ };\\
    \node {1}; & & & &  \node{ };\\
    \node {-2};\\
};
\node[draw=none] at ([yshift={2mm}]w.90) {$\bw$};
\node[draw=none] at ([yshift={2mm}]x.90) {$\bx$};
\end{tikzpicture}\endinput}}%}
%% \hspace{2mm}
%    \subfloat[\scriptsize $\summ(\bx_3(3)\odot\bw)$]{
        \scalebox{0.55}{
        \scantokens{
    % \tikzsetnextfilename{reg_deep_1dconv_c}
\begin{tikzpicture}[every node/.style={draw, minimum size=0.75cm}]
    \matrix [column sep=1mm, minimum height=6.2cm]
{
    \node (x) {1};\\
    \node {3}; & & & & \node{-1};\\
    \node[fill=lightgray, thick] {-1}; & & \node (w) {1};& &\node{7};\\
    \node[fill=lightgray, thick] {2}; & 
    \node[draw=none]{\scalebox{1.3}{$\ast$}}; & \node {0}; & 
    \node[draw=none]{=}; &
    \node [fill=gray, very thick] {5};\\
    \node[fill=lightgray, thick]{3};& & \node {2}; & & \node{ };\\
    \node {1}; & & & & \node{ };\\
    \node {-2};\\
};
\node[draw=none] at ([yshift={2mm}]w.90) {$\bw$};
\node[draw=none] at ([yshift={2mm}]x.90) {$\bx$};
\end{tikzpicture}\endinput}}%}
%% }
%% \centerline{
%% \hspace{2mm}
%    \subfloat[\scriptsize $\summ(\bx_3(4)\odot\bw)$]{
        \scalebox{0.55}{
        \scantokens{
    % \tikzsetnextfilename{reg_deep_1dconv_d}
\begin{tikzpicture}[every node/.style={draw, minimum size=0.75cm}]
    \matrix [column sep=1mm, minimum height=6.2cm]
{
    \node (x) {1};\\
    \node {3}; & & & & \node{-1};\\
    \node {-1}; & & & & \node{7};\\
    \node[fill=lightgray, thick] {2}; & & \node (w) {1}; & & \node{5};\\
    \node[fill=lightgray, thick]{3};& 
    \node[draw=none]{\scalebox{1.3}{$\ast$}}; & \node {0}; & 
    \node[draw=none]{=}; &
    \node [fill=gray, very thick] {4};\\
    \node [fill=lightgray, thick]{1};& & \node {2}; & & \node{ };\\
    \node {-2};\\
};%
\node[draw=none] at ([yshift={2mm}]w.90) {$\bw$};
\node[draw=none] at ([yshift={2mm}]x.90) {$\bx$};
\end{tikzpicture}\endinput}}%}
%% \hspace{2mm}
%    \subfloat[\scriptsize $\summ(\bx_3(5)\odot\bw)$]{
%    \label{fig:reg:deep:1dconv:e}
        \scalebox{0.55}{
        \scantokens{
    % \tikzsetnextfilename{reg_deep_1dconv_e}
\begin{tikzpicture}[every node/.style={draw, minimum size=0.75cm}]
    \matrix [column sep=1mm, minimum height=6.2cm]
{
    \node (x) {1};\\
    \node {3};  & & & & \node{-1};\\
    \node {-1};  & & & &\node{7};\\
    \node {2}; & & & &\node{5};\\
    \node[fill=lightgray, thick]{3};& & \node (w) {1}; & &\node{4};\\
    \node [fill=lightgray, thick]{1};& 
    \node[draw=none]{\scalebox{1.3}{$\ast$}}; & \node {0}; & 
    \node[draw=none]{=}; &
    \node [fill=gray, very thick] {-1};\\
    \node[fill=lightgray, thick] {-2};& &\node {2};\\
};%
\node[draw=none] at ([yshift={2mm}]w.90) {$\bw$};
\node[draw=none] at ([yshift={2mm}]x.90) {$\bx$};
\end{tikzpicture}\endinput}}}
%% \hspace{2mm}
%%     \subfloat[$\bx \ast \bw$]{
%%     \label{fig:reg:deep:1dconv:conv}
%%         \scalebox{0.75}{
%%         \scantokens{
%% \begin{tikzpicture}[every node/.style={draw, minimum size=0.75cm}]
%%     \matrix [column sep=1mm, minimum height=6.2cm]
%% {
%%     \node (x){1};\\
%%     \node {3};  & & & & \node[fill=gray]{-1};\\
%%     \node {-1}; & & \node (w) {1}; & & \node[fill=gray]{7};\\
%%     \node {2}; & [5mm] \node[draw=none]{\scalebox{1.3}{$\ast$}}; & \node {0}; &
%%     \node[draw=none]{$=$}; &
%%     \node[fill=gray]{5}; \\
%%     \node{3};& & \node {2}; & & \node[fill=gray]{4};\\
%%     \node {1}; & &  & & \node[fill=gray]{-1};\\
%%     \node {-2};\\
%% };%
%% \node[draw=none] at ([yshift={2mm}]w.90) {$\bw$};
%% \node[draw=none] at ([yshift={2mm}]x.90) {$\bx$};
%% \end{tikzpicture}\endinput}}}
\end{frame}
%}
%\vspace{0.2in}
%\caption{1D Convolution:
%\protect\subref{fig:reg:deep:1dconv:a}--\protect\subref{fig:reg:deep:1dconv:e}
%show the convolution between different sliding windows of
%$\bx$ and the filter $\bw$ (with window size $k=3$). The final convolution output is shown in
%\protect\subref{fig:reg:deep:1dconv:e}.}
%\label{fig:reg:deep:1dconv}
%    \vspace*{-0.2in}
%\end{figure}
%
%% \enlargethispage{1\baselineskip}
%\begin{example}[1D Convolution]
%\begin{frame}{1D Convolution}
%\framesubtitle{Example}
%%    % \enlargethispage*{2cm}
%%    \cref{fig:reg:deep:1dconv} 
%Figure shows a vector $\bx$ with $n=7$ and a
    %filter $\bw = (1,0,2)^T$ with window size $k=3$. The first window of
    %$\bx$ of size 3 is $\bx_3(1) = (1, 3, -1)^T$. Therefore, %as seen in
%%    \cref{fig:reg:deep:1dconv:a}, 
%we have
    %\begin{align*}
        %\summ\lB(\bx_3(1) \odot \bw\rB) = \summ\lB(
        %(1,3,-1)^T \odot (1,0,2)^T \rB) = 
        %\summ\lB( (1,0,-2)^T\rB)  = -1
    %\end{align*}
    %% \looseness=-1\relax
    %The convolution steps for different sliding windows of $\bx$ with the
    %filter $\bw$ are shown in  the figure.
%%    \cref{fig:reg:deep:1dconv}\protect\subref{fig:reg:deep:1dconv:a}--\protect\subref{fig:reg:deep:1dconv:e}.
    %The convolution $\bx \ast \bw$ has size $n-k+1 = 7-3+1 = 5$, and is
    %given as 
    %% \begin{align*}
    %\[    \bx \ast \bw = (-1, 7, 5, 4 ,-1)^T \]
    %% \end{align*}
%\end{frame}
%\end{example}
%
%% \enlargethispage{2\baselineskip}
%
%% \vspace*{-0.2in}
%\index{convolution!2D}
%\subsubsection{2D Convolution}
%We can extend the convolution operation to matrix input, for example for
%images. Let $\bX$ be an $n\times n$ input matrix, and let $\bW$ be a $k
%\times k$ matrix of weights, called a {\em 2D filter}, with $k \le n$. 
%Here $k$ is called the window size.
%Let $\bX_k(i,j)$ denote the $k \times k$ submatrix of $\bX$ 
%starting at row $i$ and column $j$, defined as
%follows:
%\begin{align*}
%    \bX_k(i,j) = \matr{&&&\\[-1em]
%        x_{i,j} &  x_{i, j+1} & \cdots & x_{i, j+k-1}\\
%        x_{i+1,j} &  x_{i+1, j+1} & \cdots & x_{i+1, j+k-1}\\
%        \vdots & \vdots & \cdots & \vdots\\
%    x_{i+k-1,j} &  x_{i+k-1, j+1} & \cdots & x_{i+k-1, j+k-1}\\[-1em]
%   &&&}
%\end{align*}
%with $1 \le i, j \le n-k+1$.
\begin{frame}{2D Convolution}
Given a $k \times k$ matrix $\bA \in \setR^{k \times k}$, 
define the summation operator as one
that adds all the elements of the matrix. That is,
\begin{align*}
    \summ(\bA) = \sum_{i=1}^k \sum_{j=1}^k a_{i,j}
\end{align*}
%where $a_{i,j}$ is the element of $\bA$ at row $i$ and column $j$.
The {\em 2D convolution} of $\bX$ and $\bW$, denoted $\bX \ast
\bW$, is defined as:
% \resizegathersetup{enable}
%\begin{empheq}[box=\tcbhighmath]{gather}
%    \scalebox{0.8}{\parbox{\linewidth}{\[

\begin{footnotesize}
\begin{equation*}
        \bX \ast \bW = \matr{ && \\[-1em]
        \summ\bigl(\bX_k(1,1) \odot \bW\bigr) &
        \cdots & \summ\bigl(\bX_k(1, n-k+1) \odot \bW\bigr)\\
        %\summ\bigl(\bX_k(2,1) \odot \bW\bigr) &
        %\cdots & \summ \bigl(\bX_k(2, n-k+1) \odot \bW\bigr)\\
        \vdots & \cdots & \vdots\\
        \summ\bigl(\bX_k(n-k+1,1) \odot \bW\bigr) &
        \cdots & 
        \summ \bigl(\bX_k(n-k+1, n-k+1) \odot \bW\bigr)\\[-1em]
&&}
\end{equation*}
\end{footnotesize}
where $\odot$ is the
element-wise product of $\bX_k(i,j)$ and $\bW$, so that
\begin{align*}
    \tcbhighmath{
    \summ\bigl(\bX_k(i,j) \odot \bW\bigr) = \sum_{a=1}^k \sum_{b=1}^k x_{i+a-1, j+b-1}
\cdot w_{a, b}}
\end{align*}
for $i, j = 1,2,\cdots,n-k+1$.
The convolution of $\bX \in \setR^{n\times n}$ and
$\bW \in \setR^{k \times k}$ results in a $(n-k+1) \times (n-k+1)$ matrix.
\end{frame}
%
%% \]}}
%% \end{empheq}
%% \resizegathersetup{disable}
%% \begin{align*}
%%     \bX \ast \bW = 
%%     \resizebox{0.9\hsize}{!}{
%%         \begin{blockarray}{cccc}
%%             \begin{block}{|c|c|c|c|}
%%             \hline
%%         $\summ\bigl(\bX_k(1,1) \odot \bW\bigr)$ &
%%         $\summ\bigl(\bX_k(1,2)\odot \bW\bigr)$ &
%%         $\cdots$ & $\summ\bigl(\bX_k(1, n-k+1) \odot \bW\bigr)$\\
%%             \hline
%%         $\summ\bigl(\bX_k(2,1) \odot \bW\bigr)$ &
%%         $\summ\bigl(\bX_k(2,2)\odot \bW\bigr)$ &
%%         $\cdots$ & $\summ \bigl(\bX_k(2, n-k+1) \odot \bW\bigr)$\\
%%             \hline
%%         $\vdots$ & $\vdots$ & $\cdots$ & $\vdots$\\
%%             \hline
%%         $\summ\bigl(\bX_k(n-k+1,1) \odot \bW\bigr)$ &
%%         $\summ\bigl(\bX_k(n-k+1,2)\odot \bW\bigr)$ &
%%         $\cdots$ & 
%%         $\summ \bigl(\bX_k(n-k+1, n-k+1) \odot \bW\bigr)$\\
%%             \hline
%%     \end{block}
%%     \end{blockarray}
%% }%
%% \end{align*}
%% \begin{align*}
%%     \bX \ast \bW = 
%%     \resizebox{0.9\hsize}{!}{$
%%         \large
%%     \matr{
%%         &&&\\[-1em]
%%         \summ\bigl(\bX_k(1,1) \odot \bW\bigr) &
%%         \summ\bigl(\bX_k(1,2)\odot \bW\bigr) &
%%         \cdots & \summ\bigl(\bX_k(1, n-k+1) \odot \bW\bigr)\\
%%         \summ\bigl(\bX_k(2,1) \odot \bW\bigr) &
%%         \summ\bigl(\bX_k(2,2)\odot \bW\bigr) &
%%         \cdots & \summ \bigl(\bX_k(2, n-k+1) \odot \bW\bigr)\\
%%         \vdots & \vdots & \cdots & \vdots\\
%%         \summ\bigl(\bX_k(n-k+1,1) \odot \bW\bigr) &
%%         \summ\bigl(\bX_k(n-k+1,2)\odot \bW\bigr) &
%%         \cdots & \summ \bigl(\bX_k(n-k+1, n-k+1) \odot
%%         \bW\bigr)\\
%%         &&&\\[-1em]
%% }%
%% $}
%% \end{align*}
%
%
%
%\begin{figure}[t!]
\begin{frame}{2D Convolution}
\framesubtitle{Example}
%    \cref{fig:reg:deep:2dconv} 
Given  matrix $\bX$ with $n=4$ and a
    filter $\bW$ with window size $k=2$. The convolution of the 
    first window of $\bX$, namely $\bX_2(1,1)$, with $\bW$
    is given as: % (see \cref{fig:reg:deep:2dconv:a})
    \begin{align*}
        \summ\bigl(\bX_2(1,1) \odot \bW\bigr) = 
        \summ\lB(
            \matr{1 & 2\\3 & 1} \odot \matr{1 & 0\\0 & 1}
        \rB) = 
        \summ\lB( \matr{1 & 0\\0& 1}\rB)  = 2
    \end{align*}
    The convolution steps for different $2\times 2$ 
    sliding windows of $\bX$ with the
    filter $\bW$ are shown.
%    \cref{fig:reg:deep:2dconv}\protect\subref{fig:reg:deep:2dconv:a}--\protect\subref{fig:reg:deep:2dconv:i}.

	\medskip

    The convolution $\bX \ast \bW$ has size $3 \times 3$, since 
    $n-k+1 = 4-2+1 = 3$, and is
    given as 
    % \begin{align*}
\[        \bX \ast \bW = \matr{2 & 6 & 4\\
            4 & 4 & 8\\
            4 & 4 & 4
    } \]
    % \end{align*}
\end{frame}
\begin{frame}{2D Convolution}
\centering
    \tikzset{empty node/.style={draw=none,fill=none}}
    \centerline{
%        \subfloat[$\summ(\bX_2(1,1)\odot\bW)$]{
    \scalebox{0.45}{\LARGE
%        \label{fig:reg:deep:2dconv:a}
        \scantokens{
    % \tikzsetnextfilename{reg_deep_2dconv_a}
            \begin{tikzpicture}[every node/.style={draw, minimum size=0.75cm}]
    \matrix [column sep=0mm, minimum height=5cm]
{
    \node[fill=lightgray, thick] {1}; & \node[fill=lightgray, thick] (x) {2}; & \node{2}; & \node{1};\\
    \node[fill=lightgray, thick] {3}; & \node[fill=lightgray, thick]{1}; & \node{4}; & \node{2};
    & \node[draw=none]{ }; & \node (w){1}; & \node{0}; 
    & & \node[fill=gray, thick]{2}; & \node{ }; & \node{ };\\
\node {2}; & \node{1}; & \node{3}; & \node (n4){4}; 
    & 
    & \node{0}; & \node (w1) {1}; 
    & \node[draw=none]{ };
    & \node{ }; & \node{ }; & \node{ };\\
    \node {1}; & \node{2}; & \node{3}; & \node{1}; 
    & & & & & \node{ }; & \node{ }; & \node{ };\\
};
\node[draw=none] at ([yshift={4mm}]w.45) {$\bW$};
\node[draw=none] at ([yshift={4mm}]x.60) {$\bX$};
\node[draw=none] at ([xshift={4mm},yshift={4mm}]n4.0) {\scalebox{1.3}{$\ast$}}; 
\node[draw=none] at ([xshift={4mm},yshift={4mm}]w1.0) {\scalebox{1}{$=$}}; 
\end{tikzpicture}
\endinput}}%}
%%\hspace{2mm}
%        \subfloat[$\summ(\bX_2(1,2)\odot\bW)$]{
%        \label{fig:reg:deep:2dconv:b}
    \scalebox{0.45}{\LARGE
        \scantokens{
    % \tikzsetnextfilename{reg_deep_2dconv_b}
\begin{tikzpicture}[every node/.style={draw, minimum size=0.75cm}]
    \matrix [column sep=0mm, minimum height=5cm]
{
    \node {1}; & \node[fill=lightgray, thick] (x) {2}; & \node[fill=lightgray, thick]{2}; & \node{1};\\
    \node {3}; & \node[fill=lightgray, thick]{1}; & \node[fill=lightgray, thick]{4}; & \node{2}; 
    & &  \node (w){1}; & \node{0}; 
    & & \node{2}; & \node[fill=gray, thick]{6}; & \node{ };\\
    \node {2}; & \node{1}; & \node{3}; & \node (n4) {4}; 
    & \node[draw=none]{ };
    & \node{0}; & \node (w1) {1}; 
    & \node[draw=none]{ };
    & \node{ }; & \node{ }; & \node{ };\\
    \node {1}; & \node{2}; & \node{3}; & \node{1}; 
    & & & & & \node{ }; & \node{ }; & \node{ };\\
};
\node[draw=none] at ([yshift={4mm}]w.45) {$\bW$};
\node[draw=none] at ([yshift={4mm}]x.60) {$\bX$};
\node[draw=none] at ([xshift={4mm},yshift={4mm}]n4.0) {\scalebox{1.3}{$\ast$}}; 
\node[draw=none] at ([xshift={4mm},yshift={4mm}]w1.0) {\scalebox{1}{$=$}}; 
\end{tikzpicture}
\endinput}}%}
%%\hspace{2mm}
%        \subfloat[$\summ(\bX_2(1,3)\odot\bW)$]{
%        \label{fig:reg:deep:2dconv:c}
    \scalebox{0.45}{\LARGE
        \scantokens{
    % \tikzsetnextfilename{reg_deep_2dconv_c}
\begin{tikzpicture}[every node/.style={draw, minimum size=0.75cm}]
    \matrix [column sep=0mm, minimum height=5cm]
{
    \node {1}; & \node (x) {2}; & \node[fill=lightgray, thick]{2}; & \node[fill=lightgray, thick]{1};\\
    \node {3}; & \node{1}; & \node[fill=lightgray, thick]{4}; & \node[fill=lightgray, thick]{2}; 
    & &  \node (w){1}; & \node{0}; 
    & & \node{2}; & \node{6}; & \node[fill=gray, thick]{4};\\
    \node {2}; & \node{1}; & \node{3}; & \node (n4){4}; 
    & \node[draw=none]{ }; 
    & \node{0}; & \node (w1) {1}; 
    & \node[draw=none]{ }; 
    & \node{ }; & \node{ }; & \node{ };\\
    \node {1}; & \node{2}; & \node{3}; & \node{1}; 
    & & & & & \node{ }; & \node{ }; & \node{ };\\
};
\node[draw=none] at ([yshift={4mm}]w.45) {$\bW$};
\node[draw=none] at ([yshift={4mm}]x.60) {$\bX$};
\node[draw=none] at ([xshift={4mm},yshift={4mm}]n4.0) {\scalebox{1.3}{$\ast$}}; 
\node[draw=none] at ([xshift={4mm},yshift={4mm}]w1.0) {\scalebox{1}{$=$}}; 
\end{tikzpicture}
\endinput}}%}
}
    \centerline{
%        \subfloat[$\summ(\bX_2(2,1)\odot\bW)$]{
    \scalebox{0.45}{\LARGE
%        \label{fig:reg:deep:2dconv:d}
        \scantokens{
    % \tikzsetnextfilename{reg_deep_2dconv_d}
\begin{tikzpicture}[every node/.style={draw, minimum size=0.75cm}]
    \matrix [column sep=0mm, minimum height=5cm]
{
    \node {1}; & \node (x) {2}; & \node{2}; & \node{1};\\
    \node[fill=lightgray, thick] {3}; & \node[fill=lightgray, thick]{1}; & \node{4}; & \node{2}; 
    & &  \node (w){1}; & \node{0}; 
    & & \node{2}; & \node{6}; & \node{4};\\
    \node[fill=lightgray, thick] {2}; & \node[fill=lightgray, thick]{1};
    & \node{3}; & \node (n4){4}; 
    & \node[draw=none]{ }; 
    & \node{0}; & \node (w1) {1}; 
    & \node[draw=none]{ }; 
    & \node[fill=gray, thick]{4}; & \node{ }; & \node{ };\\
    \node {1}; & \node{2}; & \node{3}; & \node{1}; 
    & & & & & \node{ }; & \node{ }; & \node{ };\\
};
\node[draw=none] at ([yshift={4mm}]w.45) {$\bW$};
\node[draw=none] at ([yshift={4mm}]x.60) {$\bX$};
\node[draw=none] at ([xshift={4mm},yshift={4mm}]n4.0) {\scalebox{1.3}{$\ast$}}; 
\node[draw=none] at ([xshift={4mm},yshift={4mm}]w1.0) {\scalebox{1}{$=$}}; 
\end{tikzpicture}
\endinput}}%}
%%\hspace{2mm}
%        \subfloat[$\summ(\bX_2(2,2)\odot\bW)$]{
%        \label{fig:reg:deep:2dconv:e}
    \scalebox{0.45}{\LARGE
        \scantokens{
    % \tikzsetnextfilename{reg_deep_2dconv_e}
\begin{tikzpicture}[every node/.style={draw, minimum size=0.75cm}]
    \matrix [column sep=0mm, minimum height=5cm]
{
    \node {1}; & \node (x) {2}; & \node{2}; & \node{1};\\
    \node {3}; & \node[fill=lightgray, thick]{1}; & \node[fill=lightgray, thick]{4}; & \node{2}; 
    & &  \node (w){1}; & \node{0}; 
    & & \node{2}; & \node{6}; & \node{4};\\
    \node {2}; & \node[fill=lightgray, thick]{1}; &
    \node[fill=lightgray, thick]{3}; & \node (n4) {4}; 
    & \node[draw=none]{ }; 
    & \node{0}; & \node (w1) {1}; 
    & \node[draw=none]{ }; 
    & \node{4}; & \node[fill=gray, thick]{4}; & \node{ };\\
    \node {1}; & \node{2}; & \node{3}; & \node{1}; 
    & & & & & \node{ }; & \node{ }; & \node{ };\\
};
\node[draw=none] at ([yshift={4mm}]w.45) {$\bW$};
\node[draw=none] at ([yshift={4mm}]x.60) {$\bX$};
\node[draw=none] at ([xshift={4mm},yshift={4mm}]n4.0) {\scalebox{1.3}{$\ast$}}; 
\node[draw=none] at ([xshift={4mm},yshift={4mm}]w1.0) {\scalebox{1}{$=$}}; 
\end{tikzpicture}
\endinput}}%}
%%\hspace{2mm}
%        \subfloat[$\summ(\bX_2(2,3)\odot\bW)$]{
%        \label{fig:reg:deep:2dconv:f}
    \scalebox{0.45}{\LARGE
        \scantokens{
    % \tikzsetnextfilename{reg_deep_2dconv_f}
\begin{tikzpicture}[every node/.style={draw, minimum size=0.75cm}]
    \matrix [column sep=0mm, minimum height=5cm]
{
    \node {1}; & \node (x) {2}; & \node{2}; & \node{1};\\
    \node {3}; & \node{1}; & \node[fill=lightgray, thick]{4}; & \node[fill=lightgray, thick]{2}; 
    & &  \node (w){1}; & \node{0}; 
    & & \node{2}; & \node{6}; & \node{4};\\
    \node {2}; & \node{1}; & \node[fill=lightgray, thick]{3}; &
    \node[fill=lightgray, thick] (n4) {4}; 
    & \node[draw=none]{ }; 
    & \node{0}; & \node (w1){1}; 
    & \node[draw=none]{ }; 
    & \node{4}; & \node{4}; & \node[fill=gray, thick]{8};\\
    \node {1}; & \node{2}; & \node{3}; & \node{1}; 
    & & & & & \node{ }; & \node{ }; & \node{ };\\
};
\node[draw=none] at ([yshift={4mm}]w.45) {$\bW$};
\node[draw=none] at ([yshift={4mm}]x.60) {$\bX$};
\node[draw=none] at ([xshift={4mm},yshift={4mm}]n4.0) {\scalebox{1.3}{$\ast$}}; 
\node[draw=none] at ([xshift={4mm},yshift={4mm}]w1.0) {\scalebox{1}{$=$}}; 
\end{tikzpicture}
\endinput}}}
%}
    \centerline{
%        \subfloat[$\summ(\bX_2(3,1)\odot\bW)$]{
    \scalebox{0.45}{\LARGE
%        \label{fig:reg:deep:2dconv:g}
        \scantokens{
    % \tikzsetnextfilename{reg_deep_2dconv_g}
\begin{tikzpicture}[every node/.style={draw, minimum size=0.75cm}]
    \matrix [column sep=0mm, minimum height=5cm]
{
    \node {1}; & \node (x) {2}; & \node{2}; & \node{1};\\
    \node {3}; & \node{1}; & \node{4}; & \node{2}; 
    & &  \node (w){1}; & \node{0}; 
    & & \node{2}; & \node{6}; & \node{4};\\
    \node[fill=lightgray, thick] {2}; & \node[fill=lightgray, thick]{1};
    & \node{3}; & \node (n4) {4}; 
    & \node[draw=none]{ }; 
    & \node{0}; & \node (w1){1}; 
    & \node[draw=none]{ }; 
    & \node{4}; & \node{4}; & \node{8};\\
    \node[fill=lightgray, thick] {1}; & \node[fill=lightgray, thick]{2}; & \node{3}; & \node{1}; 
    & & & & & \node[fill=gray, thick]{4}; & \node{ }; & \node{ };\\
};
\node[draw=none] at ([yshift={4mm}]w.45) {$\bW$};
\node[draw=none] at ([yshift={4mm}]x.60) {$\bX$};
\node[draw=none] at ([xshift={4mm},yshift={4mm}]n4.0) {\scalebox{1.3}{$\ast$}}; 
\node[draw=none] at ([xshift={4mm},yshift={4mm}]w1.0) {\scalebox{1}{$=$}}; 
\end{tikzpicture}
\endinput}}%}
%%\hspace{2mm}
%        \subfloat[$\summ(\bX_2(3,2)\odot\bW)$]{
%        \label{fig:reg:deep:2dconv:h}
    \scalebox{0.45}{\LARGE
        \scantokens{
    % \tikzsetnextfilename{reg_deep_2dconv_h}
\begin{tikzpicture}[every node/.style={draw, minimum size=0.75cm}]
    \matrix [column sep=0mm, minimum height=5cm]
{
    \node {1}; & \node (x) {2}; & \node{2}; & \node{1};\\
    \node {3}; & \node{1}; & \node{4}; & \node{2}; 
    & &  \node (w){1}; & \node{0}; 
    & & \node{2}; & \node{6}; & \node{4};\\
    \node {2}; & \node[fill=lightgray, thick]{1}; &
    \node[fill=lightgray, thick]{3}; & \node (n4) {4}; 
    & \node[draw=none]{ }; 
    & \node{0}; & \node (w1) {1}; 
    & \node[draw=none]{ }; 
    & \node{4}; & \node{4}; & \node{8};\\
    \node {1}; & \node[fill=lightgray, thick]{2}; & \node[fill=lightgray, thick]{3}; & \node{1}; 
    & & & & & \node{4}; & \node[fill=gray, thick]{4}; & \node{ };\\
};
\node[draw=none] at ([yshift={4mm}]w.45) {$\bW$};
\node[draw=none] at ([yshift={4mm}]x.60) {$\bX$};
\node[draw=none] at ([xshift={4mm},yshift={4mm}]n4.0) {\scalebox{1.3}{$\ast$}}; 
\node[draw=none] at ([xshift={4mm},yshift={4mm}]w1.0) {\scalebox{1}{$=$}}; 
\end{tikzpicture}
\endinput}}%}
%%\hspace{2mm}
%        \subfloat[$\summ(\bX_2(3,3)\odot\bW)$]{
%        \label{fig:reg:deep:2dconv:i}
    \scalebox{0.45}{\LARGE
        \scantokens{
    % \tikzsetnextfilename{reg_deep_2dconv_i}
\begin{tikzpicture}[every node/.style={draw, minimum size=0.75cm}]
    \matrix [column sep=0mm, minimum height=5cm]
{
    \node {1}; & \node (x) {2}; & \node{2}; & \node{1};\\
    \node {3}; & \node{1}; & \node{4}; & \node{2}; 
    & &  \node (w){1}; & \node{0}; 
    & & \node{2}; & \node{6}; & \node{4};\\
    \node {2}; & \node{1}; & \node[fill=lightgray, thick]{3}; &
    \node[fill=lightgray, thick] (n4) {4}; 
    & \node[draw=none]{ }; 
    & \node{0}; & \node (w1){1}; 
    & \node[draw=none]{ }; 
    & \node{4}; & \node{4}; & \node{8};\\
    \node {1}; & \node{2}; & \node[fill=lightgray, thick]{3}; & \node[fill=lightgray, thick]{1}; 
    & & & & & \node{4}; & \node{4}; & \node[fill=gray, thick]{4};\\
};
\node[draw=none] at ([yshift={4mm}]w.45) {$\bW$};
\node[draw=none] at ([yshift={4mm}]x.60) {$\bX$};
\node[draw=none] at ([xshift={4mm},yshift={4mm}]n4.0) {\scalebox{1.3}{$\ast$}}; 
\node[draw=none] at ([xshift={4mm},yshift={4mm}]w1.0) {\scalebox{1}{$=$}}; 
\end{tikzpicture}
\endinput}}}
\end{frame}
%}
%\vspace{0.2in}
%\caption{2D Convolution:
%\protect\subref{fig:reg:deep:2dconv:a}--\protect\subref{fig:reg:deep:2dconv:i}
%show the 2D convolution between different $2 \times 2$ sliding windows of
%$\bX$ and the filter $\bW$. The final 2D convolution output is shown in
%\protect\subref{fig:reg:deep:2dconv:i}.}
%\label{fig:reg:deep:2dconv}
%\end{figure}
%
%\enlargethispage{1\baselineskip}
%\begin{example}[2D Convolution]
%\end{example}
%
%
%\index{convolution!3D}
%\subsubsection{3D Convolution}
%We now extend the convolution operation to 
%a three-dimensional matrix, which is also called a 
%{\em 3D tensor}.
%The first dimension comprises the rows, the
%second the columns, and the third the {\em channels}. 
%Let $\bX$ be an $n\times n \times m$ tensor, with $n$ rows, $n$ columns
%and $m$ channels.
%The assumption
%is that the input $\bX$ is a collection of $n \times n$ matrices obtained by
%applying $m$ {\em filters}, which specify the $m$ channels. 
%For example, for $n \times n$ image inputs, each channel
%may correspond to a different color filter --- red, green or blue. 
%
\begin{frame}{3D Convolution}
Let $\bW$ be a $k \times k \times r$ tensor of weights, called a {\em 3D
filter}, with $k \le n$ and $r \le m$. 
Let $\bX_k(i,j,q)$ denote the $k \times k \times r$ subtensor of $\bX$ 
starting at row $i$, column
$j$ and channel $q$, as illustrated in the figure, %\cref{fig:deep:cnn:Xijq},
with $1 \le i, j \le n-k+1$, and $1 \le q \le m-r+1$.

\medskip

Given a $k \times k \times r$ tensor $\bA \in \setR^{k \times k \times r}$, 
define the summation operator as one
that adds all the elements of the tensor. That is,
\begin{align*}
    \summ(\bA) = \sum_{i=1}^k \sum_{j=1}^k \sum^{r}_{q=1} a_{i,j,q}
\end{align*}
where $a_{i,j,q}$ is the element of $\bA$ at row $i$, column $j$, and
channel $q$.
\begin{align*}
    \tcbhighmath{
    \summ\bigl(\bX_k(i,j,q) \odot \bW\bigr) = \sum_{a=1}^k \sum_{b=1}^k \sum_{c=1}^r
x_{i+a-1, j+b-1, q+c-1} \cdot w_{a, b, c} }
\end{align*}
for $i, j = 1,2,\cdots,n-k+1$ and $q = 1,2,\cdots,m-r+1$.

\medskip

The convolution of $\bX$ and
$\bW$ results in a $(n-k+1) \times
(n-k+1) \times (m-r+1)$ tensor.

\end{frame}

\begin{frame}{3D Convolution}
The {\em 3D convolution} of $\bX$ and $\bW$, denoted $\bX \ast
\bW$, is defined as:

\medskip

\centerline{
\begin{footnotesize}
   \scalebox{0.85}{\parbox{\linewidth}{\[
               \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!
    \bX \ast \bW =  
    \begin{array}{ccccc}
            \ldelim({4}{*} & \summ\bigl(\bX_k(1,1,1) \odot \bW\bigr) &
            \cdots & 
            \summ\bigl(\bX_k(1, n-k+1,1) \odot \bW\bigr) & \rdelim){4}{*}\\
            & \summ\bigl(\bX_k(2,1,1) \odot \bW\bigr) &
            \cdots & 
            \summ \bigl(\bX_k(2, n-k+1,1) \odot \bW\bigr) &\\
        & \vdots & 
        \cdots & 
        \vdots & \\
        &\summ\bigl(\bX_k(n-k+1,1,1) \odot \bW\bigr) &
        \cdots & 
        \summ \bigl(\bX_k(n-k+1, n-k+1,1) \odot \bW\bigr)&\\
        \cline{2-4}
        %
        \ldelim({4}{*} & \summ\bigl(\bX_k(1,1,2) \odot \bW\bigr) &
            \cdots & \summ\bigl(\bX_k(1, n-k+1,2) \odot
        \bW\bigr) & \rdelim){4}{*}\\
           &  \summ\bigl(\bX_k(2,1,2) \odot \bW\bigr) &
            \cdots & \summ \bigl(\bX_k(2, n-k+1,2) \odot \bW\bigr) &\\
       & \vdots & \cdots & \vdots &\\
        &\summ\bigl(\bX_k(n-k+1,1,2) \odot \bW\bigr) &
        \cdots & \summ \bigl(\bX_k(n-k+1, n-k+1,2) \odot
        \bW\bigr)& \\
        \cline{2-4}
        & \vdots & 
        \vdots & 
        \vdots & \\
        %
            \ldelim({4}{*} &\summ\bigl(\bX_k(1,1,m-r+1) \odot \bW\bigr) &
            \cdots & 
            \summ\bigl(\bX_k(1, n-k+1,m-r+1) \odot
        \bW\bigr) & \rdelim){4}{*}\\
            & \summ\bigl(\bX_k(2,1,m-r+1) \odot \bW\bigr) &
            \cdots & 
            \summ \bigl(\bX_k(2, n-k+1,m-r+1) \odot \bW\bigr)& \\
        & \vdots & 
        \cdots & 
        \vdots & \\
        & \summ\bigl(\bX_k(n-k+1,1,m-r+1) \odot \bW\bigr) &
        \cdots & 
        \summ \bigl(\bX_k(n-k+1, n-k+1,m-r+1) \odot \bW\bigr) &\\
  \end{array}
  \]}}
\end{footnotesize}
}
\noindent where $\odot$ is the
element-wise product of $\bX_k(i,j,q)$ and $\bW$.
\end{frame}

%\begin{frame}{3D Convolution}
%
%\noindent where $\odot$ is the
%element-wise product of $\bX_k(i,j,q)$ and $\bW$, so that
%\begin{align}
    %\tcbhighmath{
    %\summ\bigl(\bX_k(i,j,q) \odot \bW\bigr) = \sum_{a=1}^k \sum_{b=1}^k \sum_{c=1}^r
%x_{i+a-1, j+b-1, q+c-1} \cdot w_{a, b, c} }
%\end{align}
%for $i, j = 1,2,\cdots,n-k+1$ and $q = 1,2,\cdots,m-r+1$.
%We can see that the convolution of $\bX \in \setR^{n\times n \times
%m}$ and
%$\bW \in \setR^{k \times k \times r}$ results in a $(n-k+1) \times
%(n-k+1) \times (m-r+1)$ tensor.
%\end{frame}

%\begin{align*}
%    \bX_k(i,j,q) = \quad
% %   \begin{pmatrix}
%        \begin{blockarray}{cccc}
%\begin{block}{[cccc]}
%    &&&\\
%        x_{i,j,q} &  x_{i, j+1,q} & \cdots & x_{i, j+k-1,q}\\
%        x_{i+1,j,q} &  x_{i+1, j+1,q} & \cdots & x_{i+1, j+k-1,q}\\
%        \vdots & \vdots & \cdots & \vdots\\
%    x_{i+k-1,j,q} &  x_{i+k-1, j+1,q} & \cdots & x_{i+k-1, j+k-1,q}\\
%    &&&\\
%\end{block}
%    \begin{block}{[cccc]}
%    &&&\\
%        x_{i,j,q+1} &  x_{i, j+1,q+1} & \cdots & x_{i, j+k-1,q+1}\\
%        x_{i+1,j,q+1} &  x_{i+1, j+1,q+1} & \cdots & x_{i+1, j+k-1,q+1}\\
%        \vdots & \vdots & \cdots & \vdots\\
%    x_{i+k-1,j,q+1} &  x_{i+k-1, j+1,q+1} & \cdots & x_{i+k-1, j+k-1,q+1}\\
%    &&&\\
%\end{block}
%\begin{block}{[cccc]}
%    &&&\\
%    \vdots & \vdots & \cdots & \vdots\\
%    &&&\\
%\end{block}
%\begin{block}{[cccc]}
%    &&&\\
%        x_{i,j,q+r-1} &  x_{i, j+1,q+r-1} & \cdots & x_{i, j+k-1,q+r-1}\\
%        x_{i+1,j,q+r-1} &  x_{i+1, j+1,q+r-1} & \cdots & x_{i+1,
%        j+k-1,q+r-1}\\
%        \vdots & \vdots & \cdots & \vdots\\
%    x_{i+k-1,j,q+r-1} &  x_{i+k-1, j+1, q+r-1} & \cdots & x_{i+k-1,
%j+k-1,q+r-1}\\
%    &&&\\
%\end{block}
%    \end{blockarray}
%%\end{pmatrix}
%\end{align*}
% with $1 \le i, j \le n-k+1$, and $1 \le q \le m-r+1$.

%%\begin{figure}[ht!]
%%    \scalebox{0.95}{%
%%    \psset{unit=0.25in}
%%    %\psset{viewpoint=2 1.5 1,subgriddiv=0,gridlabels=0}
%%    %\psset{viewpoint=60 50 20 rtp2xyz,Decran=50}
%%    \psset{Alpha=50,Beta=35}
%%    \centering
%%    \begin{pspicture}(-1,-6.25)(8,7.5)
%%    %\pstThreeDCoor[linecolor=black]
%%    %\ThreeDput[normal=1 0 0](0,0,0)
%%    \pstPlanePut[plane=yz](0,0,0){
%%    \begin{tabular}{|c|c|c|c|}
%%        \hline
%%        $x_{i,j,q}$ &  $x_{i, j+1,q}$ & $\cdots$ & $x_{i, j+k-1,q}$\\
%%        \hline
%%        $x_{i+1,j,q}$ &  $x_{i+1, j+1,q}$ & $\cdots$ & $x_{i+1, j+k-1,q}$\\
%%        \hline
%%        $\vdots$ & $\vdots$ & $\cdots$ & $\vdots$\\
%%        \hline
%%    $x_{i+k-1,j,q}$ &  $x_{i+k-1, j+1,q}$ & $\cdots$ & $x_{i+k-1,
%%    j+k-1,q}$\\
%%        \hline
%%    \end{tabular}
%%    }
%%    \pstPlanePut[plane=yz](-5,-1,0){
%%    %\ThreeDput[normal=1 0 0](-5,0,0)
%%    \begin{tabular}{|c|c|c|c|}
%%        \hline
%%        $x_{i,j,q+1}$ &  $x_{i, j+1,q+1}$ & $\cdots$ & $x_{i, j+k-1,q+1}$\\
%%        \hline
%%        $x_{i+1,j,q+1}$ &  $x_{i+1, j+1,q+1}$ & $\cdots$ & $x_{i+1,
%%        j+k-1,q+1}$\\
%%        \hline
%%        $\vdots$ & $\vdots$ & $\cdots$ & $\vdots$\\
%%        \hline
%%    $x_{i+k-1,j,q+1}$ &  $x_{i+k-1, j+1,q+1}$ & $\cdots$ & $x_{i+k-1,
%%    j+k-1,q+1}$\\
%%        \hline
%%\end{tabular}
%%    }
%%    \pstPlanePut[plane=xy,planecorr=normal](-8,8,0){\scalebox{1.5}{$\ldots$}}
%%    \pstPlanePut[plane=yz](-12,-2,0){
%%    %\ThreeDput[normal=1 0 0](-10,0,0)
%%    \begin{tabular}{|c|c|c|c|}
%%        \hline
%%        $x_{i,j,q+r-1}$ &  $x_{i, j+1,q+r-1}$ & $\cdots$ & $x_{i,
%%        j+k-1,q+r-1}$\\
%%        \hline
%%        $x_{i+1,j,q+r-1}$ &  $x_{i+1, j+1,q+r-1}$ & $\cdots$ & 
%%        $x_{i+1, j+k-1,q+r-1}$\\
%%        \hline
%%        $\vdots$ & $\vdots$ & $\cdots$ & $\vdots$\\
%%        \hline
%%    $x_{i+k-1,j,q+r-1}$ &  $x_{i+k-1, j+1, q+r-1}$ & $\cdots$ & 
%%    $x_{i+k-1,j+k-1,q+r-1}$\\
%%        \hline
%%\end{tabular}
%%    }
%%    \pstThreeDLine[arrows=<-](3,1.75,0)(3,1.75,5)%i
%%    \pstPlanePut[plane=yz,planecorr=normal](3,1.25,2.5){$i$}
%%    \pstThreeDLine[arrows=->](3,2,0)(3,15.5,0)%j
%%    \pstPlanePut[plane=yz,planecorr=normal](3.75,8.5,0){$j$}
%%    \pstThreeDLine[arrows=->](3,16,0)(-9.5,16,0)%q
%%    \pstPlanePut[plane=yz,planecorr=normal](-3,16.5,0){$q$}
%%    % \pstThreeDLine[arrows=->](3,15.5,0)(-10,15.5,-2)%q
%%    % \pstPlanePut[plane=yz,planecorr=normal](-3,17,0){$q$}
%%    \end{pspicture}
%%    }
%%\vspace{0.2in}
%%\caption{3D subtensor $\bX_k(i, j, q)$: $k \times k \times r$ subtensor
%%of $\bX$ starting at row $i$, column $j$ and channel $q$.}
%%    \label{fig:deep:cnn:Xijq}
%%\end{figure}
%
%\begin{figure}[t!]
%    \vspace{-0.1in}
\begin{frame}{3D subtensor $\bX_k(i, j, q)$}

$k \times k \times r$ subtensor of $\bX$ starting at row $i$, column $j$, and channel $q$.

\medskip

    \centering
    \large
%    \hspace{0.1in}
    \scalebox{0.7}{
        \scantokens{
    % \tikzsetnextfilename{reg_deep_cnn_Xijk}
            \begin{tikzpicture}[every node/.style={anchor=north
                east,minimum width=1.4cm,minimum height=7mm,fill=white}]
        \matrix (mA) [draw,matrix of math nodes]
        {
        x_{i,j,q+r-1}& x_{i, j+1,q+r-1} & \cdots & x_{i,
       j+k-1,q+r-1}\\
        x_{i+1,j,q+r-1} &x_{i+1, j+1,q+r-1} & \cdots & 
        x_{i+1, j+k-1,q+r-1}\\
        \vdots & \vdots & \cdots & \vdots\\
    x_{i+k-1,j,q+r-1} &  x_{i+k-1, j+1, q+r-1} & \cdots & 
    x_{i+k-1,j+k-1,q+r-1}\\
        };
        \matrix (mB) [draw,matrix of math nodes] at ($(mA.south
        west)+(7.5,1)$)
        {
        x_{i,j,q+1} &  x_{i, j+1,q+1} & \cdots & x_{i, j+k-1,q+1}\\
        x_{i+1,j,q+1} &  x_{i+1, j+1,q+1} & \cdots & x_{i+1, j+k-1,q+1}\\
        \vdots & \vdots & \cdots & \vdots\\
    x_{i+k-1,j,q+1} &  x_{i+k-1, j+1,q+1} & \cdots & x_{i+k-1,
   j+k-1,q+1}\\
        };
        \matrix (mC) [draw,matrix of math nodes] at ($(mB.south
        west)+(6.5,1)$)
        {
        x_{i,j,q} &  x_{i, j+1,q} & \cdots & x_{i, j+k-1,q}\\
        x_{i+1,j,q} &  x_{i+1, j+1,q} & \cdots & x_{i+1, j+k-1,q}\\
        \vdots & \vdots & \cdots & \vdots\\
    x_{i+k-1,j,q} &  x_{i+k-1, j+1,q} & \cdots& x_{i+k-1,
    j+k-1,q}\\
        };
        \draw[dashed](mA.north east)--(mC.north east);
        \draw[dashed](mA.north west)--(mC.north west);
        \draw[dashed](mA.south east)--(mC.south east);
        \end{tikzpicture}%
\endinput}}
\end{frame}
%\vspace{0.2in}
%\caption{3D subtensor $\bX_k(i, j, q)$: $k \times k \times r$ subtensor
%of $\bX$ starting at row $i$, column $j$, and channel $q$.}
%    \label{fig:deep:cnn:Xijq}
%\vspace*{-0.2in}
%\end{figure}
%
%
%% Let $\bX_k^a(i,j)$ denote the $k \times k$ submatrix starting at row $i$
%% and column $j$, for the given channel $a$, defined as
%% \begin{align*}
%%     \bX_k^a(i,j) = \matr{
%%         &&&\\[-1em]
%%         x_{i,j,a} &  x_{i, j+1,a} & \cdots & x_{i, j+k-1,a}\\
%%         x_{i+1,j,a} &  x_{i+1, j+1,a} & \cdots & x_{i+1,
%%         j+k-1,a}\\
%%         \vdots & \vdots & \cdots & \vdots\\
%%     x_{i+k-1,j,a} &  x_{i+k-1, j+1, a} & \cdots & x_{i+k-1,
%%     j+k-1,a}\\
%%         &&&\\[-1em]
%% }
%% \end{align*}
%% Then, we can write the $k \times k \times r$ subtensor of $\bX$ at
%% row $i$, column $j$ and channel $q$ compactly as
%% \begin{align*}
%%     \bX_k(i,j,q) = \matr{&&&\\[-1em]
%%         \Bigl[ \bX_k^q(i,j) \Bigr] & 
%%         \Bigl[ \bX_k^{q+1}(i,j) \Bigr] & 
%%         \cdots &
%%         \Bigl[ \bX_k^{q+r-1}(i,j) \Bigr]\\
%%         &&&\\[-1em]
%%     }
%% \end{align*}
%
%
%
%%\begin{figure}[ht!]
%%    \scalebox{0.6}{%
%%    \psset{unit=0.25in}
%%    %\psset{viewpoint=2 1.5 1,subgriddiv=0,gridlabels=0}
%%    %\psset{viewpoint=60 50 20 rtp2xyz,Decran=50}
%%    \psset{Alpha=90,Beta=35}
%%    \centering
%%    \begin{pspicture}(-1,-4.5)(8,9.5)
%%    %\pstThreeDCoor[linecolor=black]
%%    %\ThreeDput[normal=1 0 0](0,0,0)
%%    \pstPlanePut[plane=yz](0,-1,0){
%%    \begin{tabular}{|c|c|c|c|}
%%        \hline
%%            $\summ\bigl(\bX_k(1,1,1) \odot \bW\bigr)$ &
%%            $\summ\bigl(\bX_k(1,2,1)\odot \bW\bigr)$ &
%%            $\cdots$ & 
%%            $\summ\bigl(\bX_k(1, n-k+1,1) \odot \bW\bigr)$\\
%%        \hline
%%            $\summ\bigl(\bX_k(2,1,1) \odot \bW\bigr)$ &
%%            $\summ\bigl(\bX_k(2,2,1)\odot \bW\bigr)$ &
%%            $\cdots$ & 
%%            $\summ \bigl(\bX_k(2, n-k+1,1) \odot \bW\bigr)$\\
%%        \hline
%%        $\vdots$ & 
%%        $\vdots$ & 
%%        $\cdots$ & 
%%        $\vdots$\\
%%        \hline
%%        $\summ\bigl(\bX_k(n-k+1,1,1) \odot \bW\bigr)$ &
%%        $\summ\bigl(\bX_k(n-k+1,2,1)\odot \bW\bigr)$ &
%%        $\cdots$ & 
%%        $\summ \bigl(\bX_k(n-k+1, n-k+1,1) \odot \bW\bigr)$\\
%%        \hline
%%    \end{tabular}
%%    }
%%    \pstPlanePut[plane=yz](-8,-1,0){
%%    %\ThreeDput[normal=1 0 0](-5,0,0)
%%    \begin{tabular}{|c|c|c|c|}
%%        \hline
%%            $\summ\bigl(\bX_k(1,1,2) \odot \bW\bigr)$ &
%%            $\summ\bigl(\bX_k(1,2,2)\odot \bW\bigr)$ &
%%            $\cdots$ & $\summ\bigl(\bX_k(1, n-k+1,2) \odot \bW\bigr)$\\
%%        \hline
%%            $\summ\bigl(\bX_k(2,1,2) \odot \bW\bigr)$ &
%%            $\summ\bigl(\bX_k(2,2,2)\odot \bW\bigr)$ &
%%            $\cdots$ & $\summ \bigl(\bX_k(2, n-k+1,2) \odot \bW\bigr)$\\
%%        \hline
%%        $\vdots$ & $\vdots$ & $\cdots$ & $\vdots$\\
%%        \hline
%%        $\summ\bigl(\bX_k(n-k+1,1,2) \odot \bW\bigr)$ &
%%        $\summ\bigl(\bX_k(n-k+1,2,2)\odot \bW\bigr)$ &
%%        $\cdots$ & $\summ \bigl(\bX_k(n-k+1, n-k+1,2) \odot \bW\bigr)$\\
%%        \hline
%%\end{tabular}
%%    }
%%    \pstPlanePut[plane=xy,planecorr=normal](-8,8,0){\scalebox{1.5}{$\ldots$}}
%%    \pstPlanePut[plane=yz](-15,-4,0){
%%    %\ThreeDput[normal=1 0 0](-10,0,0)
%%    \begin{tabular}{|c|c|c|c|}
%%        \hline
%%            $\summ\bigl(\bX_k(1,1,m-r+1) \odot \bW\bigr)$ &
%%            $\summ\bigl(\bX_k(1,2,m-r+1)\odot \bW\bigr)$ &
%%            $\cdots$ & 
%%            $\summ\bigl(\bX_k(1, n-k+1,m-r+1) \odot \bW\bigr)$\\
%%        \hline
%%            $\summ\bigl(\bX_k(2,1,m-r+1) \odot \bW\bigr)$ &
%%            $\summ\bigl(\bX_k(2,2,m-r+1)\odot \bW\bigr)$ &
%%            $\cdots$ & 
%%            $\summ \bigl(\bX_k(2, n-k+1,m-r+1) \odot \bW\bigr)$\\
%%        \hline
%%        $\vdots$ & 
%%        $\vdots$ & 
%%        $\cdots$ & 
%%        $\vdots$\\
%%        \hline
%%        $\summ\bigl(\bX_k(n-k+1,1,m-r+1) \odot \bW\bigr)$ &
%%        $\summ\bigl(\bX_k(n-k+1,2,m-r+1)\odot \bW\bigr)$ &
%%        $\cdots$ & 
%%        $\summ \bigl(\bX_k(n-k+1, n-k+1,m-r+1) \odot \bW\bigr)$\\
%%        \hline
%%\end{tabular}
%%    }
%%    \pstThreeDLine[arrows=<-](3,1.75,0)(3,1.75,5)%i
%%    \pstPlanePut[plane=yz,planecorr=normal](3,1.25,2.5){$i$}
%%    \pstThreeDLine[arrows=->](3,2,0)(3,17,0)%j
%%    \pstPlanePut[plane=yz,planecorr=normal](3.75,8.5,0){$j$}
%%    \pstThreeDLine[arrows=->](3,17.5,0)(-9.5,17.5,0)%q
%%    \pstPlanePut[plane=yz,planecorr=normal](-3,18,0){$q$}
%%    % \pstThreeDLine[arrows=->](3,15.5,0)(-10,15.5,-2)%q
%%    % \pstPlanePut[plane=yz,planecorr=normal](-3,17,0){$q$}
%%    \end{pspicture}
%%    }
%%\vspace{0.2in}
%%\caption{3D subtensor $\bX_k(i, j, q)$: $k \times k \times r$ subtensor
%%of $\bX$ starting at row $i$, column $j$ and channel $q$.}
%%    \label{fig:deep:cnn:Xijq}
%%\end{figure}
%
%% \enlargethispage{3\baselineskip}
%Given a $k \times k \times r$ tensor $\bA \in \setR^{k \times k \times r}$, 
%define the summation operator as one
%that adds all the elements of the tensor. That is,
%\begin{align*}
%    \summ(\bA) = \sum_{i=1}^k \sum_{j=1}^k \sum^{r}_{q=1} a_{i,j,q}
%\end{align*}
%where $a_{i,j,q}$ is the element of $\bA$ at row $i$, column $j$, and
%channel $q$.
%The {\em 3D convolution} of $\bX$ and $\bW$, denoted $\bX \ast
%\bW$, is defined as:
%
%\centerline{
%\begin{small}
%   \scalebox{0.85}{\parbox{\linewidth}{\[
%               \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!
%    \bX \ast \bW =  
%    \begin{array}{ccccc}
%            \ldelim({4}{*} & \summ\bigl(\bX_k(1,1,1) \odot \bW\bigr) &
%            \cdots & 
%            \summ\bigl(\bX_k(1, n-k+1,1) \odot \bW\bigr) & \rdelim){4}{*}\\
%            & \summ\bigl(\bX_k(2,1,1) \odot \bW\bigr) &
%            \cdots & 
%            \summ \bigl(\bX_k(2, n-k+1,1) \odot \bW\bigr) &\\
%        & \vdots & 
%        \cdots & 
%        \vdots & \\
%        &\summ\bigl(\bX_k(n-k+1,1,1) \odot \bW\bigr) &
%        \cdots & 
%        \summ \bigl(\bX_k(n-k+1, n-k+1,1) \odot \bW\bigr)&\\
%        \cline{2-4}
%        %
%        \ldelim({4}{*} & \summ\bigl(\bX_k(1,1,2) \odot \bW\bigr) &
%            \cdots & \summ\bigl(\bX_k(1, n-k+1,2) \odot
%        \bW\bigr) & \rdelim){4}{*}\\
%           &  \summ\bigl(\bX_k(2,1,2) \odot \bW\bigr) &
%            \cdots & \summ \bigl(\bX_k(2, n-k+1,2) \odot \bW\bigr) &\\
%       & \vdots & \cdots & \vdots &\\
%        &\summ\bigl(\bX_k(n-k+1,1,2) \odot \bW\bigr) &
%        \cdots & \summ \bigl(\bX_k(n-k+1, n-k+1,2) \odot
%        \bW\bigr)& \\
%        \cline{2-4}
%        & \vdots & 
%        \vdots & 
%        \vdots & \\
%        %
%            \ldelim({4}{*} &\summ\bigl(\bX_k(1,1,m-r+1) \odot \bW\bigr) &
%            \cdots & 
%            \summ\bigl(\bX_k(1, n-k+1,m-r+1) \odot
%        \bW\bigr) & \rdelim){4}{*}\\
%            & \summ\bigl(\bX_k(2,1,m-r+1) \odot \bW\bigr) &
%            \cdots & 
%            \summ \bigl(\bX_k(2, n-k+1,m-r+1) \odot \bW\bigr)& \\
%        & \vdots & 
%        \cdots & 
%        \vdots & \\
%        & \summ\bigl(\bX_k(n-k+1,1,m-r+1) \odot \bW\bigr) &
%        \cdots & 
%        \summ \bigl(\bX_k(n-k+1, n-k+1,m-r+1) \odot \bW\bigr) &\\
%  \end{array}
%  \]}}
%\end{small}
%}
%\noindent where $\odot$ is the
%element-wise product of $\bX_k(i,j,q)$ and $\bW$, so that
%\begin{align}
%    \tcbhighmath{
%    \summ\bigl(\bX_k(i,j,q) \odot \bW\bigr) = \sum_{a=1}^k \sum_{b=1}^k \sum_{c=1}^r
%x_{i+a-1, j+b-1, q+c-1} \cdot w_{a, b, c} }
%\end{align}
%for $i, j = 1,2,\cdots,n-k+1$ and $q = 1,2,\cdots,m-r+1$.
%We can see that the convolution of $\bX \in \setR^{n\times n \times
%m}$ and
%$\bW \in \setR^{k \times k \times r}$ results in a $(n-k+1) \times
%(n-k+1) \times (m-r+1)$ tensor.
%%\begin{small}
%%    \begin{empheq}[left={\bX \ast \bW =}]{align*}
%%    % \bX \ast \bW = 
%%        & \matr{%
%%            &&\\
%%            \summ\bigl(\bX_k(1,1,1) \odot \bW\bigr) &
%%            \cdots & 
%%            \summ\bigl(\bX_k(1, n-k+1,1) \odot \bW\bigr)\\
%%            \summ\bigl(\bX_k(2,1,1) \odot \bW\bigr) &
%%            \cdots & 
%%            \summ \bigl(\bX_k(2, n-k+1,1) \odot \bW\bigr)\\
%%        \vdots & 
%%        \cdots & 
%%        \vdots\\
%%        \summ\bigl(\bX_k(n-k+1,1,1) \odot \bW\bigr) &
%%        \cdots & 
%%        \summ \bigl(\bX_k(n-k+1, n-k+1,1) \odot \bW\bigr)\\
%%    &&}\\
%%       % \hline
%%        % 
%%     & \matr{
%%         &&\\
%%        \summ\bigl(\bX_k(1,1,2) \odot \bW\bigr) &
%%            \cdots & \summ\bigl(\bX_k(1, n-k+1,2) \odot \bW\bigr)\\
%%            \summ\bigl(\bX_k(2,1,2) \odot \bW\bigr) &
%%            \cdots & \summ \bigl(\bX_k(2, n-k+1,2) \odot \bW\bigr)\\
%%        \vdots & \cdots & \vdots\\
%%        \summ\bigl(\bX_k(n-k+1,1,2) \odot \bW\bigr) &
%%        \cdots & \summ \bigl(\bX_k(n-k+1, n-k+1,2) \odot
%%        \bW\bigr)\\
%%        &&
%%    }\\
%%     %   \hline
%%    & \matr{  \vdots &\vdots &\vdots }\\
%%        %\hline
%%    & \matr{
%%        &&\\
%%            \summ\bigl(\bX_k(1,1,m-r+1) \odot \bW\bigr) &
%%            \cdots & 
%%            \summ\bigl(\bX_k(1, n-k+1,m-r+1) \odot \bW\bigr)\\
%%            \summ\bigl(\bX_k(2,1,m-r+1) \odot \bW\bigr) &
%%            \cdots & 
%%            \summ \bigl(\bX_k(2, n-k+1,m-r+1) \odot \bW\bigr)\\
%%        \vdots & 
%%        \cdots & 
%%        \vdots\\
%%        \summ\bigl(\bX_k(n-k+1,1,m-r+1) \odot \bW\bigr) &
%%        \cdots & 
%%        \summ \bigl(\bX_k(n-k+1, n-k+1,m-r+1) \odot \bW\bigr)\\
%%  &&}
%%\end{empheq}
%%\end{small}
%%%\end{equation*}
%%% \end{empheq}
%%% }}}
%%\begin{align*}
%%    \small
%%    \bX \ast \bW = 
%%    \resizebox{0.9\hsize}{!}{
%%    \begin{blockarray}{cccc}
%%        \begin{block}{|c|c|c|c|}
%%        \hline
%%            $\summ\bigl(\bX_k(1,1,1) \odot \bW\bigr)$ &
%%            $\summ\bigl(\bX_k(1,2,1)\odot \bW\bigr)$ &
%%            $\cdots$ & 
%%        $\summ\bigl(\bX_k(1, n-k+1,1) \odot \bW\bigr)$
%%    \rdelim\}{3}{3mm}[]\\
%%        \hline
%%            $\summ\bigl(\bX_k(2,1,1) \odot \bW\bigr)$ &
%%            $\summ\bigl(\bX_k(2,2,1)\odot \bW\bigr)$ &
%%            $\cdots$ & 
%%            $\summ \bigl(\bX_k(2, n-k+1,1) \odot \bW\bigr)$\\
%%        \hline
%%        $\vdots$ & 
%%        $\vdots$ & 
%%        $\cdots$ & 
%%        $\vdots$\\
%%        \hline
%%        $\summ\bigl(\bX_k(n-k+1,1,1) \odot \bW\bigr)$ &
%%        $\summ\bigl(\bX_k(n-k+1,2,1)\odot \bW\bigr)$ &
%%        $\cdots$ & 
%%        $\summ \bigl(\bX_k(n-k+1, n-k+1,1) \odot \bW\bigr)$\\
%%        \hline
%%\end{block}
%%\begin{block}{cccc}
%%    &&&\\
%%\end{block}
%%    \begin{block}{|c|c|c|c|}
%%        \hline
%%            $\summ\bigl(\bX_k(1,1,2) \odot \bW\bigr)$ &
%%            $\summ\bigl(\bX_k(1,2,2)\odot \bW\bigr)$ &
%%            $\cdots$ & $\summ\bigl(\bX_k(1, n-k+1,2) \odot \bW\bigr)$\\
%%        \hline
%%            $\summ\bigl(\bX_k(2,1,2) \odot \bW\bigr)$ &
%%            $\summ\bigl(\bX_k(2,2,2)\odot \bW\bigr)$ &
%%            $\cdots$ & $\summ \bigl(\bX_k(2, n-k+1,2) \odot \bW\bigr)$\\
%%        \hline
%%        $\vdots$ & $\vdots$ & $\cdots$ & $\vdots$\\
%%        \hline
%%        $\summ\bigl(\bX_k(n-k+1,1,2) \odot \bW\bigr)$ &
%%        $\summ\bigl(\bX_k(n-k+1,2,2)\odot \bW\bigr)$ &
%%        $\cdots$ & $\summ \bigl(\bX_k(n-k+1, n-k+1,2) \odot \bW\bigr)$\\
%%        \hline
%%\end{block}
%%\begin{block}{cccc}
%%    &&&\\
%%\end{block}
%%\begin{block}{cccc}
%%    $\vdots$ &$\vdots$ &$\vdots$ &$\vdots$ \\
%%\end{block}
%%\begin{block}{cccc}
%%    &&&\\
%%\end{block}
%%    \begin{block}{|c|c|c|c|}
%%        \hline
%%            $\summ\bigl(\bX_k(1,1,m-r+1) \odot \bW\bigr)$ &
%%            $\summ\bigl(\bX_k(1,2,m-r+1)\odot \bW\bigr)$ &
%%            $\cdots$ & 
%%            $\summ\bigl(\bX_k(1, n-k+1,m-r+1) \odot \bW\bigr)$\\
%%        \hline
%%            $\summ\bigl(\bX_k(2,1,m-r+1) \odot \bW\bigr)$ &
%%            $\summ\bigl(\bX_k(2,2,m-r+1)\odot \bW\bigr)$ &
%%            $\cdots$ & 
%%            $\summ \bigl(\bX_k(2, n-k+1,m-r+1) \odot \bW\bigr)$\\
%%        \hline
%%        $\vdots$ & 
%%        $\vdots$ & 
%%        $\cdots$ & 
%%        $\vdots$\\
%%        \hline
%%        $\summ\bigl(\bX_k(n-k+1,1,m-r+1) \odot \bW\bigr)$ &
%%        $\summ\bigl(\bX_k(n-k+1,2,m-r+1)\odot \bW\bigr)$ &
%%        $\cdots$ & 
%%        $\summ \bigl(\bX_k(n-k+1, n-k+1,m-r+1) \odot \bW\bigr)$\\
%%        \hline
%%\end{block}
%%\end{blockarray}
%%}
%%\end{align*}
%    % $
%    % \begin{blockarray}{cccc}
%    %     \begin{block}{[|c|c|c|c|]}
%    % &&&\\
%    %         \summ\bigl(\bX_k(1,1,1) \odot \bW\bigr) &
%    %         \summ\bigl(\bX_k(1,2,1)\odot \bW\bigr) &
%    %         \cdots & \summ\bigl(\bX_k(1, n-k+1,1) \odot \bW\bigr)\\
%    %         % \hline
%    %         \summ\bigl(\bX_k(2,1,1) \odot \bW\bigr) &
%    %         \summ\bigl(\bX_k(2,2,1)\odot \bW\bigr) &
%    %         \cdots & \summ \bigl(\bX_k(2, n-k+1,1) \odot \bW\bigr)\\
%    %         % \hline
%    %     \vdots & \vdots & \cdots & \vdots\\
%    %         % \hline
%    %     \summ\bigl(\bX_k(n-k+1,1,1) \odot \bW\bigr) &
%    %     \summ\bigl(\bX_k(n-k+1,2,1)\odot \bW\bigr) &
%    %     \cdots & \summ \bigl(\bX_k(n-k+1, n-k+1,1) \odot \bW\bigr)\\
%    % &&&\\
%    % \end{block}
%    %     \begin{block}{[cccc]}
%    % &&&\\
%    %         \summ\bigl(\bX_k(1,1,2) \odot \bW\bigr) &
%    %         \summ\bigl(\bX_k(1,2,2)\odot \bW\bigr) &
%    %         \cdots & \summ\bigl(\bX_k(1, n-k+1,2) \odot \bW\bigr)\\
%    %         \summ\bigl(\bX_k(2,1,2) \odot \bW\bigr) &
%    %         \summ\bigl(\bX_k(2,2,2)\odot \bW\bigr) &
%    %         \cdots & \summ \bigl(\bX_k(2, n-k+1,2) \odot \bW\bigr)\\
%    %     \vdots & \vdots & \cdots & \vdots\\
%    %     \summ\bigl(\bX_k(n-k+1,1,2) \odot \bW\bigr) &
%    %     \summ\bigl(\bX_k(n-k+1,2,2)\odot \bW\bigr) &
%    %     \cdots & \summ \bigl(\bX_k(n-k+1, n-k+1,2) \odot \bW\bigr)\\
%    % &&&\\
%    % \end{block}
%    %     \begin{block}{[cccc]}
%    % &&&\\
%    %  \vdots & \vdots & \cdots & \vdots\\
%    % &&&\\
%    % \end{block}
%    %     \begin{block}{[cccc]}
%    % &&&\\
%    %         \summ\bigl(\bX_k(1,1,m-r+1) \odot \bW\bigr) &
%    %         \summ\bigl(\bX_k(1,2,m-r+1)\odot \bW\bigr) &
%    %         \cdots & \summ\bigl(\bX_k(1, n-k+1,m-r+1) \odot \bW\bigr)\\
%    %         \summ\bigl(\bX_k(2,1,m-r+1) \odot \bW\bigr) &
%    %         \summ\bigl(\bX_k(2,2,m-r+1)\odot \bW\bigr) &
%    %         \cdots & \summ \bigl(\bX_k(2, n-k+1,m-r+1) \odot \bW\bigr)\\
%    %     \vdots & \vdots & \cdots & \vdots\\
%    %     \summ\bigl(\bX_k(n-k+1,1,m-r+1) \odot \bW\bigr) &
%    %     \summ\bigl(\bX_k(n-k+1,2,m-r+1)\odot \bW\bigr) &
%    %     \cdots & \summ \bigl(\bX_k(n-k+1, n-k+1,m-r+1) \odot \bW\bigr)\\
%    % &&&\\
%    % \end{block}
%% \end{blockarray}
%% $
%% }
%% \end{align*}
%
%
%\begin{figure}[!htbp]
\begin{frame}{3D Convolution}
\framesubtitle{Example}
%    \cref{fig:reg:deep:3dconv} 
Given a $3 \times 3 \times 3$ tensor 
    $\bX$ with $n=3$ and $m=3$, and a $2 \times 2 \times 3$
    filter $\bW$ with window size $k=2$ and $r=3$. 

	\medskip

	The convolution of the 
    first window of $\bX$, namely $\bX_2(1,1)$, with $\bW$
    is given as% (see \cref{fig:reg:deep:3dconv:a})
    \begin{align*}
        \summ\bigl(\bX_2(1,1) \odot \bW\bigr) & = 
        \summ\lB(
            \amatr{r}{1 & -1 \;\vline\; 2 & 1\;\vline\; 1 & -2\\
                  2 & 1 \;\vline\; 3 & -1\;\vline\; 2 & 1} 
                  \odot 
                  \matr{1 & 1 \;\vline\; 1 & 0\;\vline\; 0 & 1\\
                        2 & 0 \;\vline\; 0 & 1\;\vline\;1 & 0}
        \rB)\\
        & = \summ\lB( 
            \amatr{r}{1 & -1 \;\vline\; 2 & 0\;\vline\; 0 & -2\\
                        4 & 0 \;\vline\; 0 & -1\;\vline\;2 & 0}
    \rB)  = 5
    \end{align*}
    where we stack the different channels horizontally.
\end{frame}
\begin{frame}{3D Convolution}
Given a $3 \times 3 \times 3$ tensor $\bX$ with $n=3$ and $m=3$, and a $2 \times 2 \times 3$ filter $\bW$ with $k=2$ and $r=3$, the upper-left element is: 

\medskip

    \centering
    \large
    \centerline{
%        \subfloat[$\summ(\bX_2(1,1) \odot \bW)$]{
%        \label{fig:reg:deep:3dconv:a}
    \scalebox{0.55}{
        \scantokens{
    % \tikzsetnextfilename{reg_deep_3dconv_a}
            \begin{tikzpicture}[every node/.style={draw,anchor=north
                east,minimum width=1.4cm,minimum height=7mm,fill=white}]
        \matrix (mA) [matrix of math nodes]
        {
        \node[draw,fill=lightgray] (n31) {1}; &
        \node[draw,fill=lightgray] (n32){-2}; & 4\\
            \node[draw,fill=lightgray]{2}; & 
            \node[draw,fill=lightgray] (n33){1}; & -2\\
            1 & 3 & -1\\
        };
        \matrix (mB) [matrix of math nodes] at ($(mA.south
        west)+(3.5,0.1)$)
        {
            \node[draw,fill=lightgray]{2}; &
            \node[draw,fill=lightgray]{1}; & 3\\
            \node[draw,fill=lightgray]{3}; &
            \node[draw,fill=lightgray]{-1}; & 1\\
            1 & 1 & -2\\
        };
        \matrix (mC) [matrix of math nodes] at ($(mB.south
        west)+(3.5,0.1)$)
        {
            \node[draw,fill=lightgray] (n11) {1}; &
            \node[draw,fill=lightgray] (n12) {-1}; & 3\\
                    \node[draw,fill=lightgray]{2}; &
                    \node[draw,fill=lightgray] (n13) {1}; & 4\\
            3 & 1 & 2\\
        };
        \draw[dashed,thick](n11.north west)--(n31.north west);
        \draw[dashed,thick](n12.north east)--(n32.north east);
        \draw[dashed,thick](n13.south east)--(n33.south east);
        % \draw[dashed](mA.north east)--(mC.north east);
        % \draw[dashed](mA.north west)--(mC.north west);
        % \draw[dashed](mA.south east)--(mC.south east);
        \matrix (mWa) [matrix of math nodes] at 
         ($(mA.south west)+(10,1)$)
        {
            0 & 1\\
            1 & 0\\
        };
        \matrix (mWb) [matrix of math nodes] 
        at ($(mWa.south west)+(2.5,0.1)$)
        {
            1 & 0\\
            0 & 1\\
        };
        \matrix (mWc) [matrix of math nodes]
        at ($(mWb.south west)+(2.5,0.1)$)
        {
            1 & 1\\
            2 & 0\\
        };
        \draw[dashed](mWa.north east)--(mWc.north east);
        \draw[dashed](mWa.north west)--(mWc.north west);
        \draw[dashed](mWa.south east)--(mWc.south east);
        \matrix (mCC) [matrix of math nodes] at 
        ($(mA.south west)+(15,-0.5)$)
        {
            \node[draw,fill=gray]{5}; & \node[draw]{ };\\
           \node[draw]{ };  &\node[draw]{ }; \\
        };
        \node[draw=none] at ([xshift={6mm},yshift={8mm}]mWa.north) {$\bW$};
        \node[draw=none] at ([xshift={5mm},yshift={8mm}]mA.north) {$\bX$};
        \node[draw=none] at ([xshift={24mm},yshift={4mm}]mB.east) {\scalebox{1.3}{$\ast$}}; 
        \node[draw=none] at ([xshift={24mm},yshift={4mm}]mWb.east) {\scalebox{1}{$=$}}; 
        \end{tikzpicture}%
\endinput}}
}%}
\end{frame}
\begin{frame}{3D Convolution}
Given a $3 \times 3 \times 3$ tensor $\bX$ with $n=3$ and $m=3$, and a $2 \times 2 \times 3$ filter $\bW$ with $k=2$ and $r=3$, the upper-right element is: 

\medskip

    \centering
    \large
    \centerline{
%\subfloat[$\summ(\bX_2(1,2) \odot \bW)$]{
%        \label{fig:reg:deep:3dconv:b}
    \scalebox{0.55}{
        \scantokens{
    % \tikzsetnextfilename{reg_deep_3dconv_b}
            \begin{tikzpicture}[every node/.style={draw,anchor=north
                east,minimum width=1.4cm,minimum height=7mm,fill=white}]
        \matrix (mA) [draw,matrix of math nodes]
        {
            1 &
            \node[draw,fill=lightgray] (n31) {-2}; &
            \node[draw,fill=lightgray] (n32) {4};\\
            2 & 
            \node[draw,fill=lightgray]{1}; &
            \node[draw,fill=lightgray] (n33) {-2};\\
            1 & 3 & -1\\
        };
        \matrix (mB) [draw,matrix of math nodes] at ($(mA.south
        west)+(3.5,0.1)$)
        {
            2 & \node[draw,fill=lightgray]{1}; &
                \node[draw,fill=lightgray]{3};\\
                    3 & \node[draw,fill=lightgray]{-1}; &
                \node[draw,fill=lightgray]{1};\\
            1 & 1 & -2\\
        };
        \matrix (mC) [draw,matrix of math nodes] at ($(mB.south
        west)+(3.5,0.1)$)
        {
            1 & \node[draw,fill=lightgray] (n11) {-1}; &
            \node[draw,fill=lightgray] (n12){3};\\
                    2 & \node[draw,fill=lightgray]{1}; &
                    \node[draw,fill=lightgray](n13) {4};\\
            3 & 1 & 2\\
        };
        \draw[dashed,thick](n11.north west)--(n31.north west);
        \draw[dashed,thick](n12.north east)--(n32.north east);
        \draw[dashed,thick](n13.south east)--(n33.south east);
        % \draw[dashed](mA.north east)--(mC.north east);
        % \draw[dashed](mA.north west)--(mC.north west);
        % \draw[dashed](mA.south east)--(mC.south east);
        \matrix (mWa) [draw,matrix of math nodes] at 
         ($(mA.south west)+(10,1)$)
        {
            0 & 1\\
            1 & 0\\
        };
        \matrix (mWb) [draw,matrix of math nodes] 
        at ($(mWa.south west)+(2.5,0.1)$)
        {
            1 & 0\\
            0 & 1\\
        };
        \matrix (mWc) [draw,matrix of math nodes]
        at ($(mWb.south west)+(2.5,0.1)$)
        {
            1 & 1\\
            2 & 0\\
        };
        \draw[dashed](mWa.north east)--(mWc.north east);
        \draw[dashed](mWa.north west)--(mWc.north west);
        \draw[dashed](mWa.south east)--(mWc.south east);
        \matrix (mCC) [matrix of math nodes] at 
        ($(mA.south west)+(15,-0.5)$)
        {
            \node[draw]{5}; & \node[draw,fill=gray]{11};\\
           \node[draw]{ };  &\node[draw]{ }; \\
        };
        \node[draw=none] at ([xshift={6mm},yshift={8mm}]mWa.north) {$\bW$};
        \node[draw=none] at ([xshift={5mm},yshift={8mm}]mA.north) {$\bX$};
        \node[draw=none] at ([xshift={24mm},yshift={4mm}]mB.east) {\scalebox{1.3}{$\ast$}}; 
        \node[draw=none] at ([xshift={24mm},yshift={4mm}]mWb.east) {\scalebox{1}{$=$}}; 
        \end{tikzpicture}%
\endinput}}
}%}
\end{frame}
\begin{frame}{3D Convolution}
Given a $3 \times 3 \times 3$ tensor $\bX$ with $n=3$ and $m=3$, and a $2 \times 2 \times 3$ filter $\bW$ with $k=2$ and $r=3$, the bottom-left element is: 

\medskip

    \centering
    \large
\centerline{
%    \subfloat[$\summ(\bX_2(2,1) \odot \bW)$]{
%        \label{fig:reg:deep:3dconv:c}
    \scalebox{0.55}{
        \scantokens{
    % \tikzsetnextfilename{reg_deep_3dconv_c}
            \begin{tikzpicture}[every node/.style={draw,anchor=north
                east,minimum width=1.4cm,minimum height=7mm,fill=white}]
        \matrix (mA) [draw,matrix of math nodes]
        {
            1 & -2 & 4\\
            \node[draw,fill=lightgray] (n31) {2}; & 
            \node[draw,fill=lightgray] (n32) {1}; & -2\\
            \node[draw,fill=lightgray]{1}; &
            \node[draw,fill=lightgray] (n33) {3}; & -1\\
        };
        \matrix (mB) [draw,matrix of math nodes] at ($(mA.south
        west)+(3.5,0.1)$)
        {
            2 & 1 & 3\\
            \node[draw,fill=lightgray]{3}; &
            \node[draw,fill=lightgray]{-1}; & 1\\
                \node[draw,fill=lightgray]{1}; &
                \node[draw,fill=lightgray]{1}; & -2\\
        };
        \matrix (mC) [draw,matrix of math nodes] at ($(mB.south
        west)+(3.5,0.1)$)
        {
            1 & -1 & 3\\
            \node[draw,fill=lightgray] (n11) {2}; &
            \node[draw,fill=lightgray] (n12) {1}; & 4\\
                    \node[draw,fill=lightgray]{3}; &
                    \node[draw,fill=lightgray] (n13) {1}; & 2\\
        };
        \draw[dashed,thick](n11.north west)--(n31.north west);
        \draw[dashed,thick](n12.north east)--(n32.north east);
        \draw[dashed,thick](n13.south east)--(n33.south east);
        % \draw[dashed](mA.north east)--(mC.north east);
        % \draw[dashed](mA.north west)--(mC.north west);
        % \draw[dashed](mA.south east)--(mC.south east);
        \matrix (mWa) [draw,matrix of math nodes] at 
         ($(mA.south west)+(10,1)$)
        {
            0 & 1\\
            1 & 0\\
        };
        \matrix (mWb) [draw,matrix of math nodes] 
        at ($(mWa.south west)+(2.5,0.1)$)
        {
            1 & 0\\
            0 & 1\\
        };
        \matrix (mWc) [draw,matrix of math nodes]
        at ($(mWb.south west)+(2.5,0.1)$)
        {
            1 & 1\\
            2 & 0\\
        };
        \draw[dashed](mWa.north east)--(mWc.north east);
        \draw[dashed](mWa.north west)--(mWc.north west);
        \draw[dashed](mWa.south east)--(mWc.south east);
        \matrix (mCC) [matrix of math nodes] at 
        ($(mA.south west)+(15,-0.5)$)
        {
            \node[draw]{5}; & \node[draw]{11};\\
           \node[draw,fill=gray]{15};  &\node[draw]{ }; \\
        };
        \node[draw=none] at ([xshift={6mm},yshift={8mm}]mWa.north) {$\bW$};
        \node[draw=none] at ([xshift={5mm},yshift={8mm}]mA.north) {$\bX$};
        \node[draw=none] at ([xshift={24mm},yshift={4mm}]mB.east) {\scalebox{1.3}{$\ast$}}; 
        \node[draw=none] at ([xshift={24mm},yshift={4mm}]mWb.east) {\scalebox{1}{$=$}}; 
        \end{tikzpicture}%
\endinput}}
}%}
\end{frame}
\begin{frame}{3D Convolution}
    \centering
	{\large
    \centerline{
%\subfloat[$\summ(\bX_2(2,2) \odot \bW)$]{
%        \label{fig:reg:deep:3dconv:d}
    \scalebox{0.55}{
        \scantokens{
    % \tikzsetnextfilename{reg_deep_3dconv_d}
            \begin{tikzpicture}[every node/.style={draw,anchor=north
                east,minimum width=1.4cm,minimum height=7mm,fill=white}]
        \matrix (mA) [draw,matrix of math nodes]
        {
            1 &
            -2 & 4\\
            2 & 
            \node[draw,fill=lightgray] (n31) {1}; &
            \node[draw,fill=lightgray] (n32) {-2};\\
            1 & \node[draw,fill=lightgray]{3}; &
            \node[draw,fill=lightgray] (n33) {-1};\\
        };
        \matrix (mB) [draw,matrix of math nodes] at ($(mA.south
        west)+(3.5,0.1)$)
        {
            2 & 1 & 3\\
            3 & \node[draw,fill=lightgray]{-1}; &
            \node[draw,fill=lightgray]{1};\\
            1 &\node[draw,fill=lightgray]{ 1}; &
            \node[draw,fill=lightgray]{-2};\\
        };
        \matrix (mC) [draw,matrix of math nodes] at ($(mB.south
        west)+(3.5,0.1)$)
        {
            1 & -1 & 3\\
            2 & \node[draw,fill=lightgray] (n11){1}; &
            \node[draw,fill=lightgray](n12){4};\\
            3 & \node[draw,fill=lightgray]{1}; &
            \node[draw,fill=lightgray](n13) {2};\\
        };
        \draw[dashed,thick](n11.north west)--(n31.north west);
        \draw[dashed,thick](n12.north east)--(n32.north east);
        \draw[dashed,thick](n13.south east)--(n33.south east);
        % \draw[dashed](mA.north east)--(mC.north east);
        % \draw[dashed](mA.north west)--(mC.north west);
        % \draw[dashed](mA.south east)--(mC.south east);
        \matrix (mWa) [draw,matrix of math nodes] at 
         ($(mA.south west)+(10,1)$)
        {
            0 & 1\\
            1 & 0\\
        };
        \matrix (mWb) [draw,matrix of math nodes] 
        at ($(mWa.south west)+(2.5,0.1)$)
        {
            1 & 0\\
            0 & 1\\
        };
        \matrix (mWc) [draw,matrix of math nodes]
        at ($(mWb.south west)+(2.5,0.1)$)
        {
            1 & 1\\
            2 & 0\\
        };
        \draw[dashed](mWa.north east)--(mWc.north east);
        \draw[dashed](mWa.north west)--(mWc.north west);
        \draw[dashed](mWa.south east)--(mWc.south east);
        \matrix (mCC) [matrix of math nodes] at 
        ($(mA.south west)+(15,-0.5)$)
        {
            \node[draw]{5}; & \node[draw]{11};\\
           \node[draw]{15};  &\node[draw,fill=gray]{5}; \\
        };
        \node[draw=none] at ([xshift={6mm},yshift={8mm}]mWa.north) {$\bW$};
        \node[draw=none] at ([xshift={5mm},yshift={8mm}]mA.north) {$\bX$};
        \node[draw=none] at ([xshift={24mm},yshift={4mm}]mB.east) {\scalebox{1.3}{$\ast$}}; 
        \node[draw=none] at ([xshift={24mm},yshift={4mm}]mWb.east) {\scalebox{1}{$=$}}; 
        \end{tikzpicture}%
\endinput}}
}}

\medskip

    The convolution $\bX \ast \bW$ has size $2 \times 2$, since 
    $n-k+1 = 3-2+1 = 2$ and $r=m=3$; it is
    given as 
    \begin{align*}
        \bX \ast \bW = \matr{5 &11\\
            15 & 5
        }
    \end{align*}
\end{frame}
%\vspace{0.2in}
%\caption{3D Convolution:
%\protect\subref{fig:reg:deep:3dconv:a}--\protect\subref{fig:reg:deep:3dconv:d}
%Convolution between different $2 \times 2 \times 3$ sliding windows of
%$\bX$, and the filter $\bW$. The final 3D convolution output is shown in
%\protect\subref{fig:reg:deep:3dconv:d}.}
%    \label{fig:reg:deep:3dconv}
%\end{figure}
%
%
%
%
%
%% \vspace*{-0.2in}
%\index{convolutional neural networks!3D convolutions}
%\paragraph{3D Convolutions in CNNs}
%Typically in CNNs, we use a 3D filter $\bW$ of size $k \times k \times m$,
%with the number of channels $r=m$, the same as the number of channels in
%$\bX \in \setR^{n \times n \times m}$. 
%Let $\bX_{k}(i,j)$ be the $k \times k \times m$ subtensor of $\bX$
%starting at row $i$ and column $j$.
%Then the 3D convolution of $\bX$
%and $\bW$ is given as:
%\begin{small}
%\begin{equation*}
%        \bX \ast \bW = \matr{ && \\[-1em]
%        \summ\bigl(\bX_k(1,1) \odot \bW\bigr) &
%        \cdots & \summ\bigl(\bX_k(1, n-k+1) \odot \bW\bigr)\\
%        \summ\bigl(\bX_k(2,1) \odot \bW\bigr) &
%        \cdots & \summ \bigl(\bX_k(2, n-k+1) \odot \bW\bigr)\\
%        \vdots & \cdots & \vdots\\
%        \summ\bigl(\bX_k(n-k+1,1) \odot \bW\bigr) &
%        \cdots & 
%        \summ \bigl(\bX_k(n-k+1, n-k+1) \odot \bW\bigr)\\[-1em]
%&&}
%\end{equation*}
%\end{small}
%% \begin{align*}
%%     \bX \ast \bW = 
%%     \resizebox{0.9\hsize}{!}{
%%         \begin{blockarray}{cccc}
%%             \begin{block}{|c|c|c|c|}
%%             \hline
%%         $\summ\bigl(\bX_k(1,1) \odot \bW\bigr)$ &
%%         $\summ\bigl(\bX_k(1,2)\odot \bW\bigr)$ &
%%         $\cdots$ & $\summ\bigl(\bX_k(1, n-k+1) \odot \bW\bigr)$\\
%%             \hline
%%         $\summ\bigl(\bX_k(2,1) \odot \bW\bigr)$ &
%%         $\summ\bigl(\bX_k(2,2)\odot \bW\bigr)$ &
%%         $\cdots$ & $\summ \bigl(\bX_k(2, n-k+1) \odot \bW\bigr)$\\
%%             \hline
%%         $\vdots$ & $\vdots$ & $\cdots$ & $\vdots$\\
%%             \hline
%%         $\summ\bigl(\bX_k(n-k+1,1) \odot \bW\bigr)$ &
%%         $\summ\bigl(\bX_k(n-k+1,2)\odot \bW\bigr)$ &
%%         $\cdots$ & 
%%         $\summ \bigl(\bX_k(n-k+1, n-k+1) \odot \bW\bigr)$\\
%%             \hline
%%     \end{block}
%%     \end{blockarray}
%% }%
%% \end{align*}
%% \begin{align*}
%%     \bX \ast \bW = 
%%     \resizebox{0.9\hsize}{!}{$
%%     \matr{
%%         &&&\\[-1em]
%%         \summ\bigl(\bX_k(1,1) \odot \bW\bigr) &
%%         \summ\bigl(\bX_k(1,2)\odot \bW\bigr) &
%%         \cdots & \summ\bigl(\bX_k(1, n-k+1) \odot \bW\bigr)\\
%%         \summ\bigl(\bX_k(2,1) \odot \bW\bigr) &
%%         \summ\bigl(\bX_k(2,2)\odot \bW\bigr) &
%%         \cdots & \summ \bigl(\bX_k(2, n-k+1) \odot \bW\bigr)\\
%%         \vdots & \vdots & \cdots & \vdots\\
%%         \summ\bigl(\bX_k(n-k+1,1) \odot \bW\bigr) &
%%         \summ\bigl(\bX_k(n-k+1,2)\odot \bW\bigr) &
%%         \cdots & \summ \bigl(\bX_k(n-k+1, n-k+1) \odot
%%         \bW\bigr)\\
%%         &&&\\[-1em]
%% }%
%% $}
%% \end{align*}
%We can see that when $\bW\in\setR^{k \times k \times m}$, 
%its 3D convolution with 
%$\bX \in \setR^{n \times n \times m}$ results in a $(n-k+1) \times
%(n-k+1)$ matrix, since there is no freedom to move in the third
%dimension. 
%Henceforth, we will always assume that a 3D filter $\bW \in
%\setR^{k \times k \times m}$ has the same
%number of channels as the tensor $\bX$ on which it is applied. Since the number
%of channels is fixed based on $\bX$, the only parameter needed to fully
%specify $\bW$ is the window size $k$.
%
%\begin{example}[3D Convolution]
%\begin{frame}{3D Convolution}
%\framesubtitle{Example}
%%    \cref{fig:reg:deep:3dconv} 
%Figures show a $3 \times 3 \times 3$ tensor 
%    $\bX$ with $n=3$ and $m=3$, and a $2 \times 2 \times 3$
%    filter $\bW$ with window size $k=2$ and $r=3$. The convolution of the 
%    first window of $\bX$, namely $\bX_2(1,1)$, with $\bW$
%    is given as% (see \cref{fig:reg:deep:3dconv:a})
%    \begin{align*}
%        \summ\bigl(\bX_2(1,1) \odot \bW\bigr) & = 
%        \summ\lB(
%            \amatr{r}{1 & -1 \;\vline\; 2 & 1\;\vline\; 1 & -2\\
%                  2 & 1 \;\vline\; 3 & -1\;\vline\; 2 & 1} 
%                  \odot 
%                  \matr{1 & 1 \;\vline\; 1 & 0\;\vline\; 0 & 1\\
%                        2 & 0 \;\vline\; 0 & 1\;\vline\;1 & 0}
%        \rB)\\
%        & = \summ\lB( 
%            \amatr{r}{1 & -1 \;\vline\; 2 & 0\;\vline\; 0 & -2\\
%                        4 & 0 \;\vline\; 0 & -1\;\vline\;2 & 0}
%    \rB)  = 5
%    \end{align*}
%    where we stack the different channels horizontally.
%    The convolution steps for different $2\times 2 \times 3$ 
%    sliding windows of $\bX$ with the
%    filter $\bW$ are shown in figure.
%    %\cref{fig:reg:deep:3dconv}\protect\subref{fig:reg:deep:3dconv:a}--\protect\subref{fig:reg:deep:3dconv:d}.
%    The convolution $\bX \ast \bW$ has size $2 \times 2$, since 
%    $n-k+1 = 3-2+1 = 2$ and $r=m=3$; it is
%    given as 
%    \begin{align*}
%        \bX \ast \bW = \matr{5 &11\\
%            15 & 5
%        }
%    \end{align*}
%\end{frame}
%\end{example}
%
%
%
%
%\subsection{Bias and Activation Functions}
%We discuss the role of bias neurons and activation functions in the
%context of a tensor of neurons at layer $l$. Let $\bZ^l$ be an $n_l
%\times n_l \times m_l$ tensor of neurons at layer $l$ so that
%$z^l_{i,j,q}$ denotes the value of the neuron at row $i$, column $j$ and
%channel $q$ for
%layer $l$, with $1 \le i,j \le n_l$ and $1\le q \le m_l$.
%
%\index{convolutional neural networks!filter bias}
%\subsubsection{Filter Bias}
\begin{frame}{Bias and Activation Functions}
	\framesubtitle{Filter Bias}
Let $\bW$ be a $k \times k \times m_l$ 3D filter.
Recall that when we convolve $\bZ^l$ and $\bW$, we get a $(n_l-k+1)
\times (n_l-k+1)$ matrix at layer $l+1$. 

\medskip

However, so far, we have ignored the role of the bias term in the
convolution. Let $b \in \setR$ be a scalar bias value for $\bW$, and
let $\bZ^l_k(i,j)$ denote the $k \times k \times m_l$ subtensor of
$\bZ^l$ at position $(i,j)$.
%Then, the net signal at neuron
%$z^{l+1}_{i,j}$ in layer $l+1$ is given as
\begin{align*}
    \net^{\;l+1}_{i,j} = \summ\lB( \bZ^l_k(i,j) \odot \bW \rB) +
    b
\end{align*}
and the value of the neuron $z^{l+1}_{i,j}$ is obtained by applying some
$f$%activation function $f$ to the net signal
\begin{align*}
    z^{l+1}_{i,j} = f\Bigl(\summ\bigl( \bZ^l_k(i,j) \odot \bW \bigr) +
        b \Bigr)
\end{align*}
The activation function can be any of the ones typically used in neural
networks, for example, identity, sigmoid, tanh, ReLU and so on.
%In the language of convolutions, the values of the neurons
%in layer $l+1$ is given as follows
%\begin{align*}
%    \bZ^{l+1} = f\Bigl( 
%        \bigl(\bZ^l \ast \bW\bigr) \;\oplus\; b
%        \Bigr)
%\end{align*}
%where $\oplus$ indicates that the bias term $b$ is added to each
%element of the $(n_l-k+1) \times (n_l-k+1)$ matrix $\bZ^l \ast \bW$.
%
\end{frame}
%
%
%
%
%\subsubsection{Multiple 3D Filters}
\begin{frame}{Bias and Activation Functions}
\framesubtitle{Multiple 3D Filters}
We can observe that one 3D filter $\bW$ with a corresponding bias
term $b$ results in a $(n_l-k+1) \times (n_l-k+1)$ matrix of
in layer $l+1$. 

\medskip

If we desire $m_{l+1}$ channels
in layer $l+1$, then we need $m_{l+1}$ different $k \times k \times m_l$ 
filters $\bW_q$ with a corresponding $b_q$, to
obtain the $(n_l-k+1) \times (n_l-k+1) \times m_{l+1}$ tensor 
at layer $l+1$:
\begin{align*}
    \bZ^{l+1} & = \biggl\{ z^{l+1}_{i,j,q} 
    = f\Bigl(\summ\bigl( \bZ^l_k(i,j) \odot \bW_q \bigr)
        + b_q
\Bigr) \biggr\}_{i,j=1,2,\ldots, n_l-k+1 \text{ and } q = 1,2, \ldots, m_{l+1}}
\end{align*}
%which can be written more compactly as
%\begin{align*}
%\bZ^{l+1} = f\Bigl(
%    \bigl(\bZ^l \ast \bW_1\bigr) \;\oplus\; b_1,\; 
%    \bigl(\bZ^l \ast \bW_2\bigr) \;\oplus\; b_2,\; \cdots,\;
%    \bigl(\bZ^l \ast \bW_{m_{l+1}}\bigr) \;\oplus\; b_{m_{l+1}}
%    \Bigr)
%\end{align*}
%where the activation function $f$ distributes over all of its arguments.
%
In summary, a convolution layer takes as input the $n_l \times n_l
\times m_l$ tensor $\bZ^l$ of neurons from layer $l$, 
and then computes the $n_{l+1} \times n_{l+1} \times m_{l+1}$ tensor
$\bZ^{l+1}$ of
neurons for the next layer $l+1$ via
the convolution of $\bZ^l$ with a set of $m_{l+1}$ different 3D filters of size $k
\times k \times m_l$, followed by adding the bias and applying some
non-linear activation function $f$. 

	\medskip

	Note that each 3D filter applied to
$\bZ^l$ results in a new channel in layer $l+1$. Therefore, $m_{l+1}$
filters are used to yield $m_{l+1}$ channels at layer $l+1$.
\end{frame}
%
%
%\begin{figure}[!t]
%    \vspace*{-0.1in}
\begin{frame}{Multiple 3D filters.}

Example shows a $4 \times 4 \times
    2$ tensor $\bZ^l$ with $n=4$ and $m=2$. It also shows two different $2
    \times 2 \times 2$ 
    filters $\bW_1$ and $\bW_2$ with $k=2$ and $r=2$. 

\medskip

Since $r=m=2$, the
    convolution of $\bZ^l$ and $\bW_i$ (for $i=1,2$) results in a $3
    \times 3$ matrix since $n-k+1 = 4-2+1 = 3$. 

\medskip

However, $\bW_1$ yields
    one channel and $\bW_2$ yields a second channel, so that the tensor
    for the next layer $\bZ^{l+1}$ has size $3 \times 3 \times 2$, with two
    channels (one per filter).

	\medskip

\scalebox{0.8}{
    \centering
    \tikzset{every node/.style={draw,fill=none}}
 \tikzstyle{edge from parent}=[-{Stealth[length=2mm]},thick,draw]
    % \tikzsetnextfilename{reg_deep_multiple_filters}
\begin{tikzpicture}[grow=right, level distance=4cm,
    level 1/.style={sibling distance=2cm}]
    % edge from parent/.style={draw,-latex}]
    \node (mA) [matrix,draw=none,matrix of nodes,ampersand replacement=\&] {%
        \node (x2nw) {3}; \& \node {1}; \& \node (x) {4}; \& \node(x2ne){1};\\
     \node {1}; \& \node{1}; \& \node{1}; \& \node{2};\\ 
     \node {3}; \& \node{0}; \& \node{1}; \& \node {2};\\ 
     \node(x2sw) {2}; \& \node{0}; \& \node{1}; \& \node{0};\\ 
  };
  \node (mB) [matrix,draw=none,matrix of nodes,ampersand replacement=\&] 
  at ($(mA.south west)+(2.75,0.25)$)
  {%
      \node (x1nw){1}; \& \node {2}; \& \node {2}; \& \node(x1ne){1};\\
     \node {3}; \& \node{1}; \& \node{4}; \& \node{2};\\ 
     \node {2}; \& \node{1}; \& \node{3}; \& \node {4};\\ 
     \node(x1sw) {1}; \& \node{2}; \& \node{3}; \& \node{1};\\ 
  }
    child {
        node (W2A) [matrix,draw=none,matrix of nodes,ampersand replacement=\&] {%
            \node (w2nw) {1}; \& \node (w2ne) {0};\\
            \node (w2sw) {0}; \& 1\\
    }
    node (W2B) [matrix,draw=none,matrix of nodes,ampersand replacement=\&] 
        at ($(W2A.south west)+(1.5,0)$)
        {%
            \node (w22nw) {1}; \& \node (w22ne) {0};\\
            \node (w22sw) {0}; \& 1\\
    }
    child {node (T2) [matrix,draw=none,matrix of nodes,ampersand replacement=\&] {
    %\node[draw=none]{$\qquad$}; \& 
    \node (t2nw) {6}; \& 8 \& \node[minimum width=0.575cm] (t2ne) {10};\\
    %\node[draw=none]{$\qquad$}; \& 
    5 \& 6 \& \node[minimum width=0.575cm]{11};\\
          %\node[draw=none]{$\qquad$}; \& 
    \node (t2sw) {7}; \& 5 \& \node[minimum width=0.575cm]{5};\\
      }
  edge from parent node[midway,draw=none,fill=white] {$=$}}
    edge from parent node[midway,draw=none,fill=white] {$\ast$}
    }
    child {
        node (W1A) [matrix,draw=none,matrix of nodes,ampersand replacement=\&] {
            \node (w1nw) {0}; \& \node (w1ne) {1};\\
            \node (w1sw) {1}; \& 0\\
    }
    node (W1B) [matrix,draw=none,matrix of nodes,ampersand replacement=\&] 
        at ($(W1A.south west)+(1.5,0)$)
        {%
            \node (w11nw) {0}; \& \node (w11ne) {1};\\
            \node (w11sw) {1}; \& 0\\
    }
    child {node (T1) [matrix,draw=none,matrix of nodes,ampersand
        replacement=\&] {
            \node (t1nw){7}; \& \node (z) {8}; \& \node(t1ne)[minimum width=0.575cm]{7}; \&
         \node[draw=none]{$\qquad\qquad$};\\
         7 \& 6 \& \node[minimum width=0.575cm]{8};\& \node[draw=none]{$\qquad\qquad$};\\
         \node (t1sw){4}; \& 6 \& \node[minimum width=0.575cm]{10};\& \node[draw=none]{$\qquad\qquad$};\\
      }
  edge from parent node[midway,draw=none,fill=white] {$=$}}
    edge from parent node[midway,draw=none,fill=white] {$\ast$}
};
\node[draw=none] at ([yshift={2mm}]z.north) {$\bZ^{l+1}$};
\node[draw=none] at ([yshift={2mm}]x.north west) {$\bZ^l$};
\node[draw=none] at ([yshift={2mm}]w1ne.north west) {$\bW_1$};
\node[draw=none] at ([yshift={2mm}]w2ne.north west) {$\bW_2$};
\draw[dashed](w1ne.north east)--(w11ne.north east);
\draw[dashed](w1nw.north west)--(w11nw.north west);
\draw[dashed](w1sw.south west)--(w11sw.south west);
\draw[dashed](w2ne.north east)--(w22ne.north east);
\draw[dashed](w2nw.north west)--(w22nw.north west);
\draw[dashed](w2sw.south west)--(w22sw.south west);
\draw[dashed](x1ne.north east)--(x2ne.north east);
\draw[dashed](x1nw.north west)--(x2nw.north west);
\draw[dashed](x1sw.south west)--(x2sw.south west);
\draw[dashed](t1ne.north east)--(t2ne.north east);
\draw[dashed](t1nw.north west)--(t2nw.north west);
\draw[dashed](t1sw.south west)--(t2sw.south west);
\end{tikzpicture}
}
\end{frame}
%% \vspace{0.1in}
%\caption{Multiple 3D filters.}
%\label{fig:reg:deep:multiple_filters}
%    \vspace*{-0.2in}
%\end{figure}
%
%
%% \begin{figure}[!ht]
%% \centering
%%     \tikzset{empty node/.style={draw=none,fill=none}}
%%     \begin{tikzpicture}[grow=right,draw=none]
%%         \node (a) {%
%%     \scalebox{0.75}{\LARGE
%%         \scantokens{
%%             \begin{tikzpicture}[every node/.style={draw, minimum
%%                 size=0.75cm}]
%%     \matrix [column sep=0mm, minimum height=5cm]
%% {
%%     \node {1}; & \node (x) {2}; & \node{2}; & \node{1};\\
%%     \node {3}; & \node{1}; & \node{4}; & \node{2}; 
%%     & &  \node (w){1}; & \node{0}; 
%%     & & \node{2}; & \node{6}; & \node{4};\\
%%     \node {2}; & \node{1}; & \node[fill=lightgray, thick]{3}; &
%%     \node[fill=lightgray, thick] (n4) {4}; 
%%     & \node[draw=none]{ }; 
%%     & \node{0}; & \node (w1){1}; 
%%     & \node[draw=none]{ }; 
%%     & \node{4}; & \node{4}; & \node{8};\\
%%     \node {1}; & \node{2}; & \node[fill=lightgray, thick]{3}; & \node[fill=lightgray, thick]{1}; 
%%     & & & & & \node{4}; & \node{4}; & \node[fill=gray, thick]{4};\\
%% };
%% \node[draw=none] at ([yshift={4mm}]w.45) {$\bW$};
%% \node[draw=none] at ([yshift={4mm}]x.60) {$\bX$};
%% \node[draw=none] at ([xshift={4mm},yshift={4mm}]n4.0) {\scalebox{1.3}{$\ast$}}; 
%% \node[draw=none] at ([xshift={4mm},yshift={4mm}]w1.0) {\scalebox{1}{$=$}}; 
%% \end{tikzpicture}
%% \endinput
%% }}}
%% child{
%%         \node (b) {%
%%     \scalebox{0.75}{\LARGE
%%         \scantokens{
%%     \begin{tikzpicture}[every node/.style={draw, minimum
%%                 size=0.75cm}]
%%     \matrix [column sep=0mm, minimum height=5cm]
%% {
%%     \node {1}; & \node (x) {2}; & \node{2}; & \node{1};\\
%%     \node {3}; & \node{1}; & \node{4}; & \node{2}; 
%%     & &  \node (w){1}; & \node{0}; 
%%     & & \node{2}; & \node{6}; & \node{4};\\
%%     \node {2}; & \node{1}; & \node[fill=lightgray, thick]{3}; &
%%     \node[fill=lightgray, thick] (n4) {4}; 
%%     & \node[draw=none]{ }; 
%%     & \node{0}; & \node (w1){1}; 
%%     & \node[draw=none]{ }; 
%%     & \node{4}; & \node{4}; & \node{8};\\
%%     \node {1}; & \node{2}; & \node[fill=lightgray, thick]{3}; & \node[fill=lightgray, thick]{1}; 
%%     & & & & & \node{4}; & \node{4}; & \node[fill=gray, thick]{4};\\
%% };
%% \node[draw=none] at ([yshift={4mm}]w.45) {$\bW$};
%% \node[draw=none] at ([yshift={4mm}]x.60) {$\bX$};
%% \node[draw=none] at ([xshift={4mm},yshift={4mm}]n4.0) {\scalebox{1.3}{$\ast$}}; 
%% \node[draw=none] at ([xshift={4mm},yshift={4mm}]w1.0) {\scalebox{1}{$=$}}; 
%% \end{tikzpicture}
%% \endinput}}}
%% };
%% \end{tikzpicture}
%% \caption{Multiple Filters}
%% \label{fig:reg:deep:multiple_filters}
%% \end{figure}
%
%
%
%
%% \vspace*{-0.1in}
%\begin{example}[Multiple 3D Filters]
%\begin{frame}{Multiple 3D Filters}
%%    \cref{fig:reg:deep:multiple_filters} 
%Figure shows how applying different filters
    %yield the channels for the next layer. 
%
%\medskip
%
%It shows a $4 \times 4 \times
    %2$ tensor $\bZ^l$ with $n=4$ and $m=2$. It also shows two different $2
    %\times 2 \times 2$ 
    %filters $\bW_1$ and $\bW_2$ with $k=2$ and $r=2$. 
%
%\medskip
%
%Since $r=m=2$, the
    %convolution of $\bZ^l$ and $\bW_i$ (for $i=1,2$) results in a $3
    %\times 3$ matrix since $n-k+1 = 4-2+1 = 3$. 
%
%\medskip
%
%However, $\bW_1$ yields
    %one channel and $\bW_2$ yields a second channel, so that the tensor
    %for the next layer $\bZ^{l+1}$ has size $3 \times 3 \times 2$, with two
    %channels (one per filter).
%\end{frame}
%\end{example}
%
%% \vspace*{-0.1in}
%\subsection{Padding and Striding}

\ifdefined\wox \begin{frame} \titlepage \end{frame} \fi


\begin{frame}{Padding and Striding}
One of the issues with the convolution operation is
that the size of the tensors will necessarily decrease in each
successive CNN layer. 

\medskip

If layer $l$ has size $n_l \times n_l \times m_l$,
and we use filters of size $k \times k \times m_l$, then each channel in
layer $l+1$ will have size $(n_l -k +1) \times (n_l -k +1)$. 

\medskip

That is the
number of rows and columns for each successive tensor will shrink by
$k-1$
and that will limit the number of layers the CNN can have. 
\end{frame}
%
%\index{padding}
%\index{convolutional neural networks!padding}
%\subsubsection{Padding} 
\begin{frame}{Padding}
To get around
this limitation, a simple solution is to pad each tensor
along both the rows and columns in each channel by some default value, 
typically zero. 

\medskip

For
uniformity, we always pad by adding the same number of rows at the top
and at
the bottom, and likewise the same number of columns on the left and on the right.
%That is, assume that we add $p$ rows both on top and bottom, and $p$ columns
%both on the left and right. 


\medskip

With padding $p$, the size of layer $l$ tensor is then $(n_l+2p) \times
(n_l+2p) \times m_l$. Assume that each filter is of size $k \times k \times
m_l$, and assume there are $m_{l+1}$ filters, then the size of the layer $l+1$ tensor will be
$(n_l+2p -k +1) \times (n_l+2p -k +1) \times m_{l+1}$. Since we want to
preserve the size of the resulting tensor:
\begin{align*}
    n_l+2p -k +1  \ge n_l, \text{which implies}, 
    p = \lB\lceil \frac{k-1}{2} \rB\rceil
\end{align*}
With padding, we can have arbitrarily
deep convolutional layers in a CNN.
\end{frame}
%
%
%\begin{figure}[!t]
\begin{frame}{Padding 2D Convolution}
\framesubtitle{$p=0$ e $p=1$}

\begin{minipage}{4cm}
\begin{small}
Example shows a 2D convolution without and with
    padding. 
It starts with a convolution of a $5
    \times 5$ matrix $\bZ^l$ ($n=5$) with a $3 \times 3$ filter $\bW$
    ($k=3$), which results in a $3 \times 3$ matrix since $n-k+1 = 5-3+1
    = 3$. Thus, the size of the next layer $\bZ^{l+1}$ has
    decreased.

\medskip
    
    This way, we can
    chain together as many convolution layers as desired, without decrease in the
    size of the layers.
\end{small}
\end{minipage}
\hspace*{1.5cm}
\begin{minipage}{5cm}
\centering
    \tikzset{empty node/.style={draw=none,fill=none}}
    \centerline{
%        \subfloat[No padding: $p=0$]{
%        \label{fig:reg:deep:padding:a}
    \scalebox{0.4}{\LARGE
        \scantokens{
    % \tikzsetnextfilename{reg_deep_padding_a}
\begin{tikzpicture}[every node/.style={draw, minimum size=1cm}]
    \matrix [column sep=0mm, minimum height=6.5cm]
{
    \node {1}; & \node {2}; & \node (x) {2}; & \node{1}; & \node{1};\\
    \node {3}; & \node{1}; & \node{4}; & \node{2};  & \node{1};
    & &  \node {1}; & \node (w) {0}; &\node{0};  
    & & \node{7}; & \node (z) {11}; & \node{9};\\
    \node {2}; & \node{1}; & \node{3}; &
    \node {4}; & \node (n4) {3};
    & \node[draw=none]{ }; 
    & \node{0}; & \node{1}; & \node (w1){1};
    & \node[draw=none]{ }; 
    & \node{9}; & \node{11}; & \node{12};\\
    \node {1}; & \node{2}; & \node{3}; & \node{1}; & 
    \node{1}; & & \node{0}; & \node{1}; & \node{0}; & & \node{8}; &
    \node{8}; & \node{7};\\
    \node {4}; & \node{1}; & \node{3}; & \node{2}; &
    \node{1};\\
};
\node[draw=none] at ([yshift={4mm}]w.north) {$\bW$};
\node[draw=none] at ([yshift={4mm}]x.north) {$\bZ^l$};
\node[draw=none] at ([yshift={4mm}]z.north) {$\bZ^{l+1}$};
\node[draw=none] at ([xshift={4mm},yshift={1mm}]n4.0) {\scalebox{1.3}{$\ast$}}; 
\node[draw=none] at ([xshift={4mm},yshift={1mm}]w1.0) {\scalebox{1}{$=$}}; 
\end{tikzpicture}
\endinput}}}%}
    \centerline{
%        \subfloat[Padding: $p=1$]{
%        \label{fig:reg:deep:padding:b}
    \scalebox{0.45}{\LARGE
        \scantokens{
    % \tikzsetnextfilename{reg_deep_padding_b}
\begin{tikzpicture}[every node/.style={draw, minimum size=1cm}]
    \matrix [column sep=0mm, minimum height=8.5cm]
{
    \node[fill=lightgray]{0}; & \node[fill=lightgray]{0}; & \node[fill=lightgray]{0}; & \node [fill=lightgray](x){0}; &\node[fill=lightgray]{0}; &\node[fill=lightgray]{0}; &\node[fill=lightgray]{0};\\
    \node[fill=lightgray]{0}; & \node {1}; & \node {2}; & \node{2}; & \node{1}; &
    \node{1}; &\node[fill=lightgray]{0}; & & & & & & 
    \node{6}; & \node{5}; & \node (z) {7}; & \node{4}; & \node{2};\\
    \node[fill=lightgray]{0}; & \node {3}; & \node{1}; & \node{4}; & \node{2};  & \node{1};
    &\node[fill=lightgray]{0};
    & &  \node{1}; & \node (w) {0}; & \node{0};
    & & \node{6}; & \node{7}; & \node{11}; & \node{9}; & \node{5};\\
    \node[fill=lightgray]{0}; & \node {2}; & \node{1}; & \node{3}; &
    \node {4}; & \node {3};
    &\node[fill=lightgray] (n4){0}; & \node[draw=none]{ }; 
    & \node{0}; & \node{1}; & \node (w1) {1}; 
    & \node[draw=none]{ }; & 
    \node{4}; & \node{9};  & \node{11}; & \node{12}; & \node{6};\\
    \node[fill=lightgray]{0}; & \node {1}; & \node{2}; & \node{3}; & \node{1}; & 
    \node{1}; &\node[fill=lightgray]{0};& & \node{0};& \node{1};&
    \node{0};& & \node{7}; & \node{8};  &\node{8}; & \node{7}; & \node{6};\\
    \node[fill=lightgray]{0}; & \node {4}; & \node{1}; & \node{3}; & \node{2}; &
    \node{1};&\node[fill=lightgray]{0}; & & & & & & \node{5}; &
    \node{5}; & \node{7}; & \node{6}; & \node{2}; \\
    \node[fill=lightgray]{0}; & \node[fill=lightgray]{0};& \node[fill=lightgray]{0}; & \node[fill=lightgray]{0}; &\node[fill=lightgray]{0}; &\node[fill=lightgray]{0}; &\node[fill=lightgray]{0};\\
};
\node[draw=none] at ([yshift={4mm}]w.90) {$\bW$};
\node[draw=none] at ([yshift={4mm}]x.90) {$\bZ^l$};
\node[draw=none] at ([yshift={4mm}]z.north) {$\bZ^{l+1}$};
\node[draw=none] at ([xshift={4mm},yshift={1mm}]n4.0) {\scalebox{1.3}{$\ast$}}; 
\node[draw=none] at ([xshift={4mm},yshift={1mm}]w1.0) {\scalebox{1}{$=$}}; 
\end{tikzpicture}
\endinput}}}
\end{minipage}
\end{frame}
%}
%\vspace{0.2in}
%\caption{Padding: 2D Convolution
%    \protect\subref{fig:reg:deep:padding:a}     without padding,
%    and 
%\protect\subref{fig:reg:deep:padding:b} with padding.}
%\label{fig:reg:deep:padding}
%\end{figure}
%
%
%
%
%\begin{example}[Padding]
%\begin{frame}{Padding}
%\framesubtitle{Example}
%%    \cref{fig:reg:deep:padding} 
%Figure shows a 2D convolution without and with
    %padding. %\cref{fig:reg:deep:padding:a} 
%It starts with a convolution of a $5
    %\times 5$ matrix $\bZ^l$ ($n=5$) with a $3 \times 3$ filter $\bW$
    %($k=3$), which results in a $3 \times 3$ matrix since $n-k+1 = 5-3+1
    %= 3$. Thus, the size of the next layer $\bZ^{l+1}$ has
    %decreased.
%
%
%\medskip
%
%
    %On the other hand, zero padding $\bZ^l$ using $p=1$ results in a $7 \times
    %7$ matrix as shown in the figure %\cref{fig:reg:deep:padding:b}. 
%Since $p=1$, we
    %have an extra row of zeros on the top and bottom, and an extra
    %column of zeros on the left and right. The convolution of the
    %zero-padded $\bX$ with $\bW$ now results in a $5 \times 5$ matrix
    %$\bZ^{l+1}$
    %(since $7-3+1 = 5$), which preserves the size. 
%
%
%\medskip
%
    %
    %If we wanted to apply another convolution layer, we could
    %zero pad the resulting matrix $\bZ^{l+1}$ with $p=1$, which would again yield a
    %$5 \times 5$ matrix for the next layer, using a $3 \times 3$ filter. 
    %This way, we can
    %chain together as many convolution layers as desired, without decrease in the
    %size of the layers.
%\end{frame}
%\end{example}
%
%
%
%% \vspace*{-0.2in}
%\index{striding}
%\index{convolutional neural networks!striding}
%\subsubsection{Striding}
\begin{frame}{Striding}
Striding is often used to sparsify the number of sliding windows used
in the convolutions.% That is, 

\medskip

Instead of considering all possible windows
we increment the index along both rows and columns by an integer value
$s \ge 1$ called the {\em stride}. 

\medskip

A 3D convolution of $\bZ^l$ % of size
	($n_l \times n_l \times m_l$) with a
	$\bW$ ($k \times k \times m_l$), using stride $s$:

\centerline{
\begin{footnotesize}
   \scalebox{0.9}{\parbox{\linewidth}{\[
               \!\!\!\!\!\!
               \!\!\!\!\!\!
               \!\!\!\!\!\!
               \!\!\!\!\!\!
    \bZ^l \ast \bW = \matr{ &&&\\[-1em]
        \summ\bigl(\bZ^l_k(1,1) \odot \bW\bigr) &
        \summ\bigl(\bZ^l_k(1,1+s)\odot \bW\bigr) &
        \cdots & \summ\bigl(\bZ^l_k(1, 1+t \cdot s) \odot \bW\bigr)\\
        \summ\bigl(\bZ^l_k(1+s,1) \odot \bW\bigr) &
        \summ\bigl(\bZ^l_k(1+s,1+s)\odot \bW\bigr) &
        \cdots & \summ \bigl(\bZ^l_k(1+s, 1+t\cdot s) \odot \bW\bigr)\\
        \vdots & \vdots & \cdots& \vdots\\
        \summ\bigl(\bZ^l_k(1+t\cdot s,1) \odot \bW\bigr) &
        \summ\bigl(\bZ^l_k(1+t\cdot s,1+s)\odot \bW\bigr) &
        \cdots & 
        \summ \bigl(\bZ^l_k(1+t\cdot s, 1+t\cdot s) \odot
        \bW\bigr)\\[-1em]
        &&&}%
\]}}
\end{footnotesize}
}
%% \begin{align*}
%%     \bZ^l \ast_s \bW = 
%%     \resizebox{0.9\hsize}{!}{
%%         \begin{blockarray}{cccc}
%%             \begin{block}{|c|c|c|c|}
%%             \hline
%%         $\summ\bigl(\bZ^l_k(1,1) \odot \bW\bigr)$ &
%%         $\summ\bigl(\bZ^l_k(1,1+s)\odot \bW\bigr)$ &
%%         $\cdots$ & $\summ\bigl(\bZ^l_k(1, 1+t \cdot s) \odot \bW\bigr)$\\
%%             \hline
%%         $\summ\bigl(\bZ^l_k(1+s,1) \odot \bW\bigr)$ &
%%         $\summ\bigl(\bZ^l_k(1+s,1+s)\odot \bW\bigr)$ &
%%         $\cdots$ & $\summ \bigl(\bZ^l_k(1+s, 1+t\cdot s) \odot \bW\bigr)$\\
%%             \hline
%%         $\vdots$ & $\vdots$ & $\cdots$ & $\vdots$\\
%%             \hline
%%         $\summ\bigl(\bZ^l_k(1+t\cdot s,1) \odot \bW\bigr)$ &
%%         $\summ\bigl(\bZ^l_k(1+t\cdot s,1+s)\odot \bW\bigr)$ &
%%         $\cdots$ & 
%%         $\summ \bigl(\bZ^l_k(1+t\cdot s, 1+t\cdot s) \odot \bW\bigr)$\\
%%             \hline
%%     \end{block}
%%     \end{blockarray}
%% }%
%% \end{align*}
\noindent where $t = \lB\lfloor \frac{n_l-k}{s} \rB\rfloor$.

\medskip

We can observe that using stride $s$, the convolution of $\bZ^l \in
\setR^{n_l \times n_l \times m_l}$ with
$\bW \in \setR^{k \times k \times m_l}$ results in a $(t+1) \times (t+1)$ matrix.
\end{frame}
%% \begin{align*}
%%     \bZ^l \ast_s \bW = \biggl\{
%%         \summ\Bigl(\bZ^l_k\bigl(1+(i-1)\cdot s, 1+(j-1)\cdot s\bigr) \odot \bW
%%     \Bigr)\biggr\}
%% \end{align*}
%% for  $i,j = 1, 2, \cdots,  \lB\lceil \frac{n_l-k+1}{s} \rB\rceil$.
%
%% \begin{align*}
%%     \bZ \ast_s \bW = \biggl\{
%%         \summ\Bigl(\bZ^l_k(i,j) \odot_s \bW = 
%%     \sum_{a=1}^k \sum_{b=1}^k \sum_{c=1}^m
%%     \bZ^l[(i-1)s+a,\; (j-1)s+b,\; c] \cdot \bW[a, b, c]\\
%%     \text{for}\quad & i,j = 1, 2, \cdots, 
%%     \lB\lceil \frac{n_l-k+1}{s} \rB\rceil
%% \end{align*}
%
%
%\begin{figure}[!t]
%    \vspace{-0.3in}
\begin{frame}{Striding}
\framesubtitle{Example}
%    \cref{fig:reg:deep:striding} 
Example shows 2D convolution using stride $s=2$
    on a $5 \times 5$ matrix $\bZ^l$ ($n_l=5$) with a filter $\bW$ of size $3
    \times 3$ ($k=3$).

	\medskip

    Instead of the default stride of one, which would result in a $3
    \times 3$ matrix, we get a $(t+1) \times (t+1) = 2 \times 2$ matrix
    $\bZ^{l+1}$, since
    \begin{align*}
        t=\floor[\Big]{\frac{n_l-k}{s}} = \floor[\Big]{\frac{5-3}{2}} = 1
    \end{align*}

    We can see that the next window index increases by $s$ along the rows
    and columns. 

	\medskip

	For example, the first window is $\bZ^l_3(1,1)$ and
    thus the second window is $\bZ^l_3(1,1+s) = \bZ^l_3(1,3)$.
%    (see \cref{fig:reg:deep:striding:a,fig:reg:deep:striding:b}). 
%    % (see \cref{fig:reg:deep:striding}\subref{fig:reg:deep:striding:a}
%    % --\subref{fig:reg:deep:striding:b}). 
    Next, we move down by a stride of
    $s=2$, so that the third window is $\bZ^l_3(1+s,1) = \bZ^l_3(3,1)$,
    and the final window is $\bZ^l_3(3,1+s)
    = \bZ^l_3(3,3)$ %(see
%        \cref{fig:reg:deep:striding:c,fig:reg:deep:striding:d}).
%        % \cref{fig:reg:deep:striding}\subref{fig:reg:deep:striding:c}
%    % and \subref{fig:reg:deep:striding:d}).
\end{frame}
\begin{frame}{Striding}
\framesubtitle{Example: $s=2$}
\centering
    \tikzset{empty node/.style={draw=none,fill=none}}
    \centerline{
%        \subfloat[$\summ(\bZ^l_3(1,1) \odot \bW)$]{
%\label{fig:reg:deep:striding:a}
    \scalebox{0.5}{\LARGE
        \scantokens{
    % \tikzsetnextfilename{reg_deep_striding_a}
\begin{tikzpicture}[every node/.style={draw, minimum size=1cm}]
    \matrix [column sep=0mm, minimum height=6.5cm]
{
    \node[fill=lightgray] {1}; & \node[fill=lightgray] {2}; &
    \node[fill=lightgray] (x) {2}; & \node{1}; & \node{1};\\
    \node[fill=lightgray] {3}; & \node[fill=lightgray]{1}; &
    \node[fill=lightgray]{4}; & \node{2};  & \node{1};
    & &  \node {1}; & \node (w) {0}; &\node{0};  
    & & \node[fill=gray] (z) {7}; & \node{ };\\
    \node[fill=lightgray] {2}; & \node[fill=lightgray]{1}; &
    \node[fill=lightgray]{3}; &
    \node {4}; & \node (n4) {3};
    & \node[draw=none]{ }; 
    & \node{0}; & \node{1}; & \node (w1){1};
    & \node[draw=none]{ }; 
    & \node{ }; & \node{ };\\
    \node {1}; & \node{2}; & \node{3}; & \node{1}; & 
    \node{1}; & & \node{0}; & \node{1}; & \node{0};\\
    \node {4}; & \node{1}; & \node{3}; & \node{2}; &
    \node{1};\\
};
\node[draw=none] at ([yshift={4mm}]w.90) {$\bW$};
\node[draw=none] at ([yshift={4mm}]x.90) {$\bZ^l$};
\node[draw=none] at ([yshift={4mm}]z.north east) {$\bZ^{l+1}$};
\node[draw=none] at ([xshift={4mm}]n4.0) {\scalebox{1.3}{$\ast$}}; 
\node[draw=none] at ([xshift={4mm}]w1.0) {\scalebox{1}{$=$}}; 
\end{tikzpicture}
\endinput}}%}
%\subfloat[$\summ(\bZ^l_3(1,3) \odot \bW)$]{
    \scalebox{0.5}{\LARGE
%\label{fig:reg:deep:striding:b}
        \scantokens{
    % \tikzsetnextfilename{reg_deep_striding_b}
\begin{tikzpicture}[every node/.style={draw, minimum size=1cm}]
    \matrix [column sep=0mm, minimum height=6.5cm]
{
    \node {1}; & \node {2}; &
    \node[fill=lightgray] (x) {2}; & \node[fill=lightgray]{1}; & \node[fill=lightgray]{1};\\
    \node{3}; & \node{1}; &
    \node[fill=lightgray]{4}; & \node[fill=lightgray]{2};  & \node[fill=lightgray]{1};
    & &  \node {1}; & \node (w) {0}; &\node{0};  
    & & \node (z) {7}; & \node[fill=gray]{9};\\
    \node{2}; & \node{1}; &
    \node[fill=lightgray]{3}; &
    \node[fill=lightgray] {4}; & \node[fill=lightgray] (n4) {3};
    & \node[draw=none]{ }; 
    & \node{0}; & \node{1}; & \node (w1){1};
    & \node[draw=none]{ }; 
    & \node{ }; & \node{ };\\
    \node {1}; & \node{2}; & \node{3}; & \node{1}; & 
    \node{1}; & & \node{0}; & \node{1}; & \node{0};\\
    \node {4}; & \node{1}; & \node{3}; & \node{2}; &
    \node{1};\\
};
\node[draw=none] at ([yshift={4mm}]w.90) {$\bW$};
\node[draw=none] at ([yshift={4mm}]x.90) {$\bZ^l$};
\node[draw=none] at ([yshift={4mm}]z.north east) {$\bZ^{l+1}$};
\node[draw=none] at ([xshift={4mm}]n4.0) {\scalebox{1.3}{$\ast$}}; 
\node[draw=none] at ([xshift={4mm}]w1.0) {\scalebox{1}{$=$}}; 
\end{tikzpicture}
\endinput}}}
%}
    \centerline{
%        \subfloat[$\summ(\bZ^l_3(3,1) \odot \bW)$]{
%\label{fig:reg:deep:striding:c}
    \scalebox{0.5}{\LARGE
        \scantokens{
    % \tikzsetnextfilename{reg_deep_striding_c}
\begin{tikzpicture}[every node/.style={draw, minimum size=1cm}]
    \matrix [column sep=0mm, minimum height=6.5cm]
{
    \node {1}; & \node {2}; &
    \node (x) {2}; & \node{1}; & \node{1};\\
    \node {3}; & \node{1}; &
    \node{4}; & \node{2};  & \node{1};
    & &  \node {1}; & \node (w) {0}; &\node{0};  
    & & \node (z) {7}; & \node{9};\\
    \node[fill=lightgray] {2}; & \node[fill=lightgray]{1}; &
    \node[fill=lightgray]{3}; &
    \node {4}; & \node (n4) {3};
    & \node[draw=none]{ }; 
    & \node{0}; & \node{1}; & \node (w1){1};
    & \node[draw=none]{ }; 
    & \node[fill=gray]{8}; & \node{ };\\
    \node[fill=lightgray] {1}; & \node[fill=lightgray]{2}; & \node[fill=lightgray]{3}; & \node{1}; & 
    \node{1}; & & \node{0}; & \node{1}; & \node{0};\\
    \node[fill=lightgray] {4}; & \node[fill=lightgray]{1}; & \node[fill=lightgray]{3}; & \node{2}; &
    \node{1};\\
};
\node[draw=none] at ([yshift={4mm}]w.90) {$\bW$};
\node[draw=none] at ([yshift={4mm}]x.90) {$\bZ^l$};
\node[draw=none] at ([yshift={4mm}]z.north east) {$\bZ^{l+1}$};
\node[draw=none] at ([xshift={4mm}]n4.0) {\scalebox{1.3}{$\ast$}}; 
\node[draw=none] at ([xshift={4mm}]w1.0) {\scalebox{1}{$=$}}; 
\end{tikzpicture}
\endinput}}%}
%\subfloat[$\summ(\bZ^l_3(3,3) \odot \bW)$]{
%\label{fig:reg:deep:striding:d}
    \scalebox{0.5}{\LARGE
        \scantokens{
    % \tikzsetnextfilename{reg_deep_striding_d}
\begin{tikzpicture}[every node/.style={draw, minimum size=1cm}]
    \matrix [column sep=0mm, minimum height=6.5cm]
{
    \node {1}; & \node {2}; &
    \node (x) {2}; & \node{1}; & \node{1};\\
    \node {3}; & \node{1}; &
    \node{4}; & \node{2};  & \node{1};
    & &  \node {1}; & \node (w) {0}; &\node{0};  
    & & \node (z) {7}; & \node{9};\\
    \node {2}; & \node{1}; &
    \node[fill=lightgray]{3}; &
    \node[fill=lightgray] {4}; & \node[fill=lightgray] (n4) {3};
    & \node[draw=none]{ }; 
    & \node{0}; & \node{1}; & \node (w1){1};
    & \node[draw=none]{ }; 
    & \node{8}; & \node[fill=gray]{7};\\
    \node {1}; & \node{2}; & \node[fill=lightgray]{3}; & \node[fill=lightgray]{1}; & 
    \node[fill=lightgray]{1}; & & \node{0}; & \node{1}; & \node{0};\\
    \node {4}; & \node{1}; & \node[fill=lightgray]{3}; & \node[fill=lightgray]{2}; &
    \node[fill=lightgray]{1};\\
};
\node[draw=none] at ([yshift={4mm}]w.90) {$\bW$};
\node[draw=none] at ([yshift={4mm}]x.90) {$\bZ^l$};
\node[draw=none] at ([yshift={4mm}]z.north east) {$\bZ^{l+1}$};
\node[draw=none] at ([xshift={4mm}]n4.0) {\scalebox{1.3}{$\ast$}}; 
\node[draw=none] at ([xshift={4mm}]w1.0) {\scalebox{1}{$=$}}; 
\end{tikzpicture}
\endinput}}}
\end{frame}
%}
%\vspace{0.2in}
%\caption{Striding: 2D Convolution with stride $s=2$.}
%\label{fig:reg:deep:striding}
%    \vspace*{-0.1in}
%\end{figure}
%
%% \vspace*{-0.1in}
%\begin{example}[Striding]
%\end{example}
%
%
%% \enlargethispage{1\baselineskip}
%% \vspace*{-0.2in}
%\subsection{Generalized Aggregation Functions: Pooling}
%\index{convolutional neural networks!pooling}
%\index{pooling}
\begin{frame}{Pooling}
%Let $\bZ^l$ be a $n_l \times n_l \times m_l$ tensor at layer $l$.
%Our discussion of convolutions so far assumes that we sum together
%all of the elements in the element-wise product of the $k \times k \times
%r$ subtensor $\bZ^l_k(i,j,q)$ and the filter $\bW$. In fact, 
CNNs also use
other types of aggregation functions in addition to summation, such as
average and maximum.

\medskip

%
%
%% \vspace*{-0.1in}
%\index{convolutional neural networks!avg-pooling}
%\index{pooling!avg-pooling}
%\subsubsection{Avg-Pooling}
\textbf{Avg-Pooling}
average over the
element-wise product of $\bZ^l_k(i,j,q)$ and $\bW$:
\begin{align*}
    \avg\bigl(\bZ^l_k(i,j,q) \odot \bW\bigr) & = 
   \displaystyle \avg_{\substack{a = 1,2,\cdots,k\\
   b=1,2,\cdots,k\\c=1,2,\cdots,r}}
    \lB\{z^l_{i+a-1, j+b-1, q+c-1} \cdot w_{a, b, c} \rB\} \\
     & = \frac{1}{k^2 \cdot r}  \cdot 
    \summ\bigl(\bZ^l_k(i,j,q) \odot \bW\bigr) 
     % \sum_{a=1}^k \sum_{b=1}^k \sum_{c=1}^r
 % x_{i+a-1, j+b-1, q+c-1} \cdot w_{a, b, c} 
\end{align*}
%
%\index{convolutional neural networks!max-pooling}
%\index{pooling!max-pooling}
%\subsubsection{Max-Pooling}
\textbf{Max-Pooling} 
maximum over the
element-wise product of $\bZ^l_k(i,j,q)$ and $\bW$:
\begin{align*}
    \tcbhighmath{
    \max\bigl(\bZ^l_k(i,j,q) \odot \bW\bigr) = 
   \displaystyle \max_{\substack{a =
   1,2,\cdots,k\\b=1,2,\cdots,k\\c=1,2,\cdots,r}}
    \lB\{z^l_{i+a-1, j+b-1, q+c-1} \cdot w_{a, b, c} \rB\} }
\end{align*}
\end{frame}
%The 3D convolution of $\bZ^l \in \setR^{n_l \times n_l \times m_l}$ with filter
%$\bW \in \setR^{k \times k \times r}$ using max-pooling,
%denoted $\bZ^{l} \ast_{\max} \bW$, results in a
%$(n_l - k +1) \times (n_l-k+1) \times (m_l-r+1)$ tensor, given as:
%
%\centerline{
%\begin{small}
%   \scalebox{0.81}{\parbox{\linewidth}{\[
%               \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!
%               \bZ^{l} \ast_{\max} \bW =  
%    \begin{array}{ccccc}
%            \ldelim({4}{*} & \max\bigl(\bX_k(1,1,1) \odot \bW\bigr) &
%            \cdots & 
%            \max\bigl(\bX_k(1, n-k+1,1) \odot \bW\bigr) & \rdelim){4}{*}\\
%            & \max\bigl(\bX_k(2,1,1) \odot \bW\bigr) &
%            \cdots & 
%            \max \bigl(\bX_k(2, n-k+1,1) \odot \bW\bigr) &\\
%        & \vdots & 
%        \cdots & 
%        \vdots & \\
%        &\max\bigl(\bX_k(n-k+1,1,1) \odot \bW\bigr) &
%        \cdots & 
%        \max \bigl(\bX_k(n-k+1, n-k+1,1) \odot \bW\bigr)&\\
%        \cline{2-4}
%        %
%        \ldelim({4}{*} & \max\bigl(\bX_k(1,1,2) \odot \bW\bigr) &
%            \cdots & \max\bigl(\bX_k(1, n-k+1,2) \odot
%        \bW\bigr) & \rdelim){4}{*}\\
%           &  \max\bigl(\bX_k(2,1,2) \odot \bW\bigr) &
%            \cdots & \max \bigl(\bX_k(2, n-k+1,2) \odot \bW\bigr) &\\
%       & \vdots & \cdots & \vdots &\\
%        &\max\bigl(\bX_k(n-k+1,1,2) \odot \bW\bigr) &
%        \cdots & \max \bigl(\bX_k(n-k+1, n-k+1,2) \odot
%        \bW\bigr)& \\
%        \cline{2-4}
%        & \vdots & 
%        \vdots & 
%        \vdots & \\
%        %
%            \ldelim({4}{*} &\max\bigl(\bX_k(1,1,m-r+1) \odot \bW\bigr) &
%            \cdots & 
%            \max\bigl(\bX_k(1, n-k+1,m-r+1) \odot
%        \bW\bigr) & \rdelim){4}{*}\\
%            & \max\bigl(\bX_k(2,1,m-r+1) \odot \bW\bigr) &
%            \cdots & 
%            \max \bigl(\bX_k(2, n-k+1,m-r+1) \odot \bW\bigr)& \\
%        & \vdots & 
%        \cdots & 
%        \vdots & \\
%        & \max\bigl(\bX_k(n-k+1,1,m-r+1) \odot \bW\bigr) &
%        \cdots & 
%        \max \bigl(\bX_k(n-k+1, n-k+1,m-r+1) \odot \bW\bigr) &\\
%  \end{array}
%  \]}}
%\end{small}
%}
%
%% \begin{align*}
%%     \large
%%     \bZ^l \ast_{\max} \bW = 
%%     \resizebox{0.85\hsize}{!}{
%%     \begin{blockarray}{cccc}
%%         \begin{block}{|c|c|c|c|}
%%         \hline
%%             $\max\bigl(\bX_k(1,1,1) \odot \bW\bigr)$ &
%%             $\max\bigl(\bX_k(1,2,1)\odot \bW\bigr)$ &
%%             $\cdots$ & 
%%             $\max\bigl(\bX_k(1, n_l-k+1,1) \odot \bW\bigr)$\\
%%         \hline
%%             $\max\bigl(\bX_k(2,1,1) \odot \bW\bigr)$ &
%%             $\max\bigl(\bX_k(2,2,1)\odot \bW\bigr)$ &
%%             $\cdots$ & 
%%             $\max \bigl(\bX_k(2, n_l-k+1,1) \odot \bW\bigr)$\\
%%         \hline
%%         $\vdots$ & 
%%         $\vdots$ & 
%%         $\cdots$ & 
%%         $\vdots$\\
%%         \hline
%%         $\max\bigl(\bX_k(n_l-k+1,1,1) \odot \bW\bigr)$ &
%%         $\max\bigl(\bX_k(n_l-k+1,2,1)\odot \bW\bigr)$ &
%%         $\cdots$ & 
%%         $\max \bigl(\bX_k(n_l-k+1, n_l-k+1,1) \odot \bW\bigr)$\\
%%         \hline
%% \end{block}
%% \begin{block}{cccc}
%%     &&&\\
%% \end{block}
%%     \begin{block}{|c|c|c|c|}
%%         \hline
%%             $\max\bigl(\bX_k(1,1,2) \odot \bW\bigr)$ &
%%             $\max\bigl(\bX_k(1,2,2)\odot \bW\bigr)$ &
%%             $\cdots$ & $\max\bigl(\bX_k(1, n_l-k+1,2) \odot \bW\bigr)$\\
%%         \hline
%%             $\max\bigl(\bX_k(2,1,2) \odot \bW\bigr)$ &
%%             $\max\bigl(\bX_k(2,2,2)\odot \bW\bigr)$ &
%%             $\cdots$ & $\max \bigl(\bX_k(2, n_l-k+1,2) \odot \bW\bigr)$\\
%%         \hline
%%         $\vdots$ & $\vdots$ & $\cdots$ & $\vdots$\\
%%         \hline
%%         $\max\bigl(\bX_k(n_l-k+1,1,2) \odot \bW\bigr)$ &
%%         $\max\bigl(\bX_k(n_l-k+1,2,2)\odot \bW\bigr)$ &
%%         $\cdots$ & $\max \bigl(\bX_k(n_l-k+1, n_l-k+1,2) \odot \bW\bigr)$\\
%%         \hline
%% \end{block}
%% \begin{block}{cccc}
%%     &&&\\
%% \end{block}
%% \begin{block}{cccc}
%%     $\vdots$ &$\vdots$ &$\vdots$ &$\vdots$ \\
%% \end{block}
%% \begin{block}{cccc}
%%     &&&\\
%% \end{block}
%%     \begin{block}{|c|c|c|c|}
%%         \hline
%%             $\max\bigl(\bX_k(1,1,m_l-r+1) \odot \bW\bigr)$ &
%%             $\max\bigl(\bX_k(1,2,m_l-r+1)\odot \bW\bigr)$ &
%%             $\cdots$ & 
%%             $\max\bigl(\bX_k(1, n_l-k+1,m_l-r+1) \odot \bW\bigr)$\\
%%         \hline
%%             $\max\bigl(\bX_k(2,1,m_l-r+1) \odot \bW\bigr)$ &
%%             $\max\bigl(\bX_k(2,2,m_l-r+1)\odot \bW\bigr)$ &
%%             $\cdots$ & 
%%             $\max \bigl(\bX_k(2, n_l-k+1,m_l-r+1) \odot \bW\bigr)$\\
%%         \hline
%%         $\vdots$ & 
%%         $\vdots$ & 
%%         $\cdots$ & 
%%         $\vdots$\\
%%         \hline
%%         $\max\bigl(\bX_k(n_l-k+1,1,m_l-r+1) \odot \bW\bigr)$ &
%%         $\max\bigl(\bX_k(n_l-k+1,2,m_l-r+1)\odot \bW\bigr)$ &
%%         $\cdots$ & 
%%         $\max \bigl(\bX_k(n_l-k+1, n_l-k+1,m_l-r+1) \odot \bW\bigr)$\\
%%         \hline
%% \end{block}
%% \end{blockarray}
%% }
%% \end{align*}
%
%% \begin{align*}
%%     \bZ^l \ast_{\max} \bW = 
%%     \resizebox{0.8\hsize}{!}{
%%         \begin{blockarray}{cccc}
%%             \begin{block}{|c|c|c|c|}
%%             \hline
%%         $\max\bigl(\bZ^l_k(1,1) \odot \bW\bigr)$ &
%%         $\max\bigl(\bZ^l_k(1,2)\odot \bW\bigr)$ &
%%         $\cdots$ & $\max\bigl(\bZ^l_k(1, n_l-k+1) \odot \bW\bigr)$\\
%%             \hline
%%         $\max\bigl(\bZ^l_k(2,1) \odot \bW\bigr)$ &
%%         $\max\bigl(\bZ^l_k(2,2)\odot \bW\bigr)$ &
%%         $\cdots$ & $\max \bigl(\bZ^l_k(2, n_l-k+1) \odot \bW\bigr)$\\
%%             \hline
%%         $\vdots$ & $\vdots$ & $\cdots$ & $\vdots$\\
%%             \hline
%%         $\max\bigl(\bZ^l_k(n_l-k+1,1) \odot \bW\bigr)$ &
%%         $\max\bigl(\bZ^l_k(n_l-k+1,2)\odot \bW\bigr)$ &
%%         $\cdots$ & 
%%         $\max \bigl(\bZ^l_k(n_l-k+1, n_l-k+1) \odot \bW\bigr)$\\
%%             \hline
%%     \end{block}
%%     \end{blockarray}
%% }%
%% \end{align*}
%
\begin{frame}{Max-Pooling in CNNs}
%\paragraph{Max-pooling in CNNs}
Typically, max-pooling is used more often than avg-pooling. Also, 
for pooling it is very common to set the stride equal to the filter size
($s=k$), so that the aggregation
function is applied over disjoint $k \times k$ windows in each channel in
$\bZ^l$.

\medskip

%%although
%%any value of stride can be used. 

More importantly, in pooling, the filter $\bW$ is by default taken to be a 
$k \times k \times 1$ tensor all of whose weights are fixed as $1$, so
that $\bW = \bone_{k \times k \times 1}$. 

	\medskip

	In other words, the filter
weights are fixed at 1 and are not updated during backpropagation.
Further, the filter uses a fixed zero bias 
(that is, $b = 0$).


\medskip

Finally, note that pooling implicitly uses an identity activation function.
As such, the convolution of $\bZ^l \in \setR^{n_l \times n_l
\times m_l}$ with $\bW \in
\setR^{k \times k \times 1}$, using stride
$s=k$, results in a tensor $\bZ^{l+1}$ 
of size $\lB\lfloor\frac{n_l}{s}\rB\rfloor
\times \lB\lfloor\frac{n_l}{s}\rB\rfloor \times m_l$.
\end{frame}
%
%
%\begin{figure}[!b]
\begin{frame}{Max-Pooling}
\framesubtitle{Example}
%    \cref{fig:reg:deep:maxpooling} 
Example shows max-pooling on a $4 \times 4$ matrix
    $\bZ^l$ ($n_l=4$), using window size $k=2$ and stride $s=2$ that
    equals the window size. 

	\medskip

	The resulting layer $\bZ^{l+1}$ thus has
    size $2 \times 2$, since $\floor{\tfrac{n_l}{s}} = \floor{\tfrac{4}{2}} = 2$. 
    We can see that the filter $\bW$ has fixed weights equal to 1. 

	\medskip

     The convolution of the 
    first window of $\bZ^l$, namely $\bZ^l_2(1,1)$, with $\bW$
    is given as %(see \cref{fig:reg:deep:maxpooling:a})
    \begin{align*}
        \max\bigl(\bZ^l_2(1,1) \odot \bW\bigr) = 
        \max\lB(
            \matr{1 & 2\\3 & 1} \odot \matr{1 & 1\\1 & 1}
        \rB) = 
        \max\lB( \matr{1 & 2\\3& 1}\rB)  = 3
    \end{align*}
    %The other convolution steps are shown in the Figure.
\end{frame}
\begin{frame}{Max-pooling}
\framesubtitle{Example -- Stride: $s=2$}
\centering
    \tikzset{empty node/.style={draw=none,fill=none}}
    \centerline{
%        \subfloat[$\max(\bZ^l_2(1,1) \odot \bW)$]{
    \scalebox{0.5}{\LARGE
%    \label{fig:reg:deep:maxpooling:a}
        \scantokens{
    % \tikzsetnextfilename{reg_deep_maxpooling_a}
\begin{tikzpicture}[every node/.style={draw, minimum size=1cm}]
    \matrix [column sep=0mm, minimum height=6.5cm]
{
    \node[fill=lightgray] {1}; & \node[fill=lightgray] {2}; &
    \node (x) {2}; & \node{1}; \\
    \node[fill=lightgray] {3}; & \node[fill=lightgray]{1}; &
    \node{4}; & \node{2}; 
    & \node[draw=none] { };& \node {1}; & \node (w) {1};  
    & & \node[fill=gray] (z) {3}; & \node{ };\\
    \node {2}; & \node{1}; &
    \node{3}; &
    \node (n4) {4}; &
    & \node{1}; & \node (w1){1};
    & \node[draw=none]{ }; 
    & \node{ }; & \node{ };\\
    \node {1}; & \node{2}; & \node{3}; & \node{1};\\
};
\node[draw=none] at ([yshift={4mm}]w.north west) {$\bW$};
\node[draw=none] at ([yshift={4mm}]x.north west) {$\bZ^l$};
\node[draw=none] at ([yshift={4mm}]z.north east) {$\bZ^{l+1}$};
\node[draw=none] at ([xshift={4mm}]n4.0) {\scalebox{1.3}{$\ast$}}; 
\node[draw=none] at ([xshift={4mm}]w1.0) {\scalebox{1}{$=$}}; 
\end{tikzpicture}
\endinput}}%}
%\subfloat[$\max(\bZ^l_2(1,3) \odot \bW)$]{
%    \label{fig:reg:deep:maxpooling:b}
    \scalebox{0.5}{\LARGE
        \scantokens{
    % \tikzsetnextfilename{reg_deep_maxpooling_b}
\begin{tikzpicture}[every node/.style={draw, minimum size=1cm}]
    \matrix [column sep=0mm, minimum height=6.5cm]
{
    \node {1}; & \node {2}; &
    \node[fill=lightgray] (x) {2}; & \node[fill=lightgray]{1};\\
    \node{3}; & \node{1}; &
    \node[fill=lightgray]{4}; & \node[fill=lightgray]{2};
    & &  \node {1}; & \node (w) {1};  
    & & \node (z) {3}; & \node[fill=gray]{4};\\
    \node{2}; & \node{1}; &
    \node{3}; &
    \node(n4) {4};
    & \node[draw=none]{ }; 
    & \node{1}; & \node (w1){1};
    & \node[draw=none]{ }; 
    & \node{ }; & \node{ };\\
    \node {1}; & \node{2}; & \node{3}; & \node{1};\\
};
\node[draw=none] at ([yshift={4mm}]w.north west) {$\bW$};
\node[draw=none] at ([yshift={4mm}]x.north west) {$\bZ^l$};
\node[draw=none] at ([yshift={4mm}]z.north east) {$\bZ^{l+1}$};
\node[draw=none] at ([xshift={4mm}]n4.0) {\scalebox{1.3}{$\ast$}}; 
\node[draw=none] at ([xshift={4mm}]w1.0) {\scalebox{1}{$=$}}; 
\end{tikzpicture}
\endinput}}}
%}
    \centerline{
%        \subfloat[$\max(\bZ^l_2(3,1) \odot \bW)$]{
%    \label{fig:reg:deep:maxpooling:c}
    \scalebox{0.5}{\LARGE
        \scantokens{
    % \tikzsetnextfilename{reg_deep_maxpooling_c}
\begin{tikzpicture}[every node/.style={draw, minimum size=1cm}]
    \matrix [column sep=0mm, minimum height=6.5cm]
{
    \node {1}; & \node {2}; &
    \node (x) {2}; & \node{1}; \\
    \node {3}; & \node{1}; &
    \node{4}; & \node{2};  
    & &  \node {1}; & \node (w) {1};   
    & & \node (z) {3}; & \node{4};\\
    \node[fill=lightgray] {2}; & \node[fill=lightgray]{1}; &
    \node{3}; & \node (n4) {4};
    & \node[draw=none]{ }; 
    & \node{1}; & \node (w1){1};
    & \node[draw=none]{ }; 
    & \node[fill=gray]{2}; & \node{ };\\
    \node[fill=lightgray] {1}; & \node[fill=lightgray]{2}; &
    \node{3}; & \node{1};\\
};
\node[draw=none] at ([yshift={4mm}]w.north west) {$\bW$};
\node[draw=none] at ([yshift={4mm}]x.north west) {$\bZ^l$};
\node[draw=none] at ([yshift={4mm}]z.north east) {$\bZ^{l+1}$};
\node[draw=none] at ([xshift={4mm}]n4.0) {\scalebox{1.3}{$\ast$}}; 
\node[draw=none] at ([xshift={4mm}]w1.0) {\scalebox{1}{$=$}}; 
\end{tikzpicture}
\endinput}}%}
%\subfloat[$\max(\bZ^l_2(3,3) \odot \bW)$]{
%    \label{fig:reg:deep:maxpooling:d}
    \scalebox{0.5}{\LARGE
        \scantokens{
    % \tikzsetnextfilename{reg_deep_maxpooling_d}
\begin{tikzpicture}[every node/.style={draw, minimum size=1cm}]
    \matrix [column sep=0mm, minimum height=6.5cm]
{
    \node {1}; & \node {2}; &
    \node (x) {2}; & \node{1};\\
    \node {3}; & \node{1}; &
    \node{4}; & \node{2};
    & &  \node {1}; & \node (w) {1};   
    & & \node (z) {3}; & \node{4};\\
    \node {2}; & \node{1}; &
    \node[fill=lightgray]{3}; &
     \node[fill=lightgray] (n4) {4};
    & \node[draw=none]{ }; 
    & \node{1}; & \node (w1){1};
    & \node[draw=none]{ }; 
    & \node{2}; & \node[fill=gray]{4};\\
    \node {1}; & \node{2}; & \node[fill=lightgray]{3}; &
    \node[fill=lightgray]{1};\\
};
\node[draw=none] at ([yshift={4mm}]w.north west) {$\bW$};
\node[draw=none] at ([yshift={4mm}]x.north west) {$\bZ^l$};
\node[draw=none] at ([yshift={4mm}]z.north east) {$\bZ^{l+1}$};
\node[draw=none] at ([xshift={4mm}]n4.0) {\scalebox{1.3}{$\ast$}}; 
\node[draw=none] at ([xshift={4mm}]w1.0) {\scalebox{1}{$=$}}; 
\end{tikzpicture}
\endinput}}}
\end{frame}

\ifdefined\wox \begin{frame} \titlepage \end{frame} \fi

%}
%\vspace{0.2in}
%\caption{Max-pooling: Stride $s=2$.}
%\label{fig:reg:deep:maxpooling}
%\end{figure}
%
%\begin{example}[Max-pooling]
%    \cref{fig:reg:deep:maxpooling:b,fig:reg:deep:maxpooling:c,fig:reg:deep:maxpooling:d}.
%    % \cref{fig:reg:deep:maxpooling}\subref{fig:reg:deep:maxpooling:b}--\subref{fig:reg:deep:maxpooling:d}.
%\end{example}
%
%
%% Since the pooling filter is applied independently to each channel in the
%% convolution layer, the aim is to extract the maximum (or average)
%% signal after applying some non-linearity so as to capture the most
%% relevant features, which will in turn act as inputs to the next set of
%% layers. 
%
%% For pooling it is typical to set stride $s=k$, so that the summary is
%% obtained over disjoint $k \times k$ windows in each channel, although
%% any value of stride can be used. In essence, pooling uses a fixed 
%% $k \times k \times 1$ 3D filter $\bW$ all of whose entries are 1, i.e.,
%% $\bW = \bone_{k \times k \times 1}$, and applies a different aggregation
%% function like maximum or average on $\bZ^l(i,j,c) \odot \bW$ as opposed
%% to a sum used in convolution.
%
%% Pooling is a special operation that tries to summarize the information
%% from a sliding window over the tensor of neuron values $\bZ^l$ at layer
%% $l$. Unlike the convolution operations, pooling is applied independently
%% on each channel of $\bZ^l$. 
%
%% One can also perform {\em average pooling} by taking the average value
%% over all the entries in $\bZ^l_k(i,j,c)$
%% \begin{align*}
%%     \avg\lB(\bZ^l_k(i,j,c)\rB) & = 
%%     \displaystyle \avg_{\stackrel{a=1,2,\cdots,k}{b=1,2,\cdots,k}}
%%     \Bigl\{ \bZ^l[i+a-1, j+b-1, c] \Bigr\}\\
%%     & = \frac{1}{k^2 \cdot r} \sum_{a=1}^k \sum_{b=1}^k \sum_{c=1}^r
%% x_{i+a-1, j+b-1, q+c-1} \cdot w_{a, b, c} 
%% \end{align*}
%
%
%% The max-pooling convolution of $\bX$
%% and $\bW$ is given as:
%% \begin{align*}
%% \bX \ast_\max \bW = 
%%     \resizebox{0.9\hsize}{!}{$
%%     \matr{
%%         &&&\\[-1em]
%%         \max\bigl(\bX_k(1,1) \odot \bW\bigr) &
%%         \max\bigl(\bX_k(1,2)\odot \bW\bigr) &
%%         \cdots & \max\bigl(\bX_k(1, n-k+1) \odot \bW\bigr)\\
%%         \max\bigl(\bX_k(2,1) \odot \bW\bigr) &
%%         \max\bigl(\bX_k(2,2)\odot \bW\bigr) &
%%         \cdots & \max \bigl(\bX_k(2, n-k+1) \odot \bW\bigr)\\
%%         \vdots & \vdots & \cdots & \vdots\\
%%         \max\bigl(\bX_k(n-k+1,1) \odot \bW\bigr) &
%%         \max\bigl(\bX_k(n-k+1,2)\odot \bW\bigr) &
%%         \cdots & \max \bigl(\bX_k(n-k+1, n-k+1) \odot
%%         \bW\bigr)\\
%%         &&&\\[-1em]
%% }%
%% $}
%% \end{align*}
%
%% One can also write the average pooling operation in terms of the
%% convolution operation using a fixed $k
%% \times k \times 1$ 3D filter $\bW$ all of whose entries are 1, i.e.,
%% $\bW = \bone_{k \times k \times 1}$.
%% \begin{align*}
%%     \avg(\bZ^l \ast \bW)[i, j]
%%     & = \frac{1}{k^2} \cdot \sum_{a=1}^k \sum_{b=1}^k \bZ^l[i+a-1, j+b-1, c] \cdot \bW[a, b, 1]\\
%%     & = \frac{1}{k^2} \cdot \sum_{a=1}^k \sum_{b=1}^k \bZ^l[i+a-1, j+b-1, c]
%% \end{align*}
%
%% Let $\bZ^l_k(i,j)$ denote the $k \times k$
%% submatrix at row $i$ at column $j$ in channel $c$. Define the {\em
%% max-pooling} operation as follows:
%% \begin{align*}
%%     \max\lB(\bZ^l_k(i,j,c)\rB) = \displaystyle \max_{\stackrel{a =
%%     1,2,\cdots,k}{b=1,2,\cdots,k}}
%%     \Bigl\{ \bZ^l[i+a-1, j+b-1, c] \Bigr\}
%% \end{align*}
%% for $i,j = 1,2,\cdots,n_l-k+1$ and $c=1,2, \cdots, m_l$.
%% In other words, the max-pooling operation returns the maximum value
%% among the entries of the $k \times k$ 2D submatrix at row $i$ and column
%% $j$ in channel $c$.
%
%
%
%
%
%
%
%
%
%
%
%\subsection{Deep CNNs}
\begin{frame}{Deep CNNs}
In a typical CNN architecture, one alternates between a convolution
layer (with summation as the aggregation function, and learnable filter
weights and bias term) and a pooling layer (say, with max-pooling and
    fixed filter of ones). 

\medskip

The intuition is that, whereas the convolution
    layer learns the filters to extract informative features, the pooling
    layer applies an aggregation function like $\max$ (or $\avg$) to
    extract the most important neuron value (or the mean of the neuron
    values) within
    each sliding window, in each of the channels.



\medskip

    Starting from the input layer, 
a deep CNN is comprised of multiple, typically alternating, convolution
and pooling layers, followed by one or more fully connected layers, and
then the final output layer. 

\medskip

For each convolution and pooling layer we need to
choose the window size $k$ as well as the stride value $s$, and whether
to use padding $p$ or not. We also have to choose the non-linear
activation functions for the convolution layers, and also the number of layers to
consider. 
\end{frame}
%
%
%\subsection{Training CNNs}
%To see how to train CNNs we will consider a network with
%a single convolution layer and a max-pooling layer, followed by a fully
%connected layer as shown in \cref{fig:reg:deep:cnn_training}. 
%For simplicity, we assume that there is only one channel for the
%input $\bX$, and further, we use only one filter. 
%Thus, $\bX$ denotes the input matrix of size $n_0 \times n_0$. The
%filter $\bW_0$, with bias $b_0$, for the convolution layer $l=1$,
%has size $k_1 \times k_1$, which yields the matrix of neurons
%$\bZ^1$ of size $n_1 \times n_1$, 
%where $n_1 = n_0-k+1$ using stride
%$s_1=1$. Since we use only one filter, this results in a
%single channel at layer $l=1$, i.e., $m_1=1$.
%The next layer $l=2$ is a max-pooling layer $\bZ^2$ of size 
%$n_2 \times n_2$ 
%obtained by applying a $k_2 \times k_2$ filter
%with stride $s_2=k_2$. Implicitly, the max-pooling layer uses a filter of
%weights fixed at $1$, with $0$ as the bias, that is $\bW_1 =
%\bone_{k_2 \times k_2}$ and $b_1 = 0$.
%We assume that $n_1$ is a multiple of $k_2$ so
%that $n_2 = \tfrac{n_1}{k_2}$. 
%
%% \TR[name=x]{\myfbox{$\bX$}} &
%% \TR[name=c]{\myfbox{$\bZ^1$}} &
%% \TR[name=m]{\myfbox{$\bZ^2$}} &
%% \TR[name=f]{\myfboxB{$\bZ^3$}} &
%% \TR[name=o]{\myfboxB{$\bo$}}
%\begin{figure}[!t]
\begin{frame}{Training: Convolutional neural network}
To see how to train CNNs we will consider a network with
a single convolution layer and a max-pooling layer, followed by a fully
connected layer.% as shown in \cref{fig:reg:deep:cnn_training}. 

\medskip

For simplicity, we assume that there is only one channel for the
input $\bX$, and further, we use only one filter. 

    \psset{tnpos=l,tnsep=2pt,colsep=1,rowsep=1.5,mcol=c}
\psset{arrowscale=1.5, arrows=->}
\psset{unit=0.25in}
\vspace{0.25in}
\hspace{0.3in}
\scalebox{0.9}{%
\centerline{%
\psmatrix
% \fnode[framesize=70pt](0,0){x}
% \rput[bl](-0.2,0){$\bX$}
% \fnode[framesize=60pt](6,0){c}
% \rput[bl](5.7,0){$\bZ^1$}
% \rput[bl](5.7,-1.5){$\bdelta^1$}
% \fnode[framesize=40pt](10,0){m}
% \rput[bl](9.7,0){$\bZ^2$}
% \rput[bl](9.7,-1){$\bdelta^2$}
% \fnode[framesize=20pt 70pt](14,0){f}
% \rput[bl](13.8,0){$\bZ^3$}
% \rput[bl](13.8,-1.8){$\bdelta^3$}
% \pnode(16,0){o}
\scalebox{0.8}{input}
& & &
\scalebox{0.8}{convolution}
& 
\scalebox{0.8}{max-pooling}
& 
\scalebox{0.8}{fully connected}
&\scalebox{0.8}{output}\\[-4.5em]
\scalebox{0.8}{$l=0$}
& & &
\scalebox{0.8}{$l=1$}
& 
\scalebox{0.8}{$l=2$}
& 
\scalebox{0.8}{$l=3$}
&\scalebox{0.8}{$l=4$}\\[-1.5em]
\fnode[framesize=70pt](0,0){x}
\rput[bl](-0.2,0){$\bX$}
\rput[bl](-0.6,-2.5){$n_0 \times n_0$}
& & &
\fnode[framesize=60pt](0,0){c} 
\rput[bl](-0.3,0){$\bZ^1$}
\rput[bl](-0.3,-1.5){$\bdelta^1$}
\rput[bl](-0.6,-2.3){$n_1 \times n_1$}
% \rput[bl](-0.8,-2.8){$k_1, s_1=1$}
& 
\fnode[framesize=40pt](0,0){m} 
\rput[bl](-0.3,0){$\bZ^2$}
\rput[bl](-0.3,-1){$\bdelta^2$}
\rput[bl](-0.6,-1.8){$n_2 \times n_2$}
\rput[bl](-0.6,-2.4){$k_2 = s_2$}
& 
\fnode[framesize=20pt 70pt](0,0){f} 
\rput[bl](-0.2,0){$\bz^3$}
\rput[bl](-0.2,-1.8){$\bdelta^3$}
\rput[bl](-0.2,-2.5){$n_3$}
&
%\pnode(0,0){o}
\fnode[framesize=20pt 40pt](0,0){o}
\rput[bl](-0.2,0){$\bo$}
\rput[bl](-0.2,-1){$\bdelta^o$}
\rput[bl](-0.2,-1.7){$p$}
%bias to next layer
\psset{linewidth=2pt}
\ncline{x}{c}\nbput[npos=0.5]{$\bW_0, b_0$}
\ncline{c}{m}
\ncline{m}{f}\nbput[npos=0.5]{$\bW_{2}, \bb_{2}$}
\ncline{f}{o}\nbput[npos=0.5]{$\bW_{3}, \bb_{3}$}
\endpsmatrix
}}
\end{frame}
%\vspace{0.75in}
%\caption{Training: Convolutional neural network.}
%\label{fig:reg:deep:cnn_training}
%\end{figure}
%
%
%The output of the max-pooling layer $\bZ^2$ is recast as a vector $\bz^2$
%of length $(n_2)^2$, since it is fully connected to the layer $l=3$.
%That is, all of the $(n_2)^2$ neurons in $\bz^2$ are connected to each
%of the $n_3$ neurons in $\bz^3$ with the
%weights specified by the matrix $\bW_{2}$, which has size $(n_2)^2
%\times n_3$, and the bias vector $\bb_{2}\in\setR^{n_3}$. 
%Finally, all neurons in $\bz^3$ are connected to the $p$ 
%output layer neurons $\bo$, with the weight matrix $\bW_3
%\in\setR^{n_3\times p}$ and bias vector
%$\bb_3 \in\setR^p$.
%
%
%\subsubsection{Feed-forward Phase}
\begin{frame}{Deep CNNs}
\framesubtitle{Feed-forward Phase}
Let $\bD = \{\bX_i, \by_i\}_{i=1}^n$ denote the training data,
comprising $n$ tensors $\bX_i \in \setR^{n_0 \times n_0 \times m_0}$ (with
$m_0 = 1$ for ease of explanation) and
the corresponding response vector $\by_i \in \setR^p$.

\medskip

Given a training pair $(\bX, \by) \in \bD$, 
in the feed-forward phase, the predicted output $\bo$ is given via the
following equations:
\begin{align*}
    \bZ^1 & = f^1\bigl( \lB(\bX \ast \bW_0\rB) + b_0 \bigr)\\
    \bZ^2 & = \bZ^1 \ast_{s_2, \max} \bone_{k_2 \times k_2}\\
    \bz^3 & = f^3\lB( \bW^T_{2} \bz^2 + \bb_{2} \rB)\\
    \bo & = f^o \lB( \bW_o^T \bz^3 + \bb_o \rB)
\end{align*}
where $\ast_{s_2,\max}$ denotes max-pooling with stride $s_2$. 
\end{frame}
%
%\subsubsection{Backpropagation Phase}
%\index{backpropagation!CNN}
%\index{backpropagation!convolutional neural networks}
%
\begin{frame}{Deep CNNs}
\framesubtitle{Backpropagation Phase}
%Given the true response $\by$ and predicted output $\bo$, we can use any
%loss function $\cE_\bX$ to evaluate the discrepancy between them. 
%The weights and biases are updated by computing the net gradient vector
%at the output layer, and then backpropagating the net gradients from
%layer $l=4$ to $l=1$.
Let $\bdelta^1$, $\bdelta^2$, 
and $\bdelta^3$ 
denote the net gradient vectors at 
layers $l=1, 2, 3$, respectively, and let
$\bdelta^o$ denote the net gradient vector 
at the output layer. 


\medskip


The
output net gradient vector is obtained in the regular manner by computing the
partial derivatives of the loss function ($\partial \bcE_\bX$) and the
activation function ($\partial \bff^{\;o}$):
\begin{align*}
    \bdelta^o = \partial\bff^{\;o} \odot \partial\bcE_{\bX}
\end{align*}
assuming that the output neurons are independent.


\medskip


Since layer $l=3$ is fully connected to the output layer, and likewise
the max-pooling layer $l=2$ is fully connected to $\bZ^3$, the net
gradients at these layers are computed as in a regular MLP
\begin{align*}
    \bdelta^3 & = \partial\bff^{\;3} \odot \bigl( \bW_{\!o} \cdot
    \bdelta^o \bigr)\\
    \bdelta^2 & = \partial\bff^{\;2} \odot \bigl( \bW_{\!2} \cdot
    \bdelta^3 \bigr) = \bW_{\!2} \cdot
    \bdelta^3 
\end{align*}
\end{frame}

\begin{frame}{Deep CNNs}
\framesubtitle{Backpropagation Phase}
Let last step follows from the fact that $\partial \bff^{\;2} = \bone$, since
max-pooling implicitly uses an identity activation function.
%Note that we also implicitly reshape the net gradient vector
%$\bdelta^2$, so that its size is
%$\bigl((n_2)^2 \times n_3\bigr) \times \bigl(n_3 \times 1\bigr) = (n_2)^2 \times 1 = n_2
%\times n_2$, as desired.
%
%% This is because for the max-pooling layer, the activation function is
%% the maximum value over all of its input arguments, and therefore, the
%% partial derivative of the activation function with respect to the inputs is 
%% $1$ for the maximum value input. In case there is more than one input
%% that matches the maximum value, we can choose one arbitrarily.
%
%
%
%% An easier way to understand this
%% is to consider the max-pooling layer as having an identity or linear
%% activation function, with dynamic $k_2 \times k_2$ weight matrices 
%% $\bW'_{ij}$, one per sliding window (that is, without parameter
%% sharing) that have all
%% weights set to $0$ except for the maximum value inputs for which the
%% weights are set to $1$. In this conceptual view the max-pooling
%% operation is simply a convolution operation using the matrices
%% $\bW'_{ij}$ with zero bias and an identity activation function
%% \begin{align*}
%%     z^2_{i,j} = \oplus\lB( \bZ^1_{k_2}\bigl( (i-1) \cdot k_2, (j-1)
%%     \cdot k_2 \bigr) \odot \bW'_{ij}\rB)
%% \end{align*}
%% where $\bZ^1_{k_2}\bigl( (i-1)\cdot k_2, (j-1) \cdot k_2\bigr)$ is the $k_2\times k_2$ submatrix of $\bZ^1$
%% starting at position $\bigl((i-1) \cdot k_2, (j-1) \cdot k_2\bigr)$, and
%% where $\bW'_{ij}$ is a $k_2 \times k_2$ binary matrix defined as
%% follows:
%% \begin{align*}
%%       \bW'_{ij}(a,b) = 
%%       \begin{cases}
%%           1 &  \text{if } z^1_{a,b} = \max \lB\{\bZ^1_{k_2}\bigl( (i-1)\cdot
%%           k_2, (j-1) \cdot k_2\bigr)\rB\}\\
%%           0 & \text{otherwise}
%%       \end{cases}
%% \end{align*}
%% for $a,b = 1, 2, \cdots, k_2$, and $i,j = 1, 2, \cdots, n_2$.
%
%% To compute the net gradients at the convolution layer $\bZ^1$, we
%% again make use of the dynamic weight matrices $\bW'_{ij}$. Consider the
%% net gradient at neuron $z^1_{a,b}$ in layer $l=1$ where $a, b = 1, 2,
%% \cdots, n_1$. Since we assume that the stride equals the filter size for
%% the max-pooling layer, each neuron in the convolution layer contributes
%% only to one window for the max-pooling layer. Let $\bW'_{ij}$ be the
%% dynamic weight matrix for that window that contains $z^1_{ab}$. Then, the
%% net gradient is given as
%% \begin{align*}
%%     \bdelta^1_{ab} = \partial f^1_{ab} \cdot 
%%     \lB( \bW'_{ij} \cdot \delta^2_{ij} \rB) = 
%%     \begin{cases}
%%         \partial f^1_{ab} \cdot \delta^2_{ij} & \text{if } z^1_{a,b} = \max \lB\{
%%         \bZ^1_{k_2}(i,j) \rB\}\\
%%         0 & \text{otherwise}
%%     \end{cases}
%% \end{align*}
%
Consider the
net gradient $\delta^1_{ij}$ at neuron $z^1_{ij}$ in layer $l=1$ where
$i, j = 1, 2,
\cdots, n_1$. 
%Since we assume that the stride $s_2$ equals the filter size
%$k_2$ for
%the max-pooling layer, each sliding window in the convolution layer contributes
%only to one neuron at the max-pooling layer. 
%Given stride $s_2=k_2$, the $k_2 \times k_2$ sliding window that contains
%$z^1_{ij}$ is given as $\bZ^1_{k_2}(a,b)$, where
%\begin{align*}
%    a & = \ceil[\Big]{\frac{i}{s_2}} & 
%    b & = \ceil[\Big]{\frac{j}{s_2}}
%\end{align*}
%Due to the $\max$ aggregation function, 
%the maximum valued element in
%$\bZ^1_{k_2}(a,b)$ specifies the value of neuron $z^2_{ab}$ in the
%max-pooling layer $l=2$. That is,
%\begin{align*}
%    z^2_{ab} & = \max_{i,j=1, 2, \ldots, k_2} \lB\{ z^1_{(a-1) \cdot k_2+i,
%    (b-1) \cdot k_2+j} \rB\}\\
%    i^*, j^* & = 
%    \argmax_{i,j=1,2,\ldots,k_2} \lB\{ z^1_{(a-1) \cdot k_2+i,(b-1)\cdot k_2+j} \rB\}
%\end{align*}
%where $i^*, j^*$ is the index of the maximum valued neuron in the window
%$\bZ^1_{k_2}(a,b)$. 
%
%The net gradient $\delta^1_{ij}$ at neuron $z^1_{ij}$ is therefore
%given as
%\begin{align*}
%    \delta^1_{ij} = 
%    \frac{\partial \cE_\bX}{\partial \net^{\;1}_{ij}} & = 
%    \frac{\partial \cE_\bX}{\partial \net^{\;2}_{ab}}
%    \cdot 
%    \frac{\partial \net^{\;2}_{ab}}{\partial z^1_{ij}} \cdot 
%    \frac{\partial z^1_{ij}}{\partial \net^{\;1}_{ij}}\\
%    & = \delta^2_{ab} \cdot 
%    \frac{\partial \net^{\;2}_{ab}}{\partial z^1_{ij}} 
%    \cdot \partial f^1_{ij}
%    % & \delta^2_{a,b} \cdot M(i,j) \cdot \partial f^_{i,j}
%\end{align*}
%where $\net^{\;l}_{ij}$ denotes the net input at neuron $z^l_{ij}$ in layer
%$l$.
%However, 
%since $\net^{\;2}_{ab} = z^1_{i^*,j^*}$, the partial derivative 
%$\frac{\partial \net^{\;2}_{ab}}{\partial z^1_{ij}}$ is
%either $1$ or $0$, depending on whether $z^1_{ij}$ is the maximum
%element in the window $\bZ^1_{k_2}(a,b)$ or not. 
%Putting it all together, we have
\begin{align*}
    \delta^1_{ij} & = 
    \begin{cases}
    \delta^2_{ab} \cdot \partial f^1_{ij} & \text{if $i=i^*$ and
    $j=j^*$}\\
    0 & \text{otherwise}
    \end{cases}
\end{align*}
%
%% Given the 2D window $\bZ^1_{k_1}(1+(i-1)\cdot s_2, 1+(j-1)\cdot
%% s_2)$, the neuron $z^2_{ij}$ is given as
%% \begin{align*}
%%     z^2_{ij} = \max_{a,b=1}^{k+1} \lB\{ z^1_{a,b} | z^1_{a,b} \in 
%% \bZ^1_{k_1}(1+(i-1)\cdot s_2, 1+(j-1)\cdot
%% s_2) \rB\}
%% \end{align*}
%
%% The
%% net gradient is given as
%% \begin{align*}
%%     \delta^1_{ab} = 
%%     \begin{cases}
%%         \partial f^1_{ab} \cdot \delta^2_{ij} & \text{if } z^1_{a,b} = \max \lB\{
%%         \bZ^1_{k_2}\bigl( (i-1)\cdot k_2, \; (j-1) \cdot k_2\bigr) \rB\}\\
%%         0 & \text{otherwise}
%%     \end{cases}
%% \end{align*}
%%  where $\bZ^1_{k_2}\bigl( (i-1)\cdot k_2, (j-1) \cdot k_2\bigr)$ is the
%%  $k_2\times k_2$ submatrix of $\bZ^1$ starting at position $\bigl((i-1)
%%  \cdot k_2, (j-1) \cdot k_2\bigr)$, and $z^1_{a,b}$ lies within this
%%  submatrix.
%
In other words, the net gradient at neuron $z^1_{ij}$ in the convolution
layer is zero if this neuron does not have the maximum value 
in its window. 

\medskip

Otherwise, if it is the maximum, 
the net gradient backpropagates from the max-pooling layer to this
neuron and is then multiplied by the partial derivative of the
activation function. 


\medskip

The $n_1 \times n_1$ matrix of net
gradients $\bdelta^1$ comprises the net gradients $\delta^1_{ij}$ for
all $i,j = 1,2,\cdots,n_1$.
\end{frame}
%
%% Define $\bdelta'^{2}$ as
%% the $n_1 \times n_1$ matrix of gradients backpropagated from the
%% max-pool layer, defined as follows:
%% $\delta'^{2}_{ab} = \delta^2_{ij}$ if $z^1_{a,b}$ is the maximum value
%% and $\delta'^{2}_{ab} = 0$, otherwise. We can then write the net
%% gradient vector as
%% \begin{align*}
%%     \bdelta^1 = \partial \bff^{\;1} \odot \bdelta'^{2}
%% \end{align*}
%
%From the net gradients, we can compute the gradients of the weight
%matrices and bias parameters. For the fully connected layers, that is,
%between $l=2$ and $l=3$, and $l=3$ and $l=4$, we have
%\begin{align*}
%    \grad_{\!\bW_{\!3}} & = \bZ^3 \cdot (\bdelta^o)^T &
%    \grad_{\!\bb_{3}} & = \bdelta^o &
%    \grad_{\!\bW_{\!2}} & = \bZ^2 \cdot (\bdelta^3)^T &
%    \grad_{\!\bb_{2}} & = \bdelta^3
%\end{align*}
%where we treat $\bZ^2$ as a $(n_2)^2 \times 1$ vector.
%
%%Note that the weights between the convolution and max-pooling layers
%%are all fixed at either zero or one, 
%Note that the weight matrix $\bW_1$ is fixed at $\bone_{k_2 \times k_2}$
%and the bias term $b_1$ is also fixed at $0$, 
%so there are no parameters to learn
%between the convolution and max-pooling layers. 
%Finally, we compute the weight and bias gradients between the input and
%convolution layer as
%follows:
%\begin{align*}
%    \grad_{\!\bW_{\!0}} & = \sum_{i=1}^{n_1} \sum_{j=1}^{n_1}
%    \bX_{k_1}(i,j) \cdot \delta^1_{ij} &
%    \grad_{\!b_{0}} & = \sum_{i=1}^{n_1} \sum_{j=1}^{n_1}
%    \delta^1_{ij}
%\end{align*}
%where we used the fact that the stride is $s_1 = 1$, and that $\bW_0$ is a
%shared filter for all $k_1 \times k_1$ windows of $\bX$, with the shared
%bias value $b_0$ for all windows. 
%There
%are $n_1 \times n_1$ such windows, where $n_1 = n_0 - k_1 +1$, 
%therefore, to compute the weight and bias gradients, we sum
%over all the windows. Note that if there were multiple filters (that is, if
%$m_1 > 1$), then the bias and weight gradients for the $j$th filter 
%would be learned from the corresponding channel $j$ in layer $l=1$.
%
%\begin{figure}[!b]
\begin{frame}{Convolutional Neural Network}
\framesubtitle{Example}

\vspace*{-0.5cm}

Example shows a CNN for handwritten digit
    recognition. This CNN is trained and tested on the MNIST dataset,
    that contains 60,000 training images and 10,000 test images. 
    %Some examples of handwritten digits from MNIST are shown in
%figure.

\medskip

    Each input image is a $ 28
    \times 28$ matrix of pixel values between 0 to 255, which are
    divided by 255, so that each pixel lies in the interval $[0,1]$.
    The corresponding (true)
    output $\by_i$ is a one-hot encoded binary vector that denotes a
    digit from 0 to 9;  the digit 0 is encoded as
    $\be_1 = (1,0,0,0,0,0,0,0,0,0)^T$, the digit 1 as $\be_2 =
    (0,1,0,0,0,0,0,0,0,0)^T$,
    and so on. 

    \large
\def\myCube####1####2####3####4{
    \psSolid[object=parallelepiped,a=####1,b=####2,c=####3]####4
}
\psset{arrowscale=1, arrows=->}
\psset{viewpoint=90 5 15 rtp2xyz, Decran=50}
\psset{lightsrc=100 10 20 rtp2xyz, lightintensity=2}
\vspace{0.6in}
\centerline{
%\axesIIID[showOrigin=true](1,1,1)(5,5,5)
    \hspace{0.25in}
    \scalebox{0.50}{%
    \begin{tabular}{p{0.65in}%
                    p{1in}%
                    p{0.65in}%
                    p{0.8in}%
                    p{0.4in}%
                    p{0.65in}%
                    p{0.35in}%
                    p{0.55in}%
                    p{0.2in}%
                    p{0.35in}%
                    p{0.08in}%
                    p{0.35in}%
                    p{0.08in}%
                    p{0.35in}%
                    p{0.08in}%
                }
    \myCube{3}{6}{6}{(0,0,0)}
    \psPoint(0,1,4.5){O}\uput[l](O){Input}
    \psPoint(0,0.5,0){O}\uput[l](O){$\bX$}
    \psPoint(0,1.7,-4.1){O} \uput[l](O){%
    \scalebox{1}{$28 \times 28 \times 1$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{5}{6}{6}{(0,0,0)} 
    \psPoint(0,2,4.5){O}\uput[l](O){Convolution}
    \psPoint(0,0.5,0){O} \uput[l](O){$\bZ^1$}
    \psPoint(0,2,-5){O} \uput[l](O){%
    \scalebox{1.3}{$\substack{24 \times 24 \times 6\\k_1=5}$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{5}{4}{4}{(0,0,0.5)} 
    \psPoint(0,2,4.5){O}\uput[l](O){Max-pooling}
    \psPoint(0,0.5,0){O} \uput[l](O){$\bZ^2$}
    \psPoint(0,1.8,-3.8){O} \uput[l](O){%
    \scalebox{1.3}{$\substack{12 \times 12 \times 6\\k_2=s_2=2}$}} &
        \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{7}{3}{3}{(0,0,1)} 
    \psPoint(0,2,4.5){O}\uput[l](O){Convolution}
    \psPoint(0,0.5,0){O}  \uput[l](O){$\bZ^3$}
    \psPoint(0,1.7,-2.9){O} \uput[l](O){%
    \scalebox{1.3}{$\substack{8 \times 8 \times 16\\k_3=5}$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{7}{2}{2}{(0,0,1)}
    \psPoint(0,2,4.5){O}\uput[l](O){Max-pooling}
    \psPoint(0,0.5,0){O} \uput[l](O){$\bZ^4$}
    \psPoint(0,1.5,-2.7){O} \uput[l](O){%
    \scalebox{1.3}{$\substack{4 \times 4 \times 16\\k_4=s_4=2}$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{1}{1}{7}{(0,0,0)}
    \psPoint(0,0.7,0){O} \uput[l](O){$\bZ^5$}
    \psPoint(0,0.8,-4.25){O} \uput[l](O){\scalebox{1}{$120$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{1}{1}{6}{(0,0,0)}
    \psPoint(0,3.5,4.5){O}\uput[l](O){Fully Connected Layers}
    \psPoint(0,0.7,0){O}  \uput[l](O){$\bZ^6$}
    \psPoint(0,0.8,-3.8){O} \uput[l](O){\scalebox{1}{$84$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{1}{1}{1}{(0,0,0)}
    \psPoint(0,0.5,0){O} \uput[l](O){$\Large\bo$}
    \psPoint(0,0.6,-1.1){O} \uput[l](O){\scalebox{1}{$10$}}
    \end{tabular}
}}
\end{frame}
%\vspace{0.7in}
%\caption{Convolutional neural network.}
%\label{fig:reg:deep:cnn}
%    \vspace{-0.2in}
%\end{figure}
%
%%MJZ -- commented briefly
%\begin{figure}[!t]
%    \vspace{-0.2in}
%    \centerline{
%    \subfloat[label $0$]{%
%    %\scalebox{0.25}{%
%    \resizebox{0.8in}{0.8in}{%
%    % \tikzsetnextfilename{reg_neural_mnist_ex_0}
%    \begin{tikzpicture}
%        \begin{axis}[
%            % view={0}{90},   % not needed for `matrix plot*' variant
%            colormap/blackwhite,
%            enlargelimits=false,
%            axis on top,
%            point meta min=0,
%            point meta max=255,
%        ]
%       \addplot [matrix plot,point meta=explicit] file [meta=index 2]
%            {REG/neural/figs/mnist_digits/digit_M0_10.txt};
%        \end{axis}
%    \end{tikzpicture}}}
%    \subfloat[label $1$]{%
%    % \scalebox{0.25}{%
%    \resizebox{0.8in}{0.8in}{%
%    % \tikzsetnextfilename{reg_neural_mnist_ex_1}
%    \begin{tikzpicture}
%        \begin{axis}[
%            % view={0}{90},   % not needed for `matrix plot*' variant
%            colormap/blackwhite,
%            enlargelimits=false,
%            axis on top,
%            point meta min=0,
%            point meta max=255,
%        ]
%       \addplot [matrix plot,point meta=explicit] file [meta=index 2]
%            {REG/neural/figs/mnist_digits/digit_M1_5.txt};
%        \end{axis}
%    \end{tikzpicture}}}
%    \subfloat[label $2$]{%
%    % \scalebox{0.25}{%
%    \resizebox{0.8in}{0.8in}{%
%    % \tikzsetnextfilename{reg_neural_mnist_ex_2}
%    \begin{tikzpicture}
%        \begin{axis}[
%            % view={0}{90},   % not needed for `matrix plot*' variant
%            colormap/blackwhite,
%            enlargelimits=false,
%            axis on top,
%            point meta min=0,
%            point meta max=255,
%        ]
%       \addplot [matrix plot,point meta=explicit] file [meta=index 2]
%            {REG/neural/figs/mnist_digits/digit_M2_35.txt};
%        \end{axis}
%    \end{tikzpicture}}}
%    \subfloat[label $3$]{%
%    \resizebox{0.8in}{0.8in}{%
%    % \scalebox{0.25}{%
%    % \tikzsetnextfilename{reg_neural_mnist_ex_3}
%    \begin{tikzpicture}
%        \begin{axis}[
%            % view={0}{90},   % not needed for `matrix plot*' variant
%            colormap/blackwhite,
%            enlargelimits=false,
%            axis on top,
%            point meta min=0,
%            point meta max=255,
%        ]
%       \addplot [matrix plot,point meta=explicit] file [meta=index 2]
%            {REG/neural/figs/mnist_digits/digit_M3_30.txt};
%        \end{axis}
%    \end{tikzpicture}}}
%    \subfloat[label $4$]{%
%    \resizebox{0.8in}{0.8in}{%
%    % \scalebox{0.25}{%
%    % \tikzsetnextfilename{reg_neural_mnist_ex_4}
%    \begin{tikzpicture}
%        \begin{axis}[
%            % view={0}{90},   % not needed for `matrix plot*' variant
%            colormap/blackwhite,
%            enlargelimits=false,
%            axis on top,
%            point meta min=0,
%            point meta max=255,
%        ]
%       \addplot [matrix plot,point meta=explicit] file [meta=index 2]
%            {REG/neural/figs/mnist_digits/digit_M4_6.txt};
%        \end{axis}
%    \end{tikzpicture}}}
%    }
%    \centerline{%
%    \subfloat[label $5$]{%
%    \resizebox{0.8in}{0.8in}{%
%    % \scalebox{0.25}{%
%    % \tikzsetnextfilename{reg_neural_mnist_ex_5}
%    \begin{tikzpicture}
%        \begin{axis}[
%            % view={0}{90},   % not needed for `matrix plot*' variant
%            colormap/blackwhite,
%            enlargelimits=false,
%            axis on top,
%            point meta min=0,
%            point meta max=255,
%        ]
%       \addplot [matrix plot,point meta=explicit] file [meta=index 2]
%            {REG/neural/figs/mnist_digits/digit_M5_15.txt};
%        \end{axis}
%    \end{tikzpicture}}}
%    \subfloat[label $6$]{%
%    \resizebox{0.8in}{0.8in}{%
%    % \scalebox{0.25}{%
%    % \tikzsetnextfilename{reg_neural_mnist_ex_6}
%    \begin{tikzpicture}
%        \begin{axis}[
%            % view={0}{90},   % not needed for `matrix plot*' variant
%            colormap/blackwhite,
%            enlargelimits=false,
%            axis on top,
%            point meta min=0,
%            point meta max=255,
%        ]
%       \addplot [matrix plot,point meta=explicit] file [meta=index 2]
%            {REG/neural/figs/mnist_digits/digit_M6_21.txt};
%        \end{axis}
%    \end{tikzpicture}}}
%    \subfloat[label $7$]{%
%    \resizebox{0.8in}{0.8in}{%
%    % \scalebox{0.25}{%
%    % \tikzsetnextfilename{reg_neural_mnist_ex_7}
%    \begin{tikzpicture}
%        \begin{axis}[
%            % view={0}{90},   % not needed for `matrix plot*' variant
%            colormap/blackwhite,
%            enlargelimits=false,
%            axis on top,
%            point meta min=0,
%            point meta max=255,
%        ]
%       \addplot [matrix plot,point meta=explicit] file [meta=index 2]
%            {REG/neural/figs/mnist_digits/digit_M7_17.txt};
%        \end{axis}
%    \end{tikzpicture}}}
%    \subfloat[label $8$]{%
%    \resizebox{0.8in}{0.8in}{%
%    % \scalebox{0.25}{%
%    % \tikzsetnextfilename{reg_neural_mnist_ex_8}
%    \begin{tikzpicture}
%        \begin{axis}[
%            % view={0}{90},   % not needed for `matrix plot*' variant
%            colormap/blackwhite,
%            enlargelimits=false,
%            axis on top,
%            point meta min=0,
%            point meta max=255,
%        ]
%       \addplot [matrix plot,point meta=explicit] file [meta=index 2]
%            {REG/neural/figs/mnist_digits/digit_M8_84.txt};
%        \end{axis}
%    \end{tikzpicture}}}
%    \subfloat[label $9$]{%
%    \resizebox{0.8in}{0.8in}{%
%    % \scalebox{0.25}{%
%    % \tikzsetnextfilename{reg_neural_mnist_ex_9}
%    \begin{tikzpicture}
%        \begin{axis}[
%            % view={0}{90},   % not needed for `matrix plot*' variant
%            colormap/blackwhite,
%            enlargelimits=false,
%            axis on top,
%            point meta min=0,
%            point meta max=255,
%        ]
%       \addplot [matrix plot,point meta=explicit] file [meta=index 2]
%            {REG/neural/figs/mnist_digits/digit_M9_9.txt};
%        \end{axis}
%    \end{tikzpicture}}}
%    }
%\vspace{0.2in}
%    \caption{MNIST dataset: Sample handwritten digits.}
%    \label{fig:reg:deep:mnist_ex}
%    \vspace{-0.2in}
%\end{figure}
%
%%\begin{figure}[!ht]
%%% Cube
%%\def\myCube{
%%    \psset{subgriddiv=1,gridlabels=0pt,unit=0.2in}
%%    \ThreeDput[normal=0 0 1](2,0,5){\psgrid(3,5)}
%%\ThreeDput[normal=0 1 0](5,5,0){\psgrid(3,5)}
%%\ThreeDput[normal=1 0 0](5,0,0){\psgrid(5,5)}
%%}
%%% \def\myCube#1#2#3#4{
%%%     \psSolid[object=parallelepiped,a=#1,b=#2,c=#3]#4
%%% }
%%\psset{tnpos=l,tnsep=2pt,colsep=2,rowsep=1.5,mcol=c,
%%arrowscale=1.5, arrows=->}
%%\psset{viewpoint=1 1.5 1}
%%%\psset{viewpoint=90 55 15 rtp2xyz, Decran=50}
%%%\psset{lightsrc=100 60 20 rtp2xyz, lightintensity=2}
%%\vspace{0.6in}
%%\centerline{
%%    \begin{tabular}{p{1.5in}p{1in}}
%%    \myCube & \myCube
%%\end{tabular}
%%%\TR[name=i]{\myCube{3}{5}{5}{(0,0,0)}} &
%%%\TR[name=c1]{\myCube{5}{4}{4}{(0,6,0)}} &
%%%\TR[name=p1]{\myCube{5}{2}{2}{(0,10,0)}}
%%}
%%\vspace{0.7in}
%%\caption{Convolutional neural network}
%%\label{fig:reg:deep:cnn}
%%\end{figure}
%
%% \enlargethispage{6pt}
%% \vspace*{-0.1in}
%\begin{example}[CNN]
\begin{frame}{Convolutional Neural Network}
\framesubtitle{Example}

\vspace*{-0.5cm}

    In our CNN model, all the convolution layers use stride equal to one,
    and do not use any padding, whereas all of the max-pooling layers
    use stride equal to the window size.
    Since each input is a
    $28 \times 28$ pixels image of a digit with 1 channel (grayscale),
    we have $n_0=28$ and $m_0=1$, and therefore, the
    input $\bX = \bZ^0$ is a $n_0 \times n_0 \times m_0 = 28 \times 28
    \times 1$ tensor. 
    The first convolution
    layer uses $m_1=6$ filters, with $k_1=5$ and stride $s_1=1$, without padding. 

    \medskip

    \large
\def\myCube####1####2####3####4{
    \psSolid[object=parallelepiped,a=####1,b=####2,c=####3]####4
}
\psset{arrowscale=1, arrows=->}
\psset{viewpoint=90 5 15 rtp2xyz, Decran=50}
\psset{lightsrc=100 10 20 rtp2xyz, lightintensity=2}
\vspace{0.6in}
\centerline{
%\axesIIID[showOrigin=true](1,1,1)(5,5,5)
    \hspace{0.25in}
    \scalebox{0.50}{%
    \begin{tabular}{p{0.65in}%
                    p{1in}%
                    p{0.65in}%
                    p{0.8in}%
                    p{0.4in}%
                    p{0.65in}%
                    p{0.35in}%
                    p{0.55in}%
                    p{0.2in}%
                    p{0.35in}%
                    p{0.08in}%
                    p{0.35in}%
                    p{0.08in}%
                    p{0.35in}%
                    p{0.08in}%
                }
    \myCube{3}{6}{6}{(0,0,0)}
    \psPoint(0,1,4.5){O}\uput[l](O){Input}
    \psPoint(0,0.5,0){O}\uput[l](O){$\bX$}
    \psPoint(0,1.7,-4.1){O} \uput[l](O){%
    \scalebox{1}{$28 \times 28 \times 1$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{5}{6}{6}{(0,0,0)} 
    \psPoint(0,2,4.5){O}\uput[l](O){Convolution}
    \psPoint(0,0.5,0){O} \uput[l](O){$\bZ^1$}
    \psPoint(0,2,-5){O} \uput[l](O){%
    \scalebox{1.3}{$\substack{24 \times 24 \times 6\\k_1=5}$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{5}{4}{4}{(0,0,0.5)} 
    \psPoint(0,2,4.5){O}\uput[l](O){Max-pooling}
    \psPoint(0,0.5,0){O} \uput[l](O){$\bZ^2$}
    \psPoint(0,1.8,-3.8){O} \uput[l](O){%
    \scalebox{1.3}{$\substack{12 \times 12 \times 6\\k_2=s_2=2}$}} &
        \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{7}{3}{3}{(0,0,1)} 
    \psPoint(0,2,4.5){O}\uput[l](O){Convolution}
    \psPoint(0,0.5,0){O}  \uput[l](O){$\bZ^3$}
    \psPoint(0,1.7,-2.9){O} \uput[l](O){%
    \scalebox{1.3}{$\substack{8 \times 8 \times 16\\k_3=5}$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{7}{2}{2}{(0,0,1)}
    \psPoint(0,2,4.5){O}\uput[l](O){Max-pooling}
    \psPoint(0,0.5,0){O} \uput[l](O){$\bZ^4$}
    \psPoint(0,1.5,-2.7){O} \uput[l](O){%
    \scalebox{1.3}{$\substack{4 \times 4 \times 16\\k_4=s_4=2}$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{1}{1}{7}{(0,0,0)}
    \psPoint(0,0.7,0){O} \uput[l](O){$\bZ^5$}
    \psPoint(0,0.8,-4.25){O} \uput[l](O){\scalebox{1}{$120$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{1}{1}{6}{(0,0,0)}
    \psPoint(0,3.5,4.5){O}\uput[l](O){Fully Connected Layers}
    \psPoint(0,0.7,0){O}  \uput[l](O){$\bZ^6$}
    \psPoint(0,0.8,-3.8){O} \uput[l](O){\scalebox{1}{$84$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{1}{1}{1}{(0,0,0)}
    \psPoint(0,0.5,0){O} \uput[l](O){$\Large\bo$}
    \psPoint(0,0.6,-1.1){O} \uput[l](O){\scalebox{1}{$10$}}
    \end{tabular}
}}
\end{frame}
\begin{frame}{Convolutional Neural Network}
\framesubtitle{Example}

\vspace*{-0.3cm}

    Each filter
    is a $5 \times 5 \times 1$ tensor of weights, and across the six
    filters the resulting
    layer $l=1$ tensor $\bZ^1$ has size $24 \times 24 \times 6$, with $n_1 = n_0
    - k_1 +1 = 28-5+1 = 24$ and $m_1 = 6$.
	The second hidden layer is a max-pooling ($k_2=2$ and 
	$s_2=2$). 


\medskip

    Since max-pooling by default uses a fixed filter $\bW =
    \bone_{k_2 \times k_2 \times 1}$, the resulting
    tensor $\bZ^2$ has size $12 \times 12 \times 6$, with $n_2 =
    \lB\lfloor \tfrac{n_1}{k_2} \rB\rfloor = 
    \lB\lfloor\tfrac{24}{2}\rB\rfloor = 12$, and $m_2 = 6$.
    The third
    layer is a convolution layer with $m_3 =16$ channels, with a
    window size of $k_3=5$ (and stride $s_3=1$), 
    resulting in the
    tensor $\bZ^3$ of size $8 \times 8 \times 16$, where $n_3 = n_2 -
    k_3 + 1 = 12-5+1 = 8$.

    \large
\def\myCube####1####2####3####4{
    \psSolid[object=parallelepiped,a=####1,b=####2,c=####3]####4
}
\psset{arrowscale=1, arrows=->}
\psset{viewpoint=90 5 15 rtp2xyz, Decran=50}
\psset{lightsrc=100 10 20 rtp2xyz, lightintensity=2}
\vspace{0.6in}
\centerline{
%\axesIIID[showOrigin=true](1,1,1)(5,5,5)
    \hspace{0.25in}
    \scalebox{0.50}{%
    \begin{tabular}{p{0.65in}%
                    p{1in}%
                    p{0.65in}%
                    p{0.8in}%
                    p{0.4in}%
                    p{0.65in}%
                    p{0.35in}%
                    p{0.55in}%
                    p{0.2in}%
                    p{0.35in}%
                    p{0.08in}%
                    p{0.35in}%
                    p{0.08in}%
                    p{0.35in}%
                    p{0.08in}%
                }
    \myCube{3}{6}{6}{(0,0,0)}
    \psPoint(0,1,4.5){O}\uput[l](O){Input}
    \psPoint(0,0.5,0){O}\uput[l](O){$\bX$}
    \psPoint(0,1.7,-4.1){O} \uput[l](O){%
    \scalebox{1}{$28 \times 28 \times 1$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{5}{6}{6}{(0,0,0)} 
    \psPoint(0,2,4.5){O}\uput[l](O){Convolution}
    \psPoint(0,0.5,0){O} \uput[l](O){$\bZ^1$}
    \psPoint(0,2,-5){O} \uput[l](O){%
    \scalebox{1.3}{$\substack{24 \times 24 \times 6\\k_1=5}$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{5}{4}{4}{(0,0,0.5)} 
    \psPoint(0,2,4.5){O}\uput[l](O){Max-pooling}
    \psPoint(0,0.5,0){O} \uput[l](O){$\bZ^2$}
    \psPoint(0,1.8,-3.8){O} \uput[l](O){%
    \scalebox{1.3}{$\substack{12 \times 12 \times 6\\k_2=s_2=2}$}} &
        \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{7}{3}{3}{(0,0,1)} 
    \psPoint(0,2,4.5){O}\uput[l](O){Convolution}
    \psPoint(0,0.5,0){O}  \uput[l](O){$\bZ^3$}
    \psPoint(0,1.7,-2.9){O} \uput[l](O){%
    \scalebox{1.3}{$\substack{8 \times 8 \times 16\\k_3=5}$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{7}{2}{2}{(0,0,1)}
    \psPoint(0,2,4.5){O}\uput[l](O){Max-pooling}
    \psPoint(0,0.5,0){O} \uput[l](O){$\bZ^4$}
    \psPoint(0,1.5,-2.7){O} \uput[l](O){%
    \scalebox{1.3}{$\substack{4 \times 4 \times 16\\k_4=s_4=2}$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{1}{1}{7}{(0,0,0)}
    \psPoint(0,0.7,0){O} \uput[l](O){$\bZ^5$}
    \psPoint(0,0.8,-4.25){O} \uput[l](O){\scalebox{1}{$120$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{1}{1}{6}{(0,0,0)}
    \psPoint(0,3.5,4.5){O}\uput[l](O){Fully Connected Layers}
    \psPoint(0,0.7,0){O}  \uput[l](O){$\bZ^6$}
    \psPoint(0,0.8,-3.8){O} \uput[l](O){\scalebox{1}{$84$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{1}{1}{1}{(0,0,0)}
    \psPoint(0,0.5,0){O} \uput[l](O){$\Large\bo$}
    \psPoint(0,0.6,-1.1){O} \uput[l](O){\scalebox{1}{$10$}}
    \end{tabular}
}}
\end{frame}

\begin{frame}{Convolutional Neural Network}
\framesubtitle{Example}

\vspace*{-0.5cm}

    This is followed
    by another max-pooling layer that uses $k_4=2$ and $s_4=2$, which
    yields the tensor $\bZ^4$ that is $4 \times 4 \times 16$, where 
    $n_4 = \lB\lfloor \tfrac{n_3}{k_4} \rB\rfloor = 
    \lB\lfloor\tfrac{8}{2}\rB\rfloor = 4$, and $m_4 = 16$.

\medskip

    The next three layers are fully connected as in a regular MLP. All
    of the $4\times 4\times 16 = 256$ neurons in layer $l=4$ are
    connected to $l=5$ $\bZ^5$ is
    simply a vector and it can be considered a
    degenerate tensor of size $120 \times 1 \times 1$. 

	\medskip

Layer $l=5$ is
    also fully connected to layer $l=6$ with $84$ neurons.
$\bo$
    has 10 neurons and uses softmax.
    $\bZ^1$, $\bZ^3$, $\bZ^5$, and $\bZ^6$ use
    ReLU activation.

    \large
\def\myCube####1####2####3####4{
    \psSolid[object=parallelepiped,a=####1,b=####2,c=####3]####4
}
\psset{arrowscale=1, arrows=->}
\psset{viewpoint=90 5 15 rtp2xyz, Decran=50}
\psset{lightsrc=100 10 20 rtp2xyz, lightintensity=2}
\vspace{0.6in}
\centerline{
%\axesIIID[showOrigin=true](1,1,1)(5,5,5)
    \hspace{0.25in}
    \scalebox{0.50}{%
    \begin{tabular}{p{0.65in}%
                    p{1in}%
                    p{0.65in}%
                    p{0.8in}%
                    p{0.4in}%
                    p{0.65in}%
                    p{0.35in}%
                    p{0.55in}%
                    p{0.2in}%
                    p{0.35in}%
                    p{0.08in}%
                    p{0.35in}%
                    p{0.08in}%
                    p{0.35in}%
                    p{0.08in}%
                }
    \myCube{3}{6}{6}{(0,0,0)}
    \psPoint(0,1,4.5){O}\uput[l](O){Input}
    \psPoint(0,0.5,0){O}\uput[l](O){$\bX$}
    \psPoint(0,1.7,-4.1){O} \uput[l](O){%
    \scalebox{1}{$28 \times 28 \times 1$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{5}{6}{6}{(0,0,0)} 
    \psPoint(0,2,4.5){O}\uput[l](O){Convolution}
    \psPoint(0,0.5,0){O} \uput[l](O){$\bZ^1$}
    \psPoint(0,2,-5){O} \uput[l](O){%
    \scalebox{1.3}{$\substack{24 \times 24 \times 6\\k_1=5}$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{5}{4}{4}{(0,0,0.5)} 
    \psPoint(0,2,4.5){O}\uput[l](O){Max-pooling}
    \psPoint(0,0.5,0){O} \uput[l](O){$\bZ^2$}
    \psPoint(0,1.8,-3.8){O} \uput[l](O){%
    \scalebox{1.3}{$\substack{12 \times 12 \times 6\\k_2=s_2=2}$}} &
        \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{7}{3}{3}{(0,0,1)} 
    \psPoint(0,2,4.5){O}\uput[l](O){Convolution}
    \psPoint(0,0.5,0){O}  \uput[l](O){$\bZ^3$}
    \psPoint(0,1.7,-2.9){O} \uput[l](O){%
    \scalebox{1.3}{$\substack{8 \times 8 \times 16\\k_3=5}$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{7}{2}{2}{(0,0,1)}
    \psPoint(0,2,4.5){O}\uput[l](O){Max-pooling}
    \psPoint(0,0.5,0){O} \uput[l](O){$\bZ^4$}
    \psPoint(0,1.5,-2.7){O} \uput[l](O){%
    \scalebox{1.3}{$\substack{4 \times 4 \times 16\\k_4=s_4=2}$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{1}{1}{7}{(0,0,0)}
    \psPoint(0,0.7,0){O} \uput[l](O){$\bZ^5$}
    \psPoint(0,0.8,-4.25){O} \uput[l](O){\scalebox{1}{$120$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{1}{1}{6}{(0,0,0)}
    \psPoint(0,3.5,4.5){O}\uput[l](O){Fully Connected Layers}
    \psPoint(0,0.7,0){O}  \uput[l](O){$\bZ^6$}
    \psPoint(0,0.8,-3.8){O} \uput[l](O){\scalebox{1}{$84$}} &
    \psPoint(0,0,0){O1}\psPoint(0,1.5,0){O2}%
    \psline[linecolor=black,linewidth=3pt]{->}(O1)(O2)&
    \myCube{1}{1}{1}{(0,0,0)}
    \psPoint(0,0.5,0){O} \uput[l](O){$\Large\bo$}
    \psPoint(0,0.6,-1.1){O} \uput[l](O){\scalebox{1}{$10$}}
    \end{tabular}
}}
\end{frame}

\begin{frame}{Convolutional Neural Network}
\framesubtitle{Example}

    We train the CNN model on $n=60000$ training images from the
    MNIST dataset; we train for $15$ epochs using step size
    $\eta=0.2$ and using cross-entropy error (since there are 10
    classes). 


\medskip

    Training was done using minibatches, using batch size of 1000.    
    After training the CNN model, we evaluate 
    it on the test  dataset of 10,000 images. 
    The CNN model makes 147 errors on the
    test set, resulting in an error rate of 1.47\%.
    %\cref{fig:reg:deep:mnist_incorrect} 


%\medskip

%Figure shows examples of images that are misclassified by the CNN. 
%We show the true label $y$ for each
%image and the predicted label $o$ (converted back from the one-hot
%encoding to the digit label). We show three examples for each
%of the labels. For example, the first three images on the first row
%are for the case when the true label is $y=0$, and the next three
%examples are for true label $y=1$, and so on. 
%We can see that several of the misclassified images are noisy, 
%incomplete or erroneous, and hard to classify correctly even by
%a human.
\end{frame}

%\end{example}
%
%\begin{figure}[!t]
\begin{frame}{MNIST: Incorrect predictions}
Figure shows examples of images that are misclassified by the CNN. 
%We show the true label $y$ for each
%image and the predicted label $o$ (converted back from the one-hot
%encoding to the digit label). We show three examples for each
%of the labels. For example, the first three images on the first row
%are for the case when the true label is $y=0$, and the next three
%examples are for true label $y=1$, and so on. 
We can see that several of the misclassified images are noisy, 
incomplete or erroneous, and hard to classify correctly even by
a human.

\medskip

    \newcommand\myplotmnistimg[2]{%
    \begin{tikzpicture}
        \begin{axis}[
            colormap/blackwhite,
            enlargelimits=false,
            axis on top,
            title={\scalebox{3.5}{####2}},
            point meta min=0,
            point meta max=255,
        ]
       \addplot [matrix plot,point meta=explicit] file [meta=index 2]
            {####1};
        \end{axis}
    \end{tikzpicture}
    }
%    % \vspace{-0.2in}
    \centerline{
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_00}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM0_0.txt}{$y=0,o=2$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_01}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM0_1.txt}{$y=0,o=8$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_02}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM0_2.txt}{$y=0,o=6$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_10}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM1_0.txt}{$y=1,o=8$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_11}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM1_1.txt}{$y=1,o=7$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_12}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM1_2.txt}{$y=1,o=5$}}
}
\vspace{0.1in}
\centerline{
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_20}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM2_0.txt}{$y=2,o=0$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_21}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM2_1.txt}{$y=2,o=8$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_22}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM2_2.txt}{$y=2,o=7$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_30}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM3_0.txt}{$y=3,o=2$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_31}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM3_1.txt}{$y=3,o=5$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_32}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM3_2.txt}{$y=3,o=8$}}
}
\vspace{0.1in}
\end{frame}
\begin{frame}{MNIST: Incorrect predictions}
    \newcommand\myplotmnistimg[2]{%
    \begin{tikzpicture}
        \begin{axis}[
            colormap/blackwhite,
            enlargelimits=false,
            axis on top,
            title={\scalebox{3.5}{####2}},
            point meta min=0,
            point meta max=255,
        ]
       \addplot [matrix plot,point meta=explicit] file [meta=index 2]
            {####1};
        \end{axis}
    \end{tikzpicture}
    }
\vspace{0.1in}
\centerline{
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_40}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM4_0.txt}{$y=4,o=2$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_41}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM4_1.txt}{$y=4,o=2$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_42}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM4_2.txt}{$y=4,o=7$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_51}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM5_0.txt}{$y=5,o=6$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_52}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM5_1.txt}{$y=5,o=3$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_53}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM5_2.txt}{$y=5,o=3$}}
}
\vspace{0.1in}
\centerline{
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_60}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM6_0.txt}{$y=6,o=4$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_61}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM6_1.txt}{$y=6,o=5$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_63}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM6_2.txt}{$y=6,o=1$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_70}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM7_0.txt}{$y=7,o=3$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_71}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM7_1.txt}{$y=7,o=8$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_72}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM7_2.txt}{$y=7,o=2$}}
}
\vspace{0.1in}
\centerline{
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_80}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM8_0.txt}{$y=8,o=6$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_81}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM8_1.txt}{$y=8,o=3$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_82}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM8_2.txt}{$y=8,o=2$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_90}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM9_0.txt}{$y=9,o=7$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_91}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM9_1.txt}{$y=9,o=4$}}
    \resizebox{0.75in}{0.75in}{%
    % \tikzsetnextfilename{reg_deep_mnist_incorrect_92}
    \myplotmnistimg{REG/deep/figs/test_loss/digit_IM9_2.txt}{$y=9,o=0$}}
    }
    % \centerline{
    %     \begin{tabular}{p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}}
    %         $\qquad0$ & $\qquad1$ & $\qquad2$ & $\qquad3$ & $\qquad4$ &
    %         $\qquad5$ & $\qquad6$ & $\qquad7$ & $\qquad8$ & $\qquad9$\\
    %     \end{tabular}
    % }
\end{frame}

\begin{frame}{Convolutional Neural Network}
\framesubtitle{Example}
    
    For comparison, we also train a deep MLP with two (fully
    connected) hidden layers with the same sizes as the two fully
    connected layers before the output layer in the CNN shown in
    %\cref{fig:reg:deep:cnn}. 
    figure. 

\medskip

Therefore, the MLP comprises the layers
    $\bX$, $\bZ^5$, $\bZ^6$, and $\bo$, with the input $28\times28$
    images viewed as a vector of size $d=784$. The first hidden layer
    has size $n_1 = 120$, the second hidden layer has size $n_2=84$, and
    the output layer has size $p=10$. We use ReLU activation function
    for all layers, except the output, which uses softmax. 

	\medskip

	We train the
    MLP model for $15$ epochs on the training dataset with $n=60000$
    images, using step size $\eta=0.5$. On the test dataset, the MLP
    made 264 errors, for an error rate of 2.64\%.
%    \cref{fig:reg:deep:mnist_cnn_mlp} 


\end{frame}
%\vspace{0.2in}
    %\caption{MNIST: Incorrect predictions by the CNN model; $y$
    %is the true label, $o$ is the predicted label.}
%    \label{fig:reg:deep:mnist_incorrect}
%    \vspace{-0.2in}
%\end{figure}
%
%% \begin{figure}[!thbp]
%%     \centering
%% \pgfplotscreateplotcyclelist{my black white}{%
%% solid, every mark/.append style={solid, fill=gray}, mark=*\\%
%% solid, every mark/.append style={solid, fill=gray}, mark=triangle*\\%
%% % densely dotted, every mark/.append style={solid, fill=gray},
%% % mark=diamond*\\%
%% % densely dashed, every mark/.append style={solid, fill=gray},
%% % mark=triangle*\\%
%% }
%%     % \tikzsetnextfilename{reg_neural_mnist_errors_mlp}
%% \begin{tikzpicture}
%%     \begin{axis}[
%%             width=3in,
%%             height=2.5in,
%%             scale only axis,
%%             xmin=1, xmax=15,
%%             ymin=100, ymax=7500,
%%             label style={font=\small},
%%             tick label style={font=\tiny},
%%             xlabel=epochs,
%%         ylabel=errors,
%%         xtick={1,2,3,4,5,6,7,8,9,10,11,12,13,14,15},
%%         % enlargelimits=false,
%%         axis on top,
%%         legend style ={ at={(0.75,0.95)},
%%             anchor=north west, draw=black,
%%         fill=white, align=left, font=\small},
%%         cycle list name=my black white,
%%         % smooth
%%     ]
%%     \addplot table [x index=0, y index=2] %
%%     {REG/deep/figs/test_loss/test_loss_output_mlp_120_84.txt};
%%     \addlegendentry{MLP};
%%     % \addplot table [x index=0, y index=2] %
%%     % {REG/deep/figs/test_loss/test_loss_output_deep_m392_196_98_49.txt};
%%     % \addlegendentry{$n_1=392,n_2=196,n_3=98,n_4=49$};
%%     \addplot table [x index=0, y index=2] %
%%     {REG/deep/figs/test_loss/test_loss_output_cnn.txt};
%%     \addlegendentry{CNN};
%%     \end{axis}
%% \end{tikzpicture}
%% \caption{MNIST: CNN versus Deep MLP; prediction error as a function of epochs}
%% \label{fig:reg:deep:mnist_cnn_mlp}
%% \end{figure}
%
%
%\begin{figure}[!b]
\begin{frame}{MNIST: CNN versus Deep MLP}
\framesubtitle{Prediction error as a function of epochs.}
Graph shows the number of errors on the
test set after each epoch of training for both the CNN and MLP
model; the CNN model achieves significantly
better accuracy than the MLP.
    
\medskip

\centering
\pgfplotscreateplotcyclelist{my black white}{%
solid, every mark/.append style={solid, fill=gray}, mark=*\\%
solid, every mark/.append style={solid, fill=gray}, mark=triangle*\\%
% densely dotted, every mark/.append style={solid, fill=gray},
% mark=diamond*\\%
% densely dashed, every mark/.append style={solid, fill=gray},
% mark=triangle*\\%
}
% \tikzsetnextfilename{reg_deep_mnist_cnn_mlp}
\vspace{0.4in}
\scalebox{0.9}{%
\begin{tikzpicture}
    \begin{axis}[
            width=3in,
            height=2in,
            scale only axis,
            xmin=1, xmax=15,
            ymin=100, ymax=7500,
            label style={font=\small},
            tick label style={font=\tiny},
            xlabel=epochs,
        ylabel=errors,
        xtick={1,2,3,4,5,6,7,8,9,10,11,12,13,14,15},
        % enlargelimits=false,
        axis on top,
        legend style ={ at={(0.75,0.95)},
            anchor=north west, draw=black,
        fill=white, align=left, font=\small},
        cycle list name=my black white,
        % smooth
    ]
    \addplot table [x index=0, y index=2] %
    {REG/deep/figs/test_loss/test_loss_output_mlp_120_84.txt};
    \addlegendentry{MLP};
    % \addplot table [x index=0, y index=2] %
    % {REG/deep/figs/test_loss/test_loss_output_deep_m392_196_98_49.txt};
    % \addlegendentry{$n_1=392,n_2=196,n_3=98,n_4=49$};
    \addplot table [x index=0, y index=2] %
    {REG/deep/figs/test_loss/test_loss_output_cnn.txt};
    \addlegendentry{CNN};
    \end{axis}
\end{tikzpicture}
}
\end{frame}
%\caption{MNIST: CNN versus Deep MLP; prediction error as a function of
%epochs.}
%\label{fig:reg:deep:mnist_cnn_mlp}
%\end{figure}
%
%\section{Regularization}
%\index{regularization}
%\index{deep learning!regularization}


\ifdefined\wox \begin{frame} \titlepage \end{frame} \fi


\begin{frame}{Regularization}
%
%Consider the squared error loss function, given as
%\begin{align*}
%    L(\by, \hby) = (\by - \hby)^2
%\end{align*}
%where $\by$ is the true response and $\hby=\bo$ the predicted response on a
%given input $\bx$.
%The goal of learning is the minimize the expected loss
%$E[L(\by, \hby)] = E[(\by-\hby)^2]$. 
%
%As described in \cref{sec:class:eval:bias-variance-decomp},
%the expected loss can be decomposed into three terms:
%noise, bias, and variance, given as 
%\begin{align}
%E\lB[ (\by - \hby)^2 \rB] = 
%\underbrace{E\Bigl[\bigl(\by - E[\by]\bigr)^2 \Bigr]}_{\rm noise} +
%\underbrace{E\Bigl[ \bigl(\hby - E[\hby]\bigr)^2\Bigr]}_{{\it average\ variance}} +
%\underbrace{E\Bigl[\bigl(E[\hby] - E[\by]\bigr)^2\Bigr]}_{{\it average\ bias}}
%  \label{eq:reg:eval:NBV}
%\end{align}
%
%The noise term is the expected variance of $\by$, since $E[(\by -
%E[\by])^2] = \var(\by)$. It contributes a f\/{i}xed
%cost to the loss independent of the model. Since it is the inherent
%uncertainty or variability of $\by$ it can be ignored
%when comparing different models. 
%The average bias term indicates how much the model deviates from the true (but
%unknown) function relating the response to the predictor variables. 
%For example, if the response is a non-linear
%function of the predictor variables, and we fit a
%simple linear model, then this model will have a high
%bias. We can try to fit a more complex non-linear model to reduce the
%bias, but then we run into the issue of overfitting and high variance,
%captured by the average variance term, which quantifies the variance of
%the predicted response $\hby$, since $E[(\hby -
%E[\hby])^2] = \var(\hby)$.
%That is, our model may try to fit noise
%and other artifacts in the data, and will therefore be
%highly susceptible to small changes in the data, resulting in
%high variance. In general, there is always a 
%trade-off between reducing bias and reducing variance.
%\index{regression!overfitting} 
%\index{deep learning!overfitting} 
%
Regularization is an approach whereby we constrain the model parameters
to reduce overfitting, by reducing the variance
at the cost of increasing the bias slightly. 
%For example, in
%ridge regression [\cref{eq:reg:linear:ridgeobj}], we add a constraint on the $L_2$-norm of the weight
%parameters as follows
%\begin{align}
%    \min_\bw\;\; J(\bw) = \norm{Y - \hY}^2 + \alpha \cdot \norm{\bw}^2
%    = \norm{Y - \bD\bw}^2 + \alpha \cdot \norm{\bw}^2
%\end{align}
%where $Y$ is the true response vector and
%$\hY$ is the predicted response vector over all training instances.
%The goal here is to drive the weights to be small, depending on the
%{\em hyperparameter} $\alpha \ge 0$ called the {\em regularization constant}.
%If $\alpha=0$, then there is no regularization, and we get a low bias,
%but possibly high variance model. If $\alpha \to \infty$
%then the effect is to drive all weights to be nearly zero, which results in a low
%variance, but high bias model. An intermediate value of $\alpha$
%tries to achieve the right balance between these two conflicting
%objectives.
%As another example, in $L_1$ regression or Lasso
%[\cref{eq:reg:linear:lasso_obj}], we minimize the $L_1$
%norm of the weights
%\begin{align*}
%    \min_\bw\;\; J(\bw) = \norm{Y - \bD\bw}^2 + \alpha \cdot \norm{\bw}_1
%\end{align*}
%where $\norm{\bw}_1 = \sum^{d}_{i=1} \abs{w_i}$. Compared to $L_2$ norm,
%which merely makes the weights small, the use of the $L_1$ norm {\em
%sparsifies} the model by forcing many of the weights to zero, acting as
%a feature subset selection method.
%

\medskip

In general, for any learning model $M$, if $L(\by, \hby)$ is some loss
function for a given input $\bx$, and $\bTheta$ denotes all the model
parameters, where $\hby = M(\bx | \bTheta)$,
the objective is to find the parameters that
minimize the loss over all instances:
\begin{align*}
    \min_{\bTheta} \; J(\bTheta) =  
   \sum_{i=1}^n L(\by_i, \hby_i) = 
    \sum_{i=1}^n L(\by_i, M(\bx_i|\bTheta))
\end{align*}
With regularization, we add a penalty on the parameters $\bTheta$:
\begin{align*}
    \tcbhighmath{
    \min_{\bTheta} \; J(\bTheta) = 
\sum_{i=1}^n L(\by_i, \hby_i) + \alpha R(\bTheta)}
\end{align*}
where $\alpha \ge 0$ is the regularization constant.
\end{frame}

%Let $\theta \in \bTheta$ be a parameter of the regression model.
%Typical regularization functions include the $L_2$ norm, the $L_1$ norm,
%or even a combination of these, called the elastic-net:
%\begin{align*}
%    R_{L_2}(\theta) & = \norm{\theta}^2_2 &
%    R_{L_1}(\theta) & = \norm{\theta}_1 &
%    R_{\text{elastic}}(\theta) &  = \lambda \cdot \norm{\theta}_2^2 +
%    (1-\lambda)\cdot \norm{\theta}_1
%\end{align*}
%with $\lambda \in [0,1]$.
%
%\subsection{$L_2$ Regularization for Deep Learning}
%\index{deep learning!L$_2$ regularization}
%\index{L$_2$ regularization}
%\index{L$_2$ regularization!deep learning}
\begin{frame}{L2 Regularization for Deep Learning}
%We now consider approaches for regularizing the deep learning models.
We first consider the case of a multilayer perceptron with one hidden
layer, and then generalize it. % to multiple hidden layers.
%Note that while our discussion of regularization is in the context of
%MLPs, since RNNs are trained via unfolding, and CNNs
%are essentially sparse MLPs, the methods described here can easily be
%generalized for any deep neural network.
%
%
%\begin{figure}[!b]
%    \centering
%    \psset{tnpos=l,tnsep=2pt,colsep=1.5,rowsep=0.75,mcol=c,
%    ArrowInside=->, arrowscale=2}
%\centerline{
%\scalebox{1}{%
%\psmatrix
%[mnode=none]{$l=0$} 
%    & [mnode=none]{$l=1$} 
%    & [mnode=none]{$l=2$} \\[-1em]
%\TR[name=l0]{\myfbox{$\bx$}}
%    & \TR[name=l1]{\myfbox{$\bz$}}
%    & \TR[name=lo]{\myfbox{$\bo$}}
%\psset{ArrowInside=-}
%\psset{arrows=->}
%\psset{linewidth=2pt}
%%bias to next layer
%\ncline{l0}{l1}\nbput[npos=0.5]{$\bW_{\!h}, \bb_{h}$}
%\ncline{l1}{lo}\nbput[npos=0.5]{$\bW_{\!o}, \bb_{o}$}
%\endpsmatrix
%}}
%\vspace{0.2in}
%\caption{Multilayer perceptron with one hidden layer.}
%\label{fig:reg:eval:MLP1h}
%\end{figure}
%
%
%% For
%% concreteness, assume that the loss function is the squared error
%% function
%% \begin{align*}
%%     \bcE_{\bx} = \frac{1}{2} \norm{\by - \bo}^2
%% \end{align*}
%% and that the activation functions at the hidden and output layer are
%% both sigmoid activations.
%% The set of parameters of the model are 
%% \begin{align*}
%%     b = \lB( \bW_{\!h}, \bb_{h}, \bW_{\!o},
%%     \bb_{o}\rB)
%% \end{align*}
%% The $L_2$ regularized objective is therefore given as
%% \begin{align*}
%%     \min_{b} \; J(b) & =  \bcE_{\bx} + \frac{\alpha}{2} \cdot
%%     R_{L_2}(\bW_{\!o}, \bW_{\!h})\\
%%      & = \frac{1}{2} \cdot \norm{\by - \bo}^2 +
%%      \frac{\alpha}{2} \cdot \lB( \norm{\bW_{\!h}}_{F}^2 + \norm{\bW_{\!o}}_{F}^2
%%     \rB)\\
%%     & = \frac{1}{2} \cdot \norm{\by - f\Bigl(\bb_{o} + \bW^T_{\!o} \cdot
%%     f\lB(\bb_{h} + \bW_{\!h}^T\bx\rB)\Bigr)}^2 + 
%%     \frac{\alpha}{2} \cdot \lB( \norm{\bW_{\!h}}_{F}^2 + \norm{\bW_{\!o}}_{F}^2
%%     \rB)
%% \end{align*}
%
%
%\subsubsection{MLP with One Hidden Layer}
%Consider regularization in the context of a feed-forward
%MLP with a single hidden layer as shown in \cref{fig:reg:eval:MLP1h}. 
%Let the input $\bx \in \setR^d$, the
%hidden layer $\bz \in \setR^m$ and let $\hby = \bo \in \setR^p$. 
The parameters of the model are 
\begin{align*}
    \bTheta = \lB\{ \bW_{\!h}, \bb_{h}, \bW_{\!o},
    \bb_{o}\rB\}
\end{align*}
%Whereas it makes sense to penalize large weights, we usually do not
%penalize the bias terms since they are just thresholds that
%shift the activation function and there is no need to force them to 
%be small values. 
The $L_2$ regularized objective is therefore given as
\begin{align*}
    \min_{\bTheta} \; J(\bTheta) & =  \cE_{\bx} + \frac{\alpha}{2} \cdot
    R_{L_2}(\bW_{\!o}, \bW_{\!h}) = 
    \cE_{\bx} +
     \frac{\alpha}{2} \cdot \lB( \norm{\bW_{\!h}}_{F}^2 + \norm{\bW_{\!o}}_{F}^2
    \rB)
\end{align*}
%Here we added the factor $1/2$ to the regularization term for convenience.
%For the $L_2$ norm of the weight matrices, 
%we use the {\em Frobenius norm},
%which has the usual sense of $L_2$-norm, since for an $n \times m$ matrix
%$\bA$, it is defined as
%\begin{align*}
%    \norm{\bA}_F^2 = \sum^{n}_{i=1} \sum^{m}_{j=1} a_{ij}^2
%\end{align*}
%where $a_{ij}$ is the $(i,j)$th element of $\bA$.
The regularized objective tries to minimize the individual weights
for pairs of neurons. 
This adds some bias, but 
reduces variance, since small weights are more robust to changes in the
input data in terms of the predicted output values.
%
%The regularized objective has two separate terms, one for the
%loss and the other for the $L_2$ norm of the weight matrices.
%Recall that we have to compute the weight gradients $\grad_{w_{ij}}$ and
%the bias gradients $\grad_{b_{\!j}}$ by computing 
%\begin{align*}
%    \grad_{w_{ij}} & = 
%    \frac{\partial J(\bTheta)}{\partial w_{ij}}
%    = \frac{\partial \cE_\bx}{\partial w_{ij}} + \frac{\alpha}{2} \cdot
%    \frac{\partial R_{L_2}(\bW_{\!o}, \bW_{\!h})}{\partial w_{ij}}
%    = \delta_{\!j} \cdot z_i + \alpha \cdot w_{ij}\\
%    \grad_{b_{\!j}} & 
%    =  \frac{\partial J(\bTheta)}{\partial b_{\!j}}
% = \frac{\partial \cE_\bx}{\partial b_{\!j}} +
%    \frac{\alpha}{2} \cdot
%\frac{\partial R_{L_2}(\bW_{\!o}, \bW_{\!h})}{\partial b_{\!j}}
%    = \frac{\partial \cE_\bx}{\partial b_{\!j}} = \delta_{\!j}
%\end{align*}
%where we use \cref{eq:reg:neural:gradwij} to note that 
%$\frac{\partial \cE_\bx}{\partial w_{ij}} = \delta_j \cdot z_i$ and 
%$\frac{\partial \cE_\bx}{\partial b_{\!j}} = \delta_j$, and 
%where 
%$\delta_{\!j} =  \frac{\partial \cE_\bx}{\partial \net_{\!j}}$ is the
%net gradient.
%Further,
%since the squared $L_2$ norm of a weight matrix is simply the 
%sum of the squared weights, only the term $w_{ij}^2$
%matters, and all other elements are just constant with respect to the
%weight $w_{ij}$ between neurons $i$ and $j$ (in $\bW_{\!h}$ or
%$\bW_{\!o}$).
%Across all the neuron pairs between the hidden and output layer, we can write
%the update rule compactly as follows:
%\begin{align*}
%    \bgrad_{\bW_{\!o}} & = \bz \cdot \bdelta_{\!o}^T + \alpha \cdot
%    \bW_{\!o} &
%    \bgrad_{\bb_{o}} & = \bdelta_{\!o}
%    % \bgrad_{\bW_{\!h}} = \bz \cdot \bdelta_{\!h}^T + \bW_{\!h}
%\end{align*}
%where $\bdelta_{\!o}$ 
%is the net gradient vector for the output neurons,
%and $\bz$ is the vector of hidden layer neuron values. 
The gradient update rule is %using the regularized weight gradient matrix is
given as
\begin{align*}
    \bW_{\!o} & = \bW_{\!o} - \eta \cdot \bgrad_{\bW_{\!o}}
    = \bW_{\!o} - \eta \cdot \lB(\bz \cdot \bdelta_{\!o}^T +
    \alpha \cdot \bW_{\!o}\rB)
     = \bW_{\!o} - \eta \cdot \alpha \cdot \bW_{\!o} - \eta \cdot \lB(\bz \cdot
     \bdelta_{\!o}^T \rB)\\
     & = (1-\eta\cdot\alpha) \cdot \bW_{\!o} - \eta \cdot \lB(\bz \cdot \bdelta_{\!o}^T \rB)
\end{align*}
$L_2$ regularization is also called {\em weight decay}, since 
the updated weight matrix uses decayed weights from the
previous step, using the decay factor $1-\eta \cdot \alpha$.
\end{frame}
%
%In a similar manner we get the weight and bias gradients between the
%input and hidden layers:
%\begin{align*}
%    \bgrad_{\bW_{\!h}} & = \bx \cdot \bdelta_{\!h}^T + \alpha \cdot
%    \bW_{\!h} &
%    \bgrad_{\bb_{h}} & = \bdelta_{\!h}
%\end{align*}
%The update rule for the weight matrix between
%the input and hidden layers is therefore given as
%\begin{align*}
%    \bW_{\!h} & = \bW_{\!h} - \eta \cdot \bgrad_{\bW_{\!h}}
%     = (1-\eta\cdot\alpha) \cdot \bW_{\!h} - \eta \cdot \lB(\bx \cdot \bdelta_{\!h}^T \rB)
%\end{align*}
%where $\bdelta_{\!h}$ 
%is the net gradient vector for the hidden neurons,
%and $\bx$ is the input vector. 
%
%\begin{figure}[!t]
%    \centering
%    \psset{tnpos=l,tnsep=2pt,colsep=1.5,rowsep=0.75,mcol=c,
%    ArrowInside=->, arrowscale=2}
%\centerline{
%\scalebox{0.9}{%
%\psmatrix
%[mnode=none]{$l=0$} 
%    & [mnode=none]{$l=1$} 
%    & [mnode=none]{$l=2$} 
%    & [mnode=none]{$\ldots$} 
%    & [mnode=none]{$l=h$} 
%    & [mnode=none]{$l=h+1$}\\[-1em]
%    %\TR[name=l0]{\psframe(-1,-1)(1,1)\rput(0,0){$\bx$}}
%\TR[name=l0]{\myfbox{$\bx$}}
%    & \TR[name=l1]{\myfbox{$\bz^1$}}
%    & \TR[name=l2]{\myfbox{$\bz^2$}}
%    & \TR[name=li]{\myfboxB[linecolor=gray]{$\cdots\!$}}
%    & \TR[name=lh]{\myfbox{$\bz^h$}}
%    & \TR[name=lo]{\myfbox{$\bo$}}
%\psset{ArrowInside=-}
%\psset{arrows=->}
%\psset{linewidth=2pt}
%%bias to next layer
%\ncline{l0}{l1}\nbput[npos=0.5]{$\bW_0, \bb_0$}
%\ncline{l1}{l2}\nbput[npos=0.5]{$\bW_1, \bb_1$}
%\ncline[linecolor=gray,linewidth=1pt]{l2}{li}%\nbput[npos=0.5]{$\bW_2, \bb_2$}
%\ncline[linecolor=gray,linewidth=1pt]{li}{lh}%\nbput[npos=0.5]{$\bW_{h-1},\bb_{h-1}$}
%\ncline{lh}{lo}\nbput[npos=0.5]{$\bW_h, \bb_{h}$}
%\endpsmatrix
%}}
%\vspace{0.2in}
%\caption{Deep multilayer perceptron.}
%\label{fig:reg:eval:deepMLP}
%\end{figure}
%
%\subsubsection{Deep MLPs}
\begin{frame}{Deep MLPs}
%Consider the deep MLP shown in \cref{fig:reg:eval:deepMLP}.
%We denote the input neurons as
%layer $l=0$, the first hidden layer as $l=1$, the last hidden layer as
%$l=h$, and the final output
%layer as $l=h+1$. 
%The vector of neuron values for layer $l$ (for $l=0,\cdots,h+1$) 
%is denoted as
%\begin{align*}
%    \bz^l = \lB(z^l_{1}, \cdots, z^l_{n_l}\rB)^T
%\end{align*}
%where $n_l$ is the number of neurons in layer $l$.
%Thus, $\bx = \bz^0$ and $\bo = \bz^{h+1}$.
%The weight matrix between 
%neurons in layer $l$ and layer $l+1$
%is denoted $\bW_{\!l} \in \setR^{n_{l} \times n_{l+1}}$, 
%and the vector of bias terms from the bias neuron 
%$z^l_0$ to neurons in layer $l+1$ is
%denoted $\bb_l \in \setR^{n_{l+1}}$,
%for $l=1,\cdots,h+1$.
%
Given the error function $\cE_{\bx}$, the $L_2$ regularized objective
function is
\begin{align*}
    \min_{\bTheta} \; J(\bTheta) & =  \cE_{\bx} + \frac{\alpha}{2} \cdot
    R_{L_2}\lB(\bW_{\!0}, \bW_{\!1}, \ldots, \bW_{h}\rB)
    & = \cE_{\bx} +
    \frac{\alpha}{2} \cdot \lB( \sum_{l=0}^h || \bW_{\!l}||_{F}^2 \rB)
\end{align*}
where the parameters of the model are 
$\bTheta = \lB\{\bW_{\!0}, \bb_{0}, \bW_{\!1}, \bb_{1}, \cdots, 
\bW_{\!h}, \bb_{h}\rB\}$.

\medskip

Based on the derivation for the one hidden layer, the
regularized gradient is:
\begin{align*}
    \tcbhighmath{
        \bgrad_{\bW_{\!l}}  = \bz^{l} \cdot (\bdelta^{l+1})^T + \alpha \cdot
\bW_{\!l}}
\end{align*}
and the update rule for weight matrices is
\begin{align*}
    \tcbhighmath{
    \bW_{\!l} = \bW_{\!l} - \eta \cdot \bgrad_{\bW_{\!l}}
     = (1-\eta\cdot\alpha) \cdot \bW_{\!l} - \eta \cdot \lB(\bz^l \cdot
 (\bdelta^{l+1})^T \rB)}
\end{align*}
for $l=0,1,\cdots,h$, where
where $\bdelta^{l}$ 
is the net gradient vector for the hidden layer $l$.
\end{frame}
%We can thus observe that incorporating $L_2$ regularization within deep MLPs
%is relatively straightforward. Likewise, it is easy to incorporate $L_2$
%regularization in other models like RNNs, CNNs, and so on.
%For $L_1$ regularization, we can apply the subgradient approach outlined
%for $L_1$ regression or Lasso in
%\cref{sec:reg:linear:lasso}.
%
%
%\subsection{Dropout Regularization}
%\index{dropout regularization}
%\index{deep learning!dropout regularization}
\begin{frame}{Dropout Regularization}
The idea behind dropout regularization is to randomly set a certain
fraction of the neuron values in a layer to zero during training time. The aim is to make the
network more robust and to avoid overfitting at the same time. By
dropping random neurons for each training point, the network is forced
to not rely on any specific set of edges. 

\medskip

From the perspective of a
given neuron, since it cannot rely on all its incoming edges to be
present, it has the effect of not concentrating the weight on specific
input edges, but rather the weight is spread out among
the incoming edges. 


\medskip

The net effect is similar to $L_2$ regularization since weight
spreading leads to smaller weights on the edges. The resulting model
with dropout is therefore more resilient to small perturbations in the
input, which can
reduce overfitting at a small price in increased bias. 

\medskip

However, note
that while $L_2$ regularization directly changes the objective function,
dropout regularization is a form of {\em structural regularization} that
does not change the objective function, but instead changes the
network topology in terms of which connections are currently active or
inactive.
\end{frame}

\begin{frame}{Dropout Regularization}
%
%\subsubsection{MLP with One Hidden Layer}
\framesubtitle{MLP with One Hidden Layer}
%Consider the one hidden layer MLP in \cref{fig:reg:eval:MLP1h}. 
%Let the input $\bx \in \setR^d$, the
%hidden layer $\bz \in \setR^m$ and let $\hby = \bo \in \setR^p$. 
During the training phase, for each input $\bx$, we create a random mask
vector to drop a fraction of the hidden neurons. Formally, let $r \in
[0,1]$ be
the probability of keeping a neuron, 
so that the dropout probability is $1-r$.


\medskip

We create a
$m$-dimensional multivariate
Bernoulli vector $\bu \in \{0,1\}^m$, called the {\em masking
vector}, each of whose entries is $0$
with dropout  probability $1-r$, and $1$ with probability $r$. 
Let $\bu
= (u_{1}, u_{2}, \cdots, u_{m})^T$, where
\begin{align*}
    u_{i} = 
    \begin{cases}
        0 & \text{ with probability } 1-r\\
        1 & \text{ with probability } r\\
    \end{cases}
\end{align*}
The feed-forward step is then given as
\begin{align*}
    \bz & = f^h\lB(\bb_{h} + \bW_{\!h}^T \bx \rB)\\
    \btz & = \bu \odot \bz\\
    \bo & = f^o\lB(\bb_{o} + \bW_{\!o}^T \btz \rB)
\end{align*}
where $\odot$ is the element-wise multiplication. 
\end{frame}
%The net effect is that
%the masking vector zeros out the $i$th hidden neuron in $\btz$ if $u_i = 0$.
%Zeroing out also has the effect that during the 
%backpropagation phase the error gradients do not flow back from the
%zeroed out neurons in the hidden layer. The effect is that any weights
%on edges adjacent to zeroed out hidden neurons are not updated.
%
%
%\paragraph{Inverted Dropout}
%\index{regularization!inverted dropout}
%\index{deep learning!inverted dropout}
\begin{frame}{Inverted Dropout}
There is one complication in the basic dropout approach above, namely,
the expected output of hidden layer neurons is different during training
and testing, since dropout is not applied during the testing phase 
(after all, we do not want the predictions to be randomly varying on a
given test input). With $r$ as the probability of retaining a
hidden neuron, its expected output value is
\begin{align*}
    E[z_i] = r \cdot z_i + (1-r) \cdot 0 = r \cdot z_i
\end{align*}
On the other hand, since there is no dropout at test time, the outputs
of the hidden neurons will be higher at testing time. So one idea is to
scale the hidden neuron values by $r$ at testing time. 

	\medskip

	On the other
hand, there is a simpler approach called {\em inverted dropout} that
does not need a change at testing time. The idea is to rescale the
hidden neurons after the dropout step during the training phase, as follows:
\begin{align*}
	\bz & = f\lB(\bb_{h} + \bW_{\!h}^T \bx \rB) &
	\btz & = \frac{1}{r} \cdot \bigl(\bu \odot \bz\bigr) &
    \bo & = f\lB(\bb_{o} + \bW_{\!o}^T \btz \rB)
\end{align*}
\end{frame}
%With the scaling factor $1/r$, the expected value of each neuron
%remains the same as without dropout, since
%\begin{align*}
%    E[z_i] = \frac{1}{r} \cdot \bigl(r \cdot z_i + (1-r)
%    \cdot 0 \bigr) = z_i
%\end{align*}
%% An added advantage is that we do not have to do any changes at testing
%% time.
%
%\subsubsection{Dropout in Deep MLPs}
\begin{frame}{Dropout in Deep MLPs}
Dropout regularization for deep MLPs is done in a similar manner. Let
$r_l \in [0,1]$, for $l=1,2,\cdots,h$ denote the probability of
retaining a hidden neuron for
layer $l$, so that $1-r_l$ is the dropout probability. 
One can also use a single rate $r$ for all the layers by
setting $r_l = r$.
Define the masking vector for hidden layer $l$, $\bu^l \in
\{0,1\}^{n_l}$, as follows:
\begin{align*}
    u^l_{i} = 
    \begin{cases}
        0 & \text{ with probability } 1-r_l\\
        1 & \text{ with probability } r_l\\
    \end{cases}
\end{align*}
The feed-forward step between layer $l$ and $l+1$ is then given as
\begin{empheq}[box=\tcbhighmath]{align*}
    \begin{aligned}
    \bz^l & = f\lB(\bb_{l} + \bW_{\!l}^T \; \btz^{l-1} \rB)\\
    \btz^l & = \frac{1}{r_l} \cdot \lB(\bu^l \odot \bz^l\rB)
\end{aligned}
\end{empheq}
using inverted dropout.
\end{frame}
%Usually, no masking is done for the input and output layers, so we can
%set $r^0=1$ and $r^{h+1} = 1$. 
%%Therefore, $\btz^0 =
%%\bx$, and $\bo = f\lB( \bb_{h} + \bW_{\!h}^T \; \btz^h \rB)$. 
%Also note that there is no dropout at testing time.
%The dropout rates are hyperparameters of the model that have to be tuned
%on a separate validation dataset. 
%
%% \subsection{Other Regularization Techniques}
%% Early Stopping, Parameter Tying and Sharing
%
%
%
%\section{Further Reading}
%\label{sec:reg:deep:ref}
%\begin{refsection}
%
%The backpropagation through time algorithm was introduced by
%\citet*{werbos1990backpropagation}. 
%Bidirectional RNNs were proposed by \citet*{schuster1997bidirectional},
%and LSTMs were proposed by
%\citet*{hochreiter1997long}, with the forget gate introduced by
%\citet*{gers2000forget}. 
%Convolutional neural networks trained via backpropagation were proposed
%by \citet*{lecun1998gradient}, with application to 
%handwritten digit recognition. Dropout regularization was introduced by
%\citet*{srivastava2014dropout}.
%
%\printbibliography[heading=emptyheading]
%\end{refsection}
%
%\section{Exercises}
%\label{sec:reg:deep:exercise}
%
%\begin{exercises}[Q1.]
%
%    \begin{figure}[!b]
%\tikzset{
%modal/.style={>=stealth',shorten >=1pt,shorten <=1pt,auto,node distance=1.5cm,
%semithick},
%world/.style={circle,draw,minimum size=0.5cm,fill=gray!15},
%point/.style={circle,draw,inner sep=0.5mm,fill=black},
%reflexive above/.style={->,loop,looseness=7,in=120,out=60},
%reflexive below/.style={->,loop,looseness=7,in=240,out=300},
%reflexive left/.style={->,loop,looseness=7,in=150,out=210},
%reflexive right/.style={->,loop,looseness=7,in=30,out=330}
%}
%    \centerline{\scantokens{
%% \tikzsetnextfilename{reg_deep_ex_1}
%\begin{tikzpicture}
%[auto, 
%    block/.style ={rectangle, draw=black, thick, fill=gray!20,
%    text width=1em,align=center, rounded corners,
%    minimum height=5em},
%    line/.style ={draw, thick, -Latex,shorten >=2pt},
%]
%\matrix [column sep=15mm,row sep=1mm]
%{
%% row 1
%\node [block] (x) {$\bx_t$}; &
%\node [block] (h) {$\bh_t$}; &
%\node [block] (o) {$\bo_t$}; \\
%};
%\begin{scope}[every path/.style=line, 
%    node/.style={font=\small\itshape,fill=white}]
%\path (x) -- (h);
%\path (h) -- (o);
%\path (h) edge [reflexive above] node [above] {-1} (h);
%\path (h) edge [reflexive below] node [midway] {-2} (h);
%\path (o) to [bend right=30] node [above, sloped] {-1} (h);
%\end{scope}
%\end{tikzpicture}
%\endinput}
%}
%\vspace{0.2in}
%\caption{RNN architecture for Q\ref{q:reg:deep:rnn_arch}.}
%\label{fig:reg:deep:q_rnn_arch}
%\end{figure}
%
%\item \label{q:reg:deep:rnn_arch} Consider the RNN architecture in
%    \cref{fig:reg:deep:q_rnn_arch}.
%    Note that an edge labeled $-1$ indicates dependence on the
%    previous time step, whereas $-2$ indicates dependence on values from two time
%    steps back. Use $f^o$ and $f^h$ as the activation functions for
%    output and hidden layers. Let $\tau$ be the longest sequence length.
%    Answer the following questions:
%    \begin{sexercises}
%        \item Unfold the RNN for three steps and show all the
%            connections. 
%
%        \item List all the parameters required in this model, including
%            the weight matrices and bias vectors.
%            Next write the forward propagation equations to compute
%            $\bo_t$.
%        
%        \item Write the equation for the net gradient vector $\bdelta^o_t$ at the output
%            neurons at time $t$.
%        
%        \item Write the equation for the net gradient vector $\bdelta^h_t$ at the
%            hidden neurons at time $t$.    
%            
%    \end{sexercises}
%
%
%
%\item \label{Q:rnn_forget} Derive the net gradient equations for backpropagation in an RNN
%    with a forget gate using vector derivatives. That is, derive equations 
%    for the net gradients at the output $\bdelta^o_t$, update
%    $\bdelta^u_t$, forget $\bdelta^\phi_t$, and hidden $\bdelta^h_t$
%    layers by computing the derivative of the error function
%    $\cE_{\bx_t}$
%    with respect to $\bnet^o_t$, $\bnet^u_t$, $\bnet^\phi_t$ and
%    $\bnet^h_t$,
%    the net inputs at the output, update, forget and hidden layers,
%    respectively, at time $t$.
%
%\item Derive the net gradient equations $\bdelta^a_t$ for backpropagation in an LSTM
%    using vector derivatives of the error function $\cE_\bx$ with
%    respect to $\bnet^{\;a}_t$, where $a \in \{o, h, c, u, \kappa, \phi,
%    \omega\}$. That is, for the output, hidden state, internal memory,
%    and candidate update layers,
%    as well as for the input, forget and output gate layers.
%
%
%\item Show that with stride $s$, in the convolution of $\bX \in
%    \setR^{n \times n \times m}$ and $\bW \in \setR^{k\times k\times m}$,
%    there are $(t+1) \times (t+1)$ possible windows, where
%    $t = \lB\lfloor \frac{n-k}{s}\rB\rfloor$.
%
%
%\item Consider a CNN that takes as input substrings of
%    length 8 over the alphabet $\Sigma = \{A, B, C, D\}$, 
%    and predicts whether the class is positive ($P$) or negative ($N$). 
%    Assume we use two 1D filters with window size $k=4$ and
%    stride $s=2$, with weights $\bw_1 =
%    (1,0,1,1)^T$ and $\bw_2 = (1,1,0,1)^T$, respectively. Assume bias is
%    zero. The 1D convolution is followed by a max-pooling operation with
%    window size $k=2$ and stride $s=1$. 
%    All of the convolutions use linear activation function, and there is
%    no padding used.
%    Finally, all of the neurons
%    after the max-pooling feed into a single output neuron that uses a
%    sigmoid activation function with bias $-15$ and all weights set to 1.
%    Given input sequence $ABACABCD$, show the neuron values after the
%    convolution and max-pooling layer, and the final output. 
%    Use one-hot encoding for the input alphabet in lexicographic order, and
%    let $P=1$ and $N=0$. What is the final output on the given input
%    sequence.
%
%
%    \begin{figure}[!h]
%     \captionsetup[subfloat]{captionskip=10pt}
%        \centerline{
%        \subfloat[Input tensor: $\bX$]{
%            \label{fig:reg:deep:q_tensor:input}
%        \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
%            \cline{1-5}\cline{7-11}\cline{13-17}%
%            \multicolumn{5}{|c|}{channel 1} &  &
%            \multicolumn{5}{|c|}{channel 2} & &
%            \multicolumn{5}{|c|}{channel 3}\\
%            \cline{1-5}\cline{7-11}\cline{13-17}%
%            \cline{1-5}\cline{7-11}\cline{13-17}%
%            1 & 1 & 3 & 1 & 1 & &
%            1 & 4 & 3 & 3 & 2 & &
%            1 & 2 & 4 & 3 & 2\\
%            \cline{1-5}\cline{7-11}\cline{13-17}%
%            2 & 4 & 4 & 1 & 2 & &
%            2 & 2 & 2 & 2 & 2 & &
%            2 & 2 & 4 & 2 & 1\\
%            \cline{1-5}\cline{7-11}\cline{13-17}%
%            2 & 2 & 3 & 1 & 1 & &
%            1 & 1 & 4 & 1 & 1 & &
%            1 & 1 & 3 & 2 & 4\\
%            \cline{1-5}\cline{7-11}\cline{13-17}%
%            1 & 2 & 2 & 3 & 1 & & 
%            1 & 2 & 3 & 1 & 1 & &
%            1 & 4 & 3 & 2 & 3\\
%            \cline{1-5}\cline{7-11}\cline{13-17}%
%            4 & 1 & 1 & 2 & 2 & &
%            1 & 4 & 1 & 1 & 2 & &
%            1 & 1 & 1 & 4 & 4\\
%            \cline{1-5}\cline{7-11}\cline{13-17}%
%        \end{tabular}
%        }}
%        \vspace{0.2in}
%        \centerline{
%            \subfloat[3D filter ($\bW$)]{
%            \label{fig:reg:deep:q_tensor:mask}
%        \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
%            \cline{1-3}\cline{5-7}\cline{9-11}%
%            \multicolumn{3}{|c|}{channel 1} &  &
%            \multicolumn{3}{|c|}{channel 2} & &
%            \multicolumn{3}{|c|}{channel 3}\\
%            \cline{1-3}\cline{5-7}\cline{9-11}%
%            \cline{1-3}\cline{5-7}\cline{9-11}%
%            1 & 0 & 0 & & 
%            0 & 0 & 1 & &
%            1 & 1 & 1 \\
%            \cline{1-3}\cline{5-7}\cline{9-11}%
%            0 & 1 & 0 & &
%            0 & 1 & 0 & &
%            1 & 1 & 1 \\
%            \cline{1-3}\cline{5-7}\cline{9-11}%
%            0 & 0 & 1 & &
%            1 & 0 & 0 & &
%            1 & 1 & 1 \\
%            \cline{1-3}\cline{5-7}\cline{9-11}%
%        \end{tabular}
%        }}
%\vspace{0.2in}
%        \caption{3D tensor for Q\ref{q:reg:deep:tensor}.}
%        \label{fig:reg:deep:q_tensor}
%    \end{figure}
%
%
%\item \label{q:reg:deep:tensor} Given the 3D tensor in
%    \cref{fig:reg:deep:q_tensor:input}. Using
%    padding $p=1$ and stride $s=2$, answer the following questions:
%        \begin{sexercises}
%            \item Compute the convolution of $\bX$ with the $3 \times
%                3 \times 3$ mask $\bW$ in
%                \cref{fig:reg:deep:q_tensor:mask}. 
%
%            
%            \item  Compute the max-pooling output using a $3
%                \times 3 \times 1$ mask $\bW$ of all ones.  
%
%        \end{sexercises}
%
%
%\end{exercises}
