\lecture{representative}{representative}

\date[Chap 13: Rep-based Clustering]{Chapter 13: Representative-based Clustering}

\begin{frame}
\titlepage
\end{frame}


\begin{frame}{Representative-based Clustering}
Given a dataset with $n$ points in a $d$-dimensional space, $\bD =
\{\bx_i \}_{i=1}^n$, and given the number of desired clusters $k$, the
goal of representative-based clustering is to partition the dataset into
$k$ groups or clusters, which is called a {\em clustering} and is
denoted as $\cC = \{C_1, C_2, \ldots, C_k\}$. 

\medskip
For each cluster
$C_i$ there exists a representative point that summarizes the cluster, a
common choice being the mean (also called the {\em centroid})
$\bmu_i$ of all points in
the cluster, that is,
\begin{align*}
  \bmu_i = {1\over n_i} \sum_{x_{j} \in C_i} \bx_{j}
\end{align*}
where $n_i = \card{C_i}$ is the number of points in cluster $C_i$.

\medskip
A brute-force or exhaustive algorithm for f\/{i}nding a good clustering is simply to generate all possible partitions of $n$ points into $k$
clusters, evaluate some optimization score for each of them,
and retain the
clustering that yields the best score.
However, this is clearly infeasilbe, since there are
$O(k^n/k!)$ clusterings of $n$ points into $k$ groups. 
\end{frame}



\begin{frame}{K-means Algorithm: Objective}
The {\em sum of squared errors}
scoring function is def\/{i}ned as
\begin{align*}
\tcbhighmath{
  SSE(\cC) = \sum_{i=1}^{k}\sum_{\bx_{j} \in
  C_{i}}\norm{\bx_{j}-\bmu_i}^2
}
\end{align*}

\medskip
The goal is to f\/{i}nd the clustering that minimizes
the SSE score:
\begin{align*}
    \cC^* = \arg \min_{\cC} \{ SSE(\cC) \}
\end{align*}

\medskip
K-means
employs a greedy iterative approach to f\/{i}nd a clustering that minimizes
the SSE objective.
As such it can converge to a local optima
instead of a globally optimal clustering.

\end{frame}


\begin{frame}{K-means Algorithm: Objective}
K-means initializes the cluster means by randomly generating $k$
points in the data space. 
Each iteration of K-means consists of two steps: (1) cluster
assignment, and (2) centroid update. 

\medskip
Given the $k$ cluster means,
in the cluster assignment step, each point $\bx_{j} \in \bD$ is
assigned to the closest mean, which induces a clustering, with
each cluster $C_i$ comprising points that are closer to $\bmu_i$
than any other cluster mean. That is, each point $\bx_{j}$ is
assigned to cluster $C_{i^*}$, where
\begin{align*}
i^* = \arg \min_{i=1}^k \Bigl\{\norm{\bx_{j} - \bmu_i}^2\Bigr\}
\end{align*}

\medskip
Given a set of clusters $C_i$, $i=1,\dots,k$, in the centroid
update step, new mean values are computed for each
cluster from the points in $C_i$.

\medskip
The cluster assignment and centroid update
steps are
carried out iteratively until we reach a f\/{i}xed point or local
minima. 
\end{frame}


\begin{frame}[fragile]{K-Means Algorithm}
\begin{tightalgo}[H]{\textwidth-18pt}
\SetKwInOut{Algorithm}{\textsc{K-means} ($\bD, k, \epsilon$)}
\Algorithm{}
$t = 0$\;
Randomly initialize $k$ centroids: $\bmu_1^t, \bmu_2^t, \dots,
\bmu_k^t \in \setR^d$\;
\Repeat{$\sum_{i=1}^k \norm{\bmu_i^{t} - \bmu_i^{t-1}}^2 \le
\epsilon$}{
    $t \assign t+1$\;
    $C_{j} \gets \emptyset \text{ for all } j = 1, \cdots, k$\;
    \tcp{Cluster Assignment Step}
    \ForEach{$\bx_{j} \in \bD$}{
    $i^* \assign \arg \min_{i} \Bigl\{\norm{\bx_{j} - \bmu_i^t}^2
    \Bigr\}$  \tcp{Assign $\bx_{j}$ to closest centroid}
        $C_{i^*} \assign C_{i^*} \cup \{ \bx_{j}\}$\;
    }
    \tcp{Centroid Update Step}
    \ForEach{$i = 1$ to $k$}{
        $\bmu_i^{t} \assign \frac{1}{|C_i|} \sum_{\bx_{j} \in C_i} \bx_{j}$\;
    }
}
\end{tightalgo}
\end{frame}


\begin{frame}[fragile]{K-means in One Dimension}
\begin{center}
\begin{figure}
    \psset{unit=0.2in,dotscale=2.5,arrowscale=2,fillcolor=gray}
    \centerline{
    \subfloat[Initial dataset]{\label{fig:clust:rep:kexa}
    \scalebox{0.75}{%
    \pspicture[](-1,-1)(31,3)
    \psline(0,0)(31,0)
    \def\tick{\psline(0,0)(0,0.5)}%
    \multips(0,0)(1,0){32}{\tick}
    \pnode(2,0){a}
    \pnode(3,0){b}
    \pnode(4,0){c}
    \pnode(10,0){d}
    \pnode(11,0){e}
    \pnode(12,0){f}
    \pnode(20,0){g}
    \pnode(25,0){h}
    \pnode(30,0){i}
    \psset{dotstyle=Bo,fillcolor=white}
    \psdot(a)\uput[-90](a){2}
    \psdot(b)\uput[-90](b){3}
    \psdot(c)\uput[-90](c){4}
    \psdot(d)\uput[-90](d){10}
    \psdot(e)\uput[-90](e){11}
    \psdot(f)\uput[-90](f){12}
    \psdot(g)\uput[-90](g){20}
    \psdot(h)\uput[-90](h){25}
    \psdot(i)\uput[-90](i){30}
    \endpspicture
    }}}
    \centerline{
    \subfloat[Iteration: $t=1$]{\label{fig:clust:rep:kex1}
    \scalebox{0.75}{%
    \pspicture[](-1,-1)(31,3)
    \psline(0,0)(31,0)
    \def\tick{\psline(0,0)(0,0.5)}%
    \multips(0,0)(1,0){32}{\tick}
    \pnode(2,0){a}
    \pnode(3,0){b}
    \pnode(4,0){c}
    \pnode(10,0){d}
    \pnode(11,0){e}
    \pnode(12,0){f}
    \pnode(20,0){g}
    \pnode(25,0){h}
    \pnode(30,0){i}
    \psset{dotstyle=Bo}
    \psline{->}(2,2)(2,0.5)
    \uput[90](2,2){$\bmu_1=2$}
    \psdot(a)\uput[-90](a){2}
    \psdot(b)\uput[-90](b){3}
    \psset{dotstyle=Btriangle}
    \psline{->}(4,2)(4,0.5)
    \uput[45](4,2){$\bmu_2=4$}
    \psdot(c)\uput[-90](c){4}
    \psdot(d)\uput[-90](d){10}
    \psdot(e)\uput[-90](e){11}
    \psdot(f)\uput[-90](f){12}
    \psdot(g)\uput[-90](g){20}
    \psdot(h)\uput[-90](h){25}
    \psdot(i)\uput[-90](i){30}
    \endpspicture
    }}}

    \centerline{
    \subfloat[Iteration: $t=2$]{\label{fig:clust:rep:kex2}
   \scalebox{0.75}{%
     \pspicture[](-1,-1)(31,3)
    \psline(0,0)(31,0)
    \def\tick{\psline(0,0)(0,0.5)}%
    \multips(0,0)(1,0){32}{\tick}
    \pnode(2,0){a}
    \pnode(3,0){b}
    \pnode(4,0){c}
    \pnode(10,0){d}
    \pnode(11,0){e}
    \pnode(12,0){f}
    \pnode(20,0){g}
    \pnode(25,0){h}
    \pnode(30,0){i}
    \psline{->}(2.5,2)(2.5,0.5)
    \uput[90](2.5,2){$\bmu_1=2.5$}
    \psset{dotstyle=Bo}
    \psdot(a)\uput[-90](a){2}
    \psdot(b)\uput[-90](b){3}
    \psdot(c)\uput[-90](c){4}
    \psline{->}(16,2)(16,0.5)
    \uput[90](16,2){$\bmu_2=16$}
    \psset{dotstyle=Btriangle}
    \psdot(d)\uput[-90](d){10}
    \psdot(e)\uput[-90](e){11}
    \psdot(f)\uput[-90](f){12}
    \psdot(g)\uput[-90](g){20}
    \psdot(h)\uput[-90](h){25}
    \psdot(i)\uput[-90](i){30}
    \endpspicture
    }}}
\end{figure}
\end{center}
\end{frame}


\begin{frame}[fragile]{K-means in One Dimension (contd.)}
\begin{center}
\begin{figure}
    \psset{unit=0.2in,dotscale=2.5,arrowscale=2,fillcolor=gray}

    \centerline{
    \subfloat[Iteration: $t=3$]{
    \scalebox{0.75}{%
    \pspicture[](-1,-1)(31,3)
    \psline(0,0)(31,0)
    \def\tick{\psline(0,0)(0,0.5)}%
    \multips(0,0)(1,0){32}{\tick}
    \pnode(2,0){a}
    \pnode(3,0){b}
    \pnode(4,0){c}
    \pnode(10,0){d}
    \pnode(11,0){e}
    \pnode(12,0){f}
    \pnode(20,0){g}
    \pnode(25,0){h}
    \pnode(30,0){i}
    \psline{->}(3,2)(3,0.5)
    \uput[90](3,2){$\bmu_1=3$}
    \psset{dotstyle=Bo}
    \psdot(a)\uput[-90](a){2}
    \psdot(b)\uput[-90](b){3}
    \psdot(c)\uput[-90](c){4}
    \psdot(d)\uput[-90](d){10}
    \psline{->}(18,2)(18,0.5)
    \uput[90](18,2){$\bmu_2=18$}
    \psset{dotstyle=Btriangle}
    \psdot(e)\uput[-90](e){11}
    \psdot(f)\uput[-90](f){12}
    \psdot(g)\uput[-90](g){20}
    \psdot(h)\uput[-90](h){25}
    \psdot(i)\uput[-90](i){30}
    \endpspicture
    }}}

    \centerline{
    \subfloat[Iteration: $t=4$]{
    \scalebox{0.75}{%
    \pspicture[](-1,-1)(31,3)
    \psline(0,0)(31,0)
    \def\tick{\psline(0,0)(0,0.5)}%
    \multips(0,0)(1,0){32}{\tick}
    \pnode(2,0){a}
    \pnode(3,0){b}
    \pnode(4,0){c}
    \pnode(10,0){d}
    \pnode(11,0){e}
    \pnode(12,0){f}
    \pnode(20,0){g}
    \pnode(25,0){h}
    \pnode(30,0){i}
    \psline{->}(4.75,2)(4.75,0.5)
    \uput[90](4.75,2){$\bmu_1=4.75$}
    \psset{dotstyle=Bo}
    \psdot(a)\uput[-90](a){2}
    \psdot(b)\uput[-90](b){3}
    \psdot(c)\uput[-90](c){4}
    \psdot(d)\uput[-90](d){10}
    \psdot(e)\uput[-90](e){11}
    \psdot(f)\uput[-90](f){12}
    \psline{->}(19.6,2)(19.6,0.5)
    \uput[90](19.6,2){$\bmu_2=19.60$}
    \psset{dotstyle=Btriangle}
    \psdot(g)\uput[-90](g){20}
    \psdot(h)\uput[-90](h){25}
    \psdot(i)\uput[-90](i){30}
    \endpspicture
    }}}

    \centerline{
    \subfloat[Iteration: $t=5$ (converged)]{
    \scalebox{0.75}{%
    \pspicture[](-1,-1)(31,3)
    \psline(0,0)(31,0)
    \def\tick{\psline(0,0)(0,0.5)}%
    \multips(0,0)(1,0){32}{\tick}
    \pnode(2,0){a}
    \pnode(3,0){b}
    \pnode(4,0){c}
    \pnode(10,0){d}
    \pnode(11,0){e}
    \pnode(12,0){f}
    \pnode(20,0){g}
    \pnode(25,0){h}
    \pnode(30,0){i}
    \psline{->}(7,2)(7,0.5)
    \uput[90](7,2){$\bmu_1=7$}
    \psset{dotstyle=Bo}
    \psdot(a)\uput[-90](a){2}
    \psdot(b)\uput[-90](b){3}
    \psdot(c)\uput[-90](c){4}
    \psdot(d)\uput[-90](d){10}
    \psdot(e)\uput[-90](e){11}
    \psdot(f)\uput[-90](f){12}
    \psline{->}(25,2)(25,0.5)
    \uput[90](25,2){$\bmu_2=25$}
    \psset{dotstyle=Btriangle}
    \psdot(g)\uput[-90](g){20}
    \psdot(h)\uput[-90](h){25}
    \psdot(i)\uput[-90](i){30}
    \endpspicture
    }}}
\end{figure}
\end{center}
\end{frame}


\begin{frame}[fragile]{K-means in 2D: Iris Principal Components}
\setcounter{subfigure}{0}
\begin{center}
\begin{figure}
    \centering
    \captionsetup[subfloat]{captionskip=0.25in}
    \def\pshlabel#1{ {\footnotesize $#1$}}
    \def\psvlabel#1{ {\footnotesize $#1$}}
    \psset{xunit=0.5in,yunit=0.65in,dotscale=1.5,
    arrowscale=2,PointName=none}
    \psset{xAxisLabel=$\bu_1$,yAxisLabel= $\bu_2$}
    \centerline{
    \subfloat[Random initialization: $t=0$]{\label{fig:clust:rep:kex2da}
        %\pspicture[](-5,-2)(3.5,1.5)
        %\psaxes[tickstyle=bottom,Dx=1,Ox=-4,Dy=0.5,Oy=-1.5]{->}(-4,-1.5)(3.5,1.5)
        \psgraph[tickstyle=bottom,Dx=1,Ox=-4,Dy=0.5,Oy=-1.5]{->}(-4,-1.5)(3.5,1.5){4in}{2in}
        \psset{dotstyle=Bo,fillcolor=lightgray}
        \input{CLUST/representative/iris-PC-kmeans-2d-C1}
        \input{CLUST/representative/iris-PC-kmeans-2d-C2}
        \input{CLUST/representative/iris-PC-kmeans-2d-C3}
        \input{CLUST/representative/iris-PC-kmeans-2d-W1}
        \input{CLUST/representative/iris-PC-kmeans-2d-W2}
        \input{CLUST/representative/iris-PC-kmeans-2d-W3}
        \psset{fillcolor=black}
        \pstGeonode[PointSymbol=Bo, dotscale=2](-0.98,-1.24){A}
        \pstGeonode[PointSymbol=Bsquare, dotscale=2](-2.95,1.16){B}
        \pstGeonode[PointSymbol=Btriangle,dotscale=2](-1.69,-0.80){C}
        \psclip{\psframe[](-4,-1.5)(3.5,1.5)}%
        {
        \psset{linestyle=none, PointSymbol=none}
        \pstMediatorAB{A}{B}{K}{KP}
        \pstMediatorAB{C}{A}{J}{JP}
        \pstMediatorAB{B}{C}{I}{IP}
        \pstInterLL[PointSymbol=none]{I}{IP}{J}{JP}{O}
        \psset{linewidth=1pt,linestyle=dashed}
        \pstGeonode[PointSymbol=none](-4,-1.5){a}(-4,1.5){b}(3.5,1.5){c}(3.5,-1.5){d}
        \pstInterLL[PointSymbol=none]{O}{I}{a}{b}{oi}
        \pstLineAB{O}{oi}
        \pstInterLL[PointSymbol=none]{O}{J}{a}{d}{oj}
        \pstLineAB{O}{oj}
        \pstInterLL[PointSymbol=none]{O}{K}{b}{c}{ok}
        \pstLineAB{O}{ok}
        }
        \endpsclip
        \endpsgraph
        %\endpspicture
    }
    }%\hspace{0.5in}
%    \vspace{0.2in}
\end{figure}
\end{center}
\end{frame}


\begin{frame}[fragile]{K-means in 2D: Iris Principal Components}
\begin{center}
\begin{figure}
    \centerline{
    \captionsetup[subfloat]{captionskip=0.25in}
    \def\pshlabel#1{ {\footnotesize $#1$}}
    \def\psvlabel#1{ {\footnotesize $#1$}}
    \psset{xunit=0.5in,yunit=0.65in,dotscale=1.5,
    arrowscale=2,PointName=none}
    \subfloat[Iteration: $t=1$]{\label{fig:clust:rep:kex2db}
        %\pspicture[](-5,-2)(3.5,1.5)
        %\psaxes[tickstyle=bottom,Dx=1,Ox=-4,Dy=0.5,Oy=-1.5]{->}(-4,-1.5)(3.5,1.5)
        \psgraph[tickstyle=bottom,Dx=1,Ox=-4,Dy=0.5,Oy=-1.5]{->}(-4,-1.5)(3.5,1.5){4in}{2in}
        \psset{dotstyle=Bo,fillcolor=lightgray}
        \input{CLUST/representative/iris-PC-kmeans-2d-C1}
        \input{CLUST/representative/iris-PC-kmeans-2d-C2}
        \input{CLUST/representative/iris-PC-kmeans-2d-C3}
        \input{CLUST/representative/iris-PC-kmeans-2d-W1}
        \input{CLUST/representative/iris-PC-kmeans-2d-W2}
        \input{CLUST/representative/iris-PC-kmeans-2d-W3}
        \psset{fillcolor=black}
        \pstGeonode[PointSymbol=Bo, dotscale=2](1.56,-0.08){A}
        \pstGeonode[PointSymbol=Bsquare,dotscale=2](-2.86,0.53){B}
        \pstGeonode[PointSymbol=Btriangle,dotscale=2](-1.50,-0.05){C}
        \psclip{\psframe[](-4,-1.5)(3.5,1.5)}%
        {
        \psset{linestyle=none, PointSymbol=none}
        \pstMediatorAB{A}{B}{K}{KP}
        \pstMediatorAB{C}{A}{J}{JP}
        \pstMediatorAB{B}{C}{I}{IP}
        \pstInterLL[PointSymbol=none]{I}{IP}{J}{JP}{O}
        \psset{linewidth=1pt,linestyle=dashed}
        \pstGeonode[PointSymbol=none](-4,-1.5){a}(-4,1.5){b}(3.5,1.5){c}(3.5,-1.5){d}
        \pstInterLL[PointSymbol=none]{O}{I}{a}{b}{oi}
        \pstLineAB{O}{oi}
        \pstInterLL[PointSymbol=none]{O}{J}{a}{d}{oj}
        \pstLineAB{O}{oj}
        \pstInterLL[PointSymbol=none]{O}{K}{b}{c}{ok}
        \pstLineAB{O}{ok}
        }
        \endpsclip
        %\endpspicture
        \endpsgraph
    } }
%    \vspace{0.2in}
\end{figure}
\end{center}
\end{frame}


\begin{frame}[fragile]{K-means in 2D: Iris Principal Components}
\begin{center}
\begin{figure}
    \centerline{
    \captionsetup[subfloat]{captionskip=0.25in}
    \def\pshlabel#1{ {\footnotesize $#1$}}
    \def\psvlabel#1{ {\footnotesize $#1$}}
    \psset{xunit=0.5in,yunit=0.65in,dotscale=1.5,
    arrowscale=2,PointName=none}
    \subfloat[Iteration: $t=8$ (converged)]{\label{fig:clust:rep:kex2dc}
        %\pspicture[](-5,-2)(3.5,1.5)
        %\psaxes[tickstyle=bottom,Dx=1,Ox=-4,Dy=0.5,Oy=-1.5]{->}(-4,-1.5)(3.5,1.5)
        \psgraph[tickstyle=bottom,Dx=1,Ox=-4,Dy=0.5,Oy=-1.5]{->}(-4,-1.5)(3.5,1.5){4in}{2in}
        \psset{dotstyle=Bo,fillcolor=lightgray}
        \input{CLUST/representative/iris-PC-kmeans-2d-C1}
        \psset{fillcolor=white}
        \input{CLUST/representative/iris-PC-kmeans-2d-W1}
        \psset{dotstyle=Bsquare,fillcolor=lightgray}
        \input{CLUST/representative/iris-PC-kmeans-2d-C2}
        \psset{fillcolor=white}
        \input{CLUST/representative/iris-PC-kmeans-2d-W2}
        \psset{dotstyle=Btriangle,fillcolor=lightgray}
        \input{CLUST/representative/iris-PC-kmeans-2d-C3}
        \psset{fillcolor=white}
        \input{CLUST/representative/iris-PC-kmeans-2d-W3}
        \psset{fillcolor=black}
        \pstGeonode[PointSymbol=Bo, dotscale=2](2.64,0.19){A}
        \pstGeonode[PointSymbol=Bsquare,
        dotscale=2](-2.35,0.27){B}
        \pstGeonode[PointSymbol=Btriangle,
        dotscale=2](-0.66,-0.33){C}
        \psclip{\psframe[](-4,-1.5)(3.5,1.5)}%
        {
        \psset{linestyle=none, PointSymbol=none}
        \pstMediatorAB{A}{B}{K}{KP}
        \pstMediatorAB{C}{A}{J}{JP}
        \pstMediatorAB{B}{C}{I}{IP}
        \pstInterLL[PointSymbol=none]{I}{IP}{J}{JP}{O}
        \psset{linewidth=1pt,linestyle=dashed}
        \pstGeonode[PointSymbol=none](-4,-1.5){a}(-4,1.5){b}(3.5,1.5){c}(3.5,-1.5){d}
        \pstInterLL[PointSymbol=none]{O}{I}{a}{b}{oi}
        \pstLineAB{O}{oi}
        \pstInterLL[PointSymbol=none]{O}{J}{a}{d}{oj}
        \pstLineAB{O}{oj}
        \pstInterLL[PointSymbol=none]{O}{K}{b}{c}{ok}
        \pstLineAB{O}{ok}
        }
        \endpsclip
        %\endpspicture
        \endpsgraph
    }}
\end{figure}
\end{center}
\end{frame}



\begin{frame}{Kernel K-means}
In K-means, the separating boundary between clusters is
linear. Kernel K-means 
allows one to extract nonlinear
boundaries between clusters via the use of the kernel trick, i.e., we
show that all operations involve only the kernel value between a pair of
points.
%

\medskip
Let  $\bx_i \in \bD$ be mapped
to $\phi(\bx_i)$ in feature space.
Let $\bK = \bigl\{K(\bx_i,
\bx_{j})\bigr\}_{i,j=1,\dots,n}$
denote the $n \times n$ symmetric kernel matrix,
where $K(\bx_i,\bx_{j}) = \phi(\bx_i)^T\phi(\bx_{j})$.

\medskip
The cluster means in
feature space are $\{\bmu^\phi_1, \ldots, \bmu^\phi_k\}$, where
$\bmu^\phi_i = {1 \over n_i} \sum_{\bx_{j} \in C_i} \phi(\bx_{j})$.


\medskip
The sum of squared errors in feature space is
\begin{align*}
    \min_{\cC} \; SSE(\cC) =
    \sum_{i=1}^k \sum_{\bx_{j} \in C_i}
    \norm{\phi(\bx_{j}) -\bmu^\phi_i}^2
     = \sum_{j=1}^n K(\bx_{j}, \bx_{j}) -
    \sum_{i=1}^k {1\over n_i} \sum_{\bx_a \in C_i}
      \sum_{\bx_b \in C_i} K(\bx_a, \bx_b)
\end{align*}
Thus, the kernel K-means SSE objective function can be expressed purely in
terms of the kernel function.
\end{frame}

\begin{frame}{Kernel K-means: Cluster Reassignment}

Consider the distance of a point $\phi(\bx_{j})$ to
the mean $\bmu_i^\phi$ in feature space
\begin{align*}
    \norm{\phi(\bx_{j}) - \bmu^\phi_i}^2 & =
        \norm{\phi(\bx_{j})}^2 - 2 \phi(\bx_{j})^T\bmu^\phi_i +
        \norm{\bmu^\phi_i}^2\\
     & = K(\bx_{j}, \bx_{j})
     - \frac{2}{n_i} \sum_{\bx_a \in C_i} K(\bx_a, \bx_{j})
     + {1\over n_i^2} \sum_{\bx_a\in C_i} \sum_{\bx_b \in C_i}
        K(\bx_a, \bx_b)
\end{align*}

Kernel K-means assign a point to the closest cluster mean as
follows:
\begin{align*}
    C^*(\bx_{j}) & = \arg \min_{i} \lB\{
    \norm{\phi(\bx_{j}) - \bmu^\phi_i}^2  \rB\}\\
    & = 
\tcbhighmath{
\arg \min_{i} \Biggl\{
    {1\over n_i^2} \sum_{\bx_a\in C_i} \sum_{\bx_b \in C_i}
        K(\bx_a, \bx_b)
     - \frac{2}{n_i} \sum_{\bx_a \in C_i} K(\bx_a, \bx_{j})
    \Biggr\}
}
\end{align*}
\end{frame}


\newcommand{\KKmeans}{\textsc{Kernel-Kmeans}}
\begin{frame}[fragile]{Kernel-Kmeans Algorithm}
\small{
\begin{tightalgo}[H]{\textwidth-18pt}
\SetKwInOut{Algorithm}{\KKmeans ($\bK, k, \epsilon$)}
\Algorithm{}
$t \assign 0$\;
$\cC^t \assign \{ C^t_1, \ldots, C^t_k\}$\tcp{Randomly partition points into $k$ clusters}
\Repeat{$1-\frac{1}{n}\sum_{i=1}^k \lB|C^t_i \cap C^{t-1}_i\rB| \le \epsilon$}{
  $t \assign t+1$\;
  \ForEach(\tcp*[h]{Squared norm of cluster means})%
    {$C_i \in \cC^{t-1}$}{
    \nllabel{alg:clust:rep:kkmeans:sqnorm}
    $\text{sqnorm}_i \assign {1\over n_i^2} \sum_{\bx_a\in C_i} \sum_{\bx_b \in
           C_i} K(\bx_a, \bx_b)$\;
  }
  \ForEach(\tcp*[h]{Average kernel value for $\bx_{j}$ and $C_i$})%
  {$\bx_{j} \in \bD$}{
  \nllabel{alg:clust:rep:kkmeans:avg}
  \ForEach{$C_i \in \cC^{t-1}$}{
  $\text{avg}_{ji} \assign \frac{1}{n_i} \sum_{\bx_a \in C_i} K(\bx_a,
      \bx_{j})$\;
    }
  }
  \tcp{F{i}nd closest cluster for each point}
    \ForEach{$\bx_{j} \in \bD$}{
        \ForEach{$C_i \in \cC^{t-1}$}{
        $d(\bx_{j}, C_i) \assign \text{sqnorm}_i - 2\cdot \text{avg}_{ji}$\;
        }
        $j^* \assign \arg \min_{i} \bigl\{ d(\bx_{j},C_i) \bigr\}$\;
        $C^{t}_{j^*} \assign C^{t}_{j^*} \cup \{\bx_{j}\}$ \tcp{Cluster
        reassignment} \nllabel{alg:clust:rep:kkmeans:cluserassignment}
    }
    $\cC^{t} \assign \lB\{ C^{t}_1, \dots, C^{t}_k \rB\}$\;
}
\end{tightalgo}
}
\end{frame}


\begin{frame}[fragile]{K-Means or Kernel K-means with Linear Kernel}
\setcounter{subfigure}{0}
	
Using linear kernel $K(\bx_i,\bx_{\!j}) = \bx_i^T\bx_{\!j}$ is equivalent to the K-means algorithm.	

\medskip

\begin{center}
\begin{figure}
  \captionsetup[subfloat]{captionskip=0.25in}
    \def\pshlabel#1{ {\footnotesize #1}}
    \def\psvlabel#1{ {\footnotesize #1}}
    \psset{xAxisLabel=$X_1$,yAxisLabel= $X_2$}
    \psset{xunit=0.35in,yunit=0.4in,dotscale=1.5,arrowscale=2,PointName=none}
    \centerline{
    \subfloat[Linear kernel: $t=5$ iterations]{\label{fig:clust:rep:kernelsLinear}
        %\pspicture[](-0.5,1)(13,6.5)
        \scalebox{0.85}{
        \psgraph[tickstyle=bottom,Oy=1.5]{->}(0,1.5)(13,6.5){4in}{2in}
        \psset{dotstyle=Bsquare,fillcolor=lightgray}
        \input{CLUST/representative/kernel-linear-C1}
        \psset{fillcolor=white}
        \input{CLUST/representative/kernel-linear-W1}
        \psset{dotstyle=Btriangle,fillcolor=lightgray}
        \input{CLUST/representative/kernel-linear-C2}
        \psset{fillcolor=white}
        \input{CLUST/representative/kernel-linear-W2}
        \psset{dotstyle=Bo,fillcolor=lightgray}
        \input{CLUST/representative/kernel-linear-C3}
        \psset{fillcolor=white}
        \input{CLUST/representative/kernel-linear-W3}
        \psset{fillcolor=black}
        \pstGeonode[PointSymbol=Bo, dotscale=2](10.02,3.0){A}
        \pstGeonode[PointSymbol=Bsquare,
        dotscale=2](2.48,4.22){B}
        \pstGeonode[PointSymbol=Btriangle,
        dotscale=2](5.53,4.21){C}
        \psclip{
        \psframe[](0,1.5)(13,6.5)}%
        {
        \psset{linestyle=none, PointSymbol=none}
        \pstMediatorAB{A}{B}{K}{KP}
        \pstMediatorAB{C}{A}{J}{JP}
        \pstMediatorAB{B}{C}{I}{IP}
        \pstInterLL[PointSymbol=none]{I}{IP}{J}{JP}{O}
        \psset{linewidth=1pt,linestyle=dashed}
        \pstGeonode[PointSymbol=none](0,1.5){a}(0,6.5){b}(13,6.5){c}(13,1.5){d}
        \pstInterLL[PointSymbol=none]{O}{I}{b}{c}{oi}
        \pstLineAB{O}{oi}
        \pstInterLL[PointSymbol=none]{O}{J}{b}{c}{oj}
        \pstLineAB{O}{oj}
        \pstInterLL[PointSymbol=none]{O}{K}{a}{d}{ok}
        \pstLineAB{O}{ok}
        }
        \endpsclip
        \endpsgraph
	}
    }}
%    \vspace{0.2in}
\end{figure}
\end{center}
\end{frame}


\begin{frame}[fragile]{Kernel K-means: Gaussian Kernel}

Using the Gaussian kernel $K(\bx_i, \bx_{\!j}) = \exp\lB\{
-\frac{\norm{\bx_{i}-\bx_{\!j}}^2}{2\sigma^2} \rB\}$, with
${\sigma=1.5}$.

\medskip

\begin{center}
\begin{figure}
  \captionsetup[subfloat]{captionskip=0.25in}
    \centerline{
    \subfloat[Gaussian kernel: $t=4$
    Iterations]{\label{fig:clust:rep:kernelsGaussian}
        %\pspicture[](-0.5,1)(13,6.5)
        \scalebox{0.85}{
        \psgraph[tickstyle=bottom,Oy=1.5]{->}(0,1.5)(13,6.5){4in}{2in}
        \psset{dotstyle=Bsquare,fillcolor=lightgray}
        \input{CLUST/representative/kernel-gaussian1.5-C1}
        \psset{fillcolor=white}
        \input{CLUST/representative/kernel-gaussian1.5-W1}
        \psset{dotstyle=Btriangle,fillcolor=lightgray}
        \input{CLUST/representative/kernel-gaussian1.5-C2}
        \psset{fillcolor=white}
        \input{CLUST/representative/kernel-gaussian1.5-W2}
        \psset{dotstyle=Bo,fillcolor=lightgray}
        \input{CLUST/representative/kernel-gaussian1.5-C3}
        \psset{fillcolor=white}
        \input{CLUST/representative/kernel-gaussian1.5-W3}
        \psset{fillcolor=black}
        \psframe[](0,1.5)(13,6.5)
        \endpsgraph
	}
    }}
\end{figure}
\end{center}
\end{frame}

\ifdefined\wox \begin{frame} \titlepage \end{frame} \fi

\begin{frame}{Expectation-Maximization Clustering}
\framesubtitle{Gaussian Mixture Model}
\small
  Let $X_a$ denote the random variable corresponding to
the $a$th attribute. 
Let $\bX = (X_1, X_2, \dots, X_d)$ denote the vector random
variable across the $d$-attributes, with $\bx_{j}$ being a data
sample from~$\bX$.


\medskip
We assume that
each cluster $C_i$ is characterized by
a multivariate normal distribution
\begin{align*}
\tcbhighmath{
f_i(\bx) = f(\bx|\bmu_i,\cov_i)
= \frac{1}{(2\pi)^\frac{d}{2}|\cov_i|^\frac{1}{2}}
    \exp\lB\{-\frac{(\bx-\bmu_{i})^{T} \cov_i^{-1}
    (\bx-\bmu_{i})}{2}\rB\}
}
\end{align*}
where the cluster mean $\bmu_i \in \setR^d$ and
covariance matrix
$\cov_i \in \setR^{d \times d}$ are both unknown parameters.


\medskip
The probability density function of $\bX$
is given as a
{\em Gaussian mixture model} over all the $k$ clusters
\begin{align*}
\tcbhighmath{
    f(\bx) = \sum_{i=1}^k f_i(\bx) P(C_i)
    = \sum_{i=1}^k f(\bx | \bmu_i, \cov_i) P(C_i)
}
\end{align*}
where the prior probabilities $P(C_i)$ are called the
{\em mixture parameters}, which must satisfy the condition
$\sum_{i=1}^k P(C_i) = 1$.


\end{frame}




\begin{frame}{Expectation-Maximization Clustering}
\framesubtitle{Maximum Likelihood Estimation}

  \small
We write the set of all the model parameters compactly as
\begin{align*}
    \btheta = \lB\{\bmu_1,\cov_1,P(C_1) \dots, \bmu_k, \cov_k,
   P(C_k)\rB\}
\end{align*}
Given the dataset $\bD$, we def\/{i}ne the {\em likelihood} of
$\btheta$ as the conditional probability of the data $\bD$ given
the model parameters $\btheta$
\begin{align*}
P(\bD | \btheta) & = \prod_{j=1}^n f(\bx_{j})
\end{align*}

The goal of maximum likelihood estimation (MLE) is to choose the
parameters $\btheta$ that maximize the likelihood. We do this by
maximizing the log of the likelihood function
\begin{align*}
    \btheta^* = \arg\max_{\btheta} \{\ln P(\bD | \btheta)\}
\end{align*}
where the {\em log-likelihood} function is given as
\begin{align*}
    \ln P(\bD | \btheta) & = \sum_{j=1}^n \ln f(\bx_{j})
    = \sum_{j=1}^n \ln \biggl( \sum_{i=1}^k f(\bx_{j} | \bmu_i,
\cov_i)P(C_i)\biggr)
\end{align*}
\end{frame}


\begin{frame}{Expectation-Maximization Clustering}
Directly maximizing the log-likelihood over $\btheta$ is hard.
Instead, we can use the
expectation-maximization (EM)
approach for f\/{i}nding the maximum likelihood estimates
for the parameters $\btheta$.

\medskip
EM is a two-step iterative approach that
starts from an initial guess for the parameters $\btheta$.
Given the current estimates for $\btheta$, in the {\em expectation step}
EM computes the cluster posterior probabilities $P(C_i|\bx_{j})$ via the Bayes
theorem:
\begin{align*}
\tcbhighmath{
  P(C_i | \bx_{j}) = \frac{P(C_i \mbox{ and } \bx_{j})}{P(\bx_{j})} =
  \frac{P(\bx_{j} | C_i) P(C_i)}{\sum_{a=1}^k P(\bx_{j} | C_a) P(C_a)}
 = \frac{f_i(\bx_{j}) \cdot P(C_{i})}
    {\sum_{a=1}^k f_a(\bx_{j}) \cdot P(C_{a})}
}
\end{align*}

In the {\em maximization step}, using the weights
$P(C_i|\bx_{j})$ EM re-estimates $\btheta$, that is, it re-estimates the parameters $\bmu_i$, $\cov_i$,
and $P(C_i)$ for each cluster $C_i$.
The re-estimated mean is given as the weighted average of
all the points, the re-estimated covariance matrix is given as
the weighted covariance over all pairs of dimensions, and the
re-estimated prior probability for each cluster is given as the
fraction of weights that contribute to that cluster.
\end{frame}



\begin{frame}{EM in One Dimension: Expectation Step}

Let $\bD$ comprise of a single attribute $X$,
with each point $x_{j} \in \setR$ a random
sample from $X$. 
For the mixture model, we use univariate normals for each
cluster:
\begin{align*}
  f_i(x) = f(x | \mu_i, \sigma_i^2) =
  \frac{1}{\sqrt{2\pi}\sigma_i} \exp \lB\{-\frac{(x -
  \mu_i)^2}{2\sigma_i^2} \rB \}
\end{align*}
with the cluster parameters $\mu_{i}$, $\sigma_{i}^2$, and
$P(C_i)$.  


\medskip{\bf Initialization:}
For each cluster $C_{i}$, with $i =
1,2,\ldots, k$, we can randomly initialize the cluster parameters
$\mu_{i}$, $\sigma_{i}^2$, and $P(C_i)$. 

\medskip
{\bf Expectation Step:}
Given 
the mean
$\mu_i$, variance $\sigma_i^2$, and prior probability $P(C_i)$ for each
cluster, the cluster posterior probability is computed as
\begin{align*}
  w_{ij} = P(C_{i}|x_{j}) & =
    \frac{f(x_{j}|\mu_i,\sigma_i^2) \cdot P(C_{i})}
    {\sum_{a=1}^k f(x_{j} |\mu_a, \sigma_a^2) \cdot P(C_{a})}
\end{align*}
\end{frame}



\begin{frame}{EM in One Dimension: Maximization Step}

Given $w_{ij}$ values, the re-estimated cluster mean is
\begin{align*}
    \mu_{i} = \frac{\sum_{j=1}^n w_{ij} \cdot x_{j}}
    {\sum_{j=1}^n w_{ij}}
\end{align*}

\smallskip
The re-estimated value of the cluster variance is computed as the
weighted variance across all the points:
\begin{align*}
\sigma_{i}^{2} =
    \frac{\sum_{j=1}^n w_{ij}(x_{j}-\mu_{i})^2}
        {\sum_{j=1}^n w_{ij}}
\end{align*}

\smallskip
The prior probability of cluster $C_i$ is re-estimated as
\begin{align*}
P(C_{i}) & =
    \frac{\sum_{j=1}^n w_{ij}}{n}
\end{align*}
\end{frame}


\begin{frame}[fragile]{EM in One Dimension}
\begin{center}
\setcounter{subfigure}{0}
\begin{figure}
    %BASED ON EM-NEW.R
    \psset{yunit=2in,xunit=0.4in,fillstyle=solid,arrowscale=2,dotscale=2.5}
    \psset{dotstyle=Bo}
    \def\pshlabel#1{ {\small $#1$}}
    \definecolor{mygray75}{gray}{0.75}%
    \definecolor{mygray25}{gray}{0.15}%
    \centerline{ \scalebox{0.8}{
    \subfloat[Initialization: $t=0$]{\label{fig:clust:rep:em1a}
    \begin{pspicture}(-1,-0.1)(12,0.6)
    \psaxes[Dy=0.1,showorigin=true,tickstyle=bottom]{->}(0,0)(-1,0)(12,0.5)
    {
    \psset{linewidth=2pt,fillcolor=mygray25,opacity=0.5}
    \psGauss[mue=6.63, sigma=1]{0}{12}
    \psset{linewidth=2pt,fillcolor=mygray75,opacity=0.5}
    \psGauss[mue=7.57,sigma=1]{0}{12}%
    }
    \pnode(1.0,0){a}\pnode(1.3,0){b}\pnode(2.2,0){c}
    \pnode(2.6,0){d}\pnode(2.8,0){e}\pnode(5.0,0){f}
    \pnode(7.3,0){g}\pnode(7.4,0){h}\pnode(7.5,0){i}
    \pnode(7.7,0){j}\pnode(7.9,0){k}
    \psdot(a)\psdot(b)\psdot(c)\psdot(d)\psdot(e)
    \psdot(f)\psdot(g)\psdot(h)\psdot(i)\psdot(j)\psdot(k)
    \psline{->}(6.63,0.45)(6.63,0.01)
    \uput[135](6.63,0.45){\small $\mu_1=6.63$}
    %\uput[90](6.63,0.5){$\sigma_1=1$}
    \psline{->}(7.57,0.45)(7.57,0.01)
    \uput[45](7.57,0.45){\small $\mu_2=7.57$}
    %\uput[90](7.57,0.5){$\sigma_2=1$}
    \end{pspicture}
    }}}
    \centerline{\scalebox{0.8}{
    \subfloat[Iteration: $t=1$]{\label{fig:clust:rep:em1b}
    \begin{pspicture}(-2,-0.1)(12,0.65)
    \psaxes[Dy=0.1,showorigin=true,tickstyle=bottom]{->}(0,0)(-2,0)(12,0.6)
    {
    \psset{linewidth=2pt,fillcolor=mygray25,opacity=0.5}
    \psGauss[mue=3.72,sigma=2]{-2}{12}
    \psset{linewidth=2pt,fillcolor=mygray75,opacity=0.5}
    \psGauss[mue=7.40,sigma=0.83]{-1}{12}
    }
    \pnode(1.0,0){a}\pnode(1.3,0){b}\pnode(2.2,0){c}
    \pnode(2.6,0){d}\pnode(2.8,0){e}\pnode(5.0,0){f}
    \pnode(7.3,0){g}\pnode(7.4,0){h}\pnode(7.5,0){i}
    \pnode(7.7,0){j}\pnode(7.9,0){k}
    \psdot(a)\psdot(b)\psdot(c)\psdot(d)\psdot(e)\psdot(f)
    \psdot(g)\psdot(h)\psdot(i)\psdot(j)\psdot(k)
    \psline{->}(3.72,0.275)(3.72,0.01)
    \uput[90](3.72,0.275){\small $\mu_1=3.72$}
    %\uput[90](3.72,0.25){$\sigma_1=2.48$}
    \psline{->}(7.4,0.55)(7.4,0.01)
    \uput[90](7.4,0.55){\small $\mu_2=7.4$}
    %\uput[90](7.4,0.6){$\sigma_2=0.83$}
    \end{pspicture}
    }}}
\end{figure}
\end{center}
\end{frame}


\begin{frame}[fragile]{EM in One Dimension: Final Clusters}
\begin{center}
\begin{figure}
    \psset{yunit=2in,xunit=0.4in,fillstyle=solid,arrowscale=2,dotscale=2.5}
    \psset{dotstyle=Bo}
    \def\pshlabel#1{ {\small $#1$}}
    \definecolor{mygray75}{gray}{0.75}%
    \definecolor{mygray25}{gray}{0.15}%
    \centerline{\scalebox{0.8}{
    \subfloat[Iteration: $t=5$ (converged)]{\label{fig:clust:rep:em1c}
    \psset{yunit=0.75in}
    \begin{pspicture}(-1,-0.3)(12,2.2)
    \psaxes[Dy=0.3,showorigin=true,tickstyle=bottom]{->}(0,0)(-1,0)(12,2)
    {
    \psset{linewidth=2pt,fillcolor=mygray25,opacity=0.5}
    \psGauss[mue=2.49,sigma=1.3]{-1}{12}
    \psset{linewidth=2pt,fillcolor=mygray75,opacity=0.5}
    \psGauss[mue=7.56,sigma=0.22,plotpoints=400]{-1}{12}
    }
    \pnode(1.0,0){a}\pnode(1.3,0){b}\pnode(2.2,0){c}
    \pnode(2.6,0){d}\pnode(2.8,0){e}\pnode(5.0,0){f}
    \pnode(7.3,0){g}\pnode(7.4,0){h}\pnode(7.5,0){i}
    \pnode(7.7,0){j}\pnode(7.9,0){k}
    \psset{fillcolor=white}
    \psdot(a)\psdot(b)\psdot(c)\psdot(d)\psdot(e)\psdot(f)
    \psset{fillcolor=lightgray}
    \psdot(g)\psdot(h)\psdot(i)\psdot(j)\psdot(k)
    \psline{->}(2.48,0.55)(2.48,0.1)
    \uput[90](2.48,0.55){$\mu_1=2.48$}
    %\uput[90](2.48,0.5){$\sigma_1=1.3$}
    \psline{->}(7.56,1.9)(7.56,0.1)
    \uput[90](7.56,1.9){$\mu_2=7.56$}
    %\uput[90](7.56,2){$\sigma_2=0.22$}
    \end{pspicture}
    }}}
\end{figure}
\end{center}
\end{frame}


\begin{frame}{EM in $d$ Dimensions}
Each
cluster we have to reestimate the $d\times d$ covariance matrix:
\begin{align*}
\cov_i = \matr{
(\sigma^i_{1})^2 & \sigma^i_{12} & \ldots & \sigma^i_{1d}\\[1ex]
\sigma^i_{21} & (\sigma^i_{2})^2 & \ldots & \sigma^i_{2d}\\[1ex]
\vdots & \vdots & \ddots\\
\sigma^i_{d1} & \sigma^i_{d2} & \ldots & (\sigma^i_{d})^2\\
}
\end{align*}
It requires $O(d^2)$ parameters, which may be too many
for reliable estimation.
A simplification is to assume
that all dimensions are independent, which leads to a diagonal
covariance matrix:
\begin{align*}
\cov_i = \matr{
      (\sigma^i_{1})^2 & 0 & \ldots & 0\\
      0 & (\sigma^i_{2})^2 & \ldots & 0\\
      \vdots & \vdots & \ddots\\
      0 &  0 & \ldots & (\sigma^i_{d})^2\\
  }
\end{align*}
\end{frame}



\begin{frame}{EM in $d$ Dimensions}

{\bf Expectation Step:}
Given $\bmu_i$, $\cov_i$, and $P(C_i)$, the posterior probability is
given as 
\begin{align*}
  w_{ij} = P(C_i | \bx_{j}) 
 = \frac{f_i(\bx_{j}) \cdot P(C_{i})}
    {\sum_{a=1}^k f_a(\bx_{j}) \cdot P(C_{a})}
\end{align*}


\medskip
{\bf Maximization Step:}
Given the weights $w_{ij}$, in the maximization step, we
re-estimate $\cov_i$, $\bmu_i$ and $P(C_i)$.

The mean $\bmu_i$ for cluster $C_i$ can be estimated as
\begin{align*}
    \bmu_i = \frac{\sum_{j=1}^n w_{ij} \cdot \bx_{j}}
                {\sum_{j=1}^n w_{ij}}
\end{align*}

The covariance matrix $\cov_i$ is re-estimated via the
outer-product form
\begin{align*}
  \cov_i = \frac{\sum_{j=1}^n w_{ij}
                    (\bx_{j}-\bmu_{i})(\bx_{j}-\bmu_{i})^T}
                    {\sum_{j=1}^n w_{ij}}
\end{align*}

The prior probability $P(C_i)$ for each cluster is 
\begin{align*}
    P(C_i) = \frac{\sum_{j=1}^n w_{ij}}{n}
\end{align*}
\end{frame}



\begin{frame}[fragile]{Expectation-Maximization Clustering Algorithm}
\begin{tightalgo}[H]{\textwidth-18pt}
\SetKwInOut{Algorithm}{\textsc{Expectation-Maximization} ($\bD, k, \epsilon$)}
\Algorithm{}
$t \assign 0$\;
%\tcp{Initialization}
Randomly initialize $\bmu^t_1,\ldots,\bmu^t_k$\;
$\cov_i^t \assign \bI,\;\forall i=1,\dots,k$\;
%$P^t(C_i) \assign {1\over k},\;\forall i=1,\dots,k$\;
\Repeat{$\sum_{i=1}^k \norm{\bmu_i^{t} - \bmu_i^{t-1}}^2  \le \epsilon$}{
    $t \assign t+1$\;
    %\tcp{Expectation Step}
    \For{$i = 1,\dots,k$ and $j=1,\dots,n$}{
    $w_{ij}  \assign
       \frac{f(\bx_{j} | \bmu_i, \cov_i) \cdot P(C_{i})}
       {\sum_{a=1}^k f(\bx_{j} | \bmu_a, \cov_a)  \cdot
       P(C_{a})}$ \tcp{ posterior probability $P^t(C_{i}|\bx_{j})$}
    }
    %\tcp{Maximization Step}
    \For{$i=1,\dots,k$}{
        $\bmu_i^t \assign \frac{\sum_{j=1}^n w_{ij} \cdot \bx_{j}}
                {\sum_{j=1}^n w_{ij}}$ \tcp{re-estimate mean}
        $\cov^t_i \assign \frac{\sum_{j=1}^n w_{ij}
                    (\bx_{j}-\bmu_{i})(\bx_{j}-\bmu_{i})^T}
                    {\sum_{j=1}^n w_{ij}}$ \tcp{re-estimate covariance
                    matrix}
        $P^t(C_{i}) \assign\frac{\sum_{j=1}^n w_{ij}}{n}$
        \tcp{re-estimate priors}
    }
}
\end{tightalgo}
\end{frame}



\begin{frame}[fragile]{EM Clustering in 2D}
  \framesubtitle{Mixture of $\textit{k}=3$ Gaussians} 
\setcounter{subfigure}{0}
\begin{center}
\begin{figure}%
\def\mye{2.7183}
\def\mypi{3.1416}
\def\msigma#1{(sqrt(#1))}%
\def\rho#1#2#3{((#3)/(\msigma{#1}*\msigma{#2}))}%sigma1, sigma2, sigma12
\def\ccA#1#2#3{((1.0)/(2*\mypi*\msigma{#1}*\msigma{#2}*sqrt(1-\rho{#1}{#2}{#3}^
2)))} %sigma1, sigma2,rho
\def\ccB#1#2#3{((-1.0)/(2*(1-(\rho{#1}{#2}{#3})^2)))}%rho
\def\fA#1#2#3#4#5{(\ccA{#3}{#4}{#5})*\mye^(
(\ccB{#3}{#4}{#5})*(((x-(#1))/(\msigma{#3}))^2 +
((y-(#2))/(\msigma{#4}))^2 -
(2*(\rho{#3}{#4}{#5})*(x-(#1))*(y-(#2)))/(
(\msigma{#3})*(\msigma{#4})) ))}%
%
\psset{arrowscale=2} \centerline{ \subfloat[Iteration:
$t=0$]{\label{fig:clust:rep:em2a} \psset{unit=0.35in,dotscale=1.5}
\begin{pspicture}(-4,-2.5)(4,3.5)
\scalebox{0.9}{%
\psset{viewpoint=30 -60 40 rtp2xyz,Decran=50}
\psset{lightsrc=viewpoint,opacity=0.3,incolor=white}
\psset{dotstyle=Bo,fillcolor=gray}
\input{CLUST/representative/iris-PC-em-3d-C1}
\input{CLUST/representative/iris-PC-em-3d-C2}
\input{CLUST/representative/iris-PC-em-3d-C3}
\input{CLUST/representative/iris-PC-em-3d-W1}
\input{CLUST/representative/iris-PC-em-3d-W2}
\input{CLUST/representative/iris-PC-em-3d-W3}
\psPoint(-3.59,0.25,0){m1} \psPoint(-1.09,-0.46,0){m2}
\psPoint(0.75,1.07,0){m3}
\psdots[dotscale=2,dotstyle=Bsquare,fillcolor=black](m1)
\psdots[dotscale=2,dotstyle=Btriangle,fillcolor=black](m2)
\psdots[dotscale=2,dotstyle=Bo,fillcolor=black](m3)
\psset{fillcolor=white} \psSurface[ngrid=0.1 0.1, linewidth=0.1pt,
linecolor=gray, intersectionplan ={[0 0 1 -0.01] [0 0 1 -0.05] [0
0 1 -0.2] [0 0 1 -0.4]}, intersectionlinewidth =1 1 1 1,
intersectioncolor=(Black) (Black) (Black) (Black),
intersectiontype=0, transform={1 1 8
scaleOpoint3d},algebraic](-4,-1.5)(4,2){
(0.333*\fA{-3.59}{0.25}{1}{1}{0}+
0.333*\fA{-1.09}{-0.46}{1}{1}{0}+ 0.333*\fA{0.75}{1.07}{1}{1}{0})
} \axesIIID[axisnames={X_1,X_2,f(\bx)}](-4,-1.5,0)(4,2,2.25) }
\end{pspicture}
}} 
\end{figure}
\end{center}
\end{frame}



\begin{frame}[fragile]{EM Clustering in 2D}
  \framesubtitle{Mixture of $\textit{k}=3$ Gaussians} 
\begin{center}
\begin{figure}%
\def\mye{2.7183}
\def\mypi{3.1416}
\def\msigma#1{(sqrt(#1))}%
\def\rho#1#2#3{((#3)/(\msigma{#1}*\msigma{#2}))}%sigma1, sigma2, sigma12
\def\ccA#1#2#3{((1.0)/(2*\mypi*\msigma{#1}*\msigma{#2}*sqrt(1-\rho{#1}{#2}{#3}^
2)))} %sigma1, sigma2,rho
\def\ccB#1#2#3{((-1.0)/(2*(1-(\rho{#1}{#2}{#3})^2)))}%rho
\def\fA#1#2#3#4#5{(\ccA{#3}{#4}{#5})*\mye^(
(\ccB{#3}{#4}{#5})*(((x-(#1))/(\msigma{#3}))^2 +
((y-(#2))/(\msigma{#4}))^2 -
(2*(\rho{#3}{#4}{#5})*(x-(#1))*(y-(#2)))/(
(\msigma{#3})*(\msigma{#4})) ))}%
%

\centerline{ \subfloat[Iteration:
$t=1$]{\label{fig:clust:rep:em2b} \psset{unit=0.35in,dotscale=1.5}
\begin{pspicture}(-4,-2.5)(4,3.5)
\scalebox{0.9}{%
\psset{viewpoint=30 -60 40 rtp2xyz,Decran=50}
\psset{lightsrc=viewpoint,opacity=0.3,incolor=white}
\psset{dotstyle=Bo,fillcolor=gray}
\input{CLUST/representative/iris-PC-em-3d-C1}
\input{CLUST/representative/iris-PC-em-3d-C2}
\input{CLUST/representative/iris-PC-em-3d-C3}
\input{CLUST/representative/iris-PC-em-3d-W1}
\input{CLUST/representative/iris-PC-em-3d-W2}
\input{CLUST/representative/iris-PC-em-3d-W3}
\psPoint(-2.55,0.343,0){m1} \psPoint(-1.06,-0.2,0){m2}
\psPoint(2.1,0.1,0){m3}
\psdots[dotscale=2,dotstyle=Bsquare,fillcolor=black](m1)
\psdots[dotscale=2,dotstyle=Btriangle,fillcolor=black](m2)
\psdots[dotscale=2,dotstyle=Bo,fillcolor=black](m3)
\psset{fillcolor=white} \psSurface[ngrid=0.1 0.1, linewidth=0.1pt,
linecolor=gray, intersectionplan ={[0 0 1 -0.01] [0 0 1 -0.1] [0 0
1 -0.5] [0 0 1 -1.0]}, intersectionlinewidth =1 1 1 1,
intersectioncolor=(Black) (Black) (Black), intersectiontype=0,
transform={1 1 6 scaleOpoint3d},algebraic](-4,-1.5)(4,2){
(0.15*\fA{-2.55}{0.343}{0.47}{0.17}{-0.18}+
0.45*\fA{-1.06}{-0.2}{0.69}{0.17}{-0.18}+
0.4*\fA{2.1}{0.1}{1.46}{0.25}{0.14}) }
\axesIIID[axisnames={X_1,X_2,f(\bx)}](-4,-1.5,0)(4,2,2.25) }
\end{pspicture}
}} 
\end{figure}
\end{center}
\end{frame}



\begin{frame}[fragile]{EM Clustering in 2D}
  \framesubtitle{Mixture of $\textit{k}=3$ Gaussians} 
\begin{center}
\begin{figure}%
\def\mye{2.7183}
\def\mypi{3.1416}
\def\msigma#1{(sqrt(#1))}%
\def\rho#1#2#3{((#3)/(\msigma{#1}*\msigma{#2}))}%sigma1, sigma2, sigma12
\def\ccA#1#2#3{((1.0)/(2*\mypi*\msigma{#1}*\msigma{#2}*sqrt(1-\rho{#1}{#2}{#3}^
2)))} %sigma1, sigma2,rho
\def\ccB#1#2#3{((-1.0)/(2*(1-(\rho{#1}{#2}{#3})^2)))}%rho
\def\fA#1#2#3#4#5{(\ccA{#3}{#4}{#5})*\mye^(
(\ccB{#3}{#4}{#5})*(((x-(#1))/(\msigma{#3}))^2 +
((y-(#2))/(\msigma{#4}))^2 -
(2*(\rho{#3}{#4}{#5})*(x-(#1))*(y-(#2)))/(
(\msigma{#3})*(\msigma{#4})) ))}%
%


\centerline{
\subfloat[iteration: $t=36$]{
\psset{unit=0.35in,dotscale=1.5}
\begin{pspicture}(-4,-3)(4,4.5)
\scalebox{0.90}{%
\psset{viewpoint=30 -60 40 rtp2xyz,Decran=50}
\psset{lightsrc=viewpoint,opacity=0.3,incolor=white}
\psset{dotstyle=Bsquare,fillcolor=gray}
\input{CLUST/representative/iris-PC-em-3d-C1}
\psset{fillcolor=white}
\input{CLUST/representative/iris-PC-em-3d-W1}
\psset{dotstyle=Btriangle,fillcolor=gray}
\input{CLUST/representative/iris-PC-em-3d-C2}
\psset{fillcolor=white}
\input{CLUST/representative/iris-PC-em-3d-W2}
\psset{dotstyle=Bo,fillcolor=gray}
\input{CLUST/representative/iris-PC-em-3d-C3}
\psset{fillcolor=white}
\input{CLUST/representative/iris-PC-em-3d-W3}
\psPoint(-2.02,0.017,0){m1}
\psPoint(-0.51,-0.23,0){m2}
\psPoint(2.64,0.19,0){m3}
\psdots[dotscale=2,dotstyle=Bsquare,fillcolor=black](m1)
\psdots[dotscale=2,dotstyle=Btriangle,fillcolor=black](m2)
\psdots[dotscale=2,dotstyle=Bo,fillcolor=black](m3)
\psset{fillcolor=white}
\psSurface[ngrid=0.1 0.1, linewidth=0.1pt,
linecolor=gray,
intersectionplan ={[0 0 1 -0.1] [0 0 1 -0.5] [0 0 1 -1.0] [0 0 1
-1.5]},
intersectionlinewidth =1 1 1 1,
intersectioncolor=(Black) (Black) (Black) (Black),
intersectiontype=0,
transform={1 1 6 scaleOpoint3d},algebraic](-4,-1.5)(4,2){
(0.36*\fA{-2.02}{0.017}{0.564}{0.232}{-0.293}+
0.31*\fA{-0.51}{-0.23}{0.364}{0.188}{-0.218}+
0.33*\fA{2.64}{0.19}{0.048}{0.215}{-0.056})
}
\axesIIID[axisnames={X_1,X_2,f(\bx)}](-4,-1.5,0)(4,2,2.25)
\multido{\ix=-4+1}{9}{%
        \psPoint(\ix\space,-1.5,0){X1}
        \psPoint(\ix\space,-1.6,0){X2}
        \psline(X1)(X2)\uput[d](X1){\small $\ix$}}
\multido{\ny=-1+1}{4}{%
        \psPoint(-4,\ny\space,0){Y1}
        \psPoint(-4.1,\ny\space,0){Y2}
        \psline(Y1)(Y2)\uput[ul](Y1){\small $\ny$}}
}
\end{pspicture}
}}
\end{figure}
\end{center}
\end{frame}

\begin{frame}{Iris Principal Components Data}
\framesubtitle{Full vs. Diagonal Covariance Matrix}

The diagonal assumption
leads to axis parallel contours for the normal density,
contrasted with the rotated contours for the full covariance
matrix.

\medskip

The full matrix yields much better
clustering, since the full covariance matrix results in 3 wrongly clustered 
points, whereas the diagonal covariance matrix results in 25.

\setcounter{subfigure}{0}
\begin{figure}
\psset{unit=0.5in}
\psset{dotscale=1.5,fillcolor=lightgray,
        arrowscale=2,PointName=none}
\psset{xAxisLabel=$X_1$, yAxisLabel= $X_2$}
	\hspace*{-2.8cm}
\subfloat[Full covariance matrix ($t=36$)]{
\label{fig:clust:rep:iris-em-fulldiag-a}
	\scalebox{0.55}{
\begin{pspicture}(-4,-0.5)(4,4.5)
%\psaxes[tickstyle=bottom,Dx=1,Ox=-4,Dy=0.5,Oy=-1.5]{->}(-4,-1.5)(4,2)
\psgraph[tickstyle=bottom,Ox=-4,Oy=-1.5,Dy=0.5]{->}(-4.0,-1.5)(4,1.5){4in}{2in}%
\psset{dotstyle=Bsquare,fillcolor=lightgray}
\input{CLUST/representative/iris-PC-em-2d-C1}
\psset{fillcolor=white}
\input{CLUST/representative/iris-PC-em-2d-W1}
\psset{dotstyle=Btriangle,fillcolor=lightgray}
\input{CLUST/representative/iris-PC-em-2d-C2}
\psset{fillcolor=white}
\input{CLUST/representative/iris-PC-em-2d-W2}
\psset{dotstyle=Bo,fillcolor=lightgray}
\input{CLUST/representative/iris-PC-em-2d-C3}
\psset{fillcolor=white}
\input{CLUST/representative/iris-PC-em-2d-W3}
\psdot[dotscale=2,dotstyle=Bo,fillcolor=black](2.64,0.19)
\psdot[dotscale=2,dotstyle=Btriangle,fillcolor=black](-0.51,-0.23)
\psdot[dotscale=2,dotstyle=Bsquare,fillcolor=black](-2.02,0.017)
\begin{psclip}{%
\psline[linewidth=1pt](-4,-1.5)(-4,1.5)(4,1.5)(4,-1.5)(-4,-1.5)}
%> ginv(Theta$sigma[[1]])
%         [,1]    [,2]
%[1,] 30.10806 7.83927
%[2,]  7.83927 6.69827
\psset{linewidth=1pt,linestyle=dotted}
\psplotImp[algebraic](-4.5,-2)(4.5,2.5){%
    30.11*(x-2.64)^2+2*7.84*(x-2.64)*(y-0.19)+6.7*(y-0.19)^2
    + ln(0.001)}
%> ginv(Theta$sigma[[2]])
%          [,1]     [,2]
%[1,]  8.958392 10.36548
%[2,] 10.365483 17.30429
\psplotImp[algebraic](-4.5,-2)(4.5,2.5){%
    8.96*(x+0.51)^2+2*10.37*(x+0.51)*(y+0.23)+17.3*(y+0.23)^2
    + ln(0.1)}
%> ginv(Theta$sigma[[3]])
%         [,1]      [,2]
%[1,] 5.155760  6.516119
%[2,] 6.516119 12.545074
\psplotImp[algebraic](-4.5,-2)(4.5,2.5){%
    5.16*(x+2.02)^2+2*6.52*(x+2.02)*(y-0.017)+12.55*(y-0.017)^2
    + ln(0.15)}
\end{psclip}
\endpsgraph
\end{pspicture}
}
}
	\hspace*{-5.5cm}
\subfloat[Diagonal covariance matrix ($t=29$)]{
\psset{unit=0.5in}
\psset{dotscale=1.5,fillcolor=lightgray,
        arrowscale=2,PointName=none}
\psset{xAxisLabel=$X_1$, yAxisLabel= $X_2$}
\centerline{
\label{fig:clust:rep:iris-em-fulldiag-b}
\scalebox{0.55}{
\begin{pspicture}(-4,-0.5)(4,4.5)
%\psaxes[tickstyle=bottom,Dx=1,Ox=-4,Dy=0.5,Oy=-1.5]{->}(-4,-1.5)(4,2)
\psgraph[tickstyle=bottom,Ox=-4,Oy=-1.5,Dy=0.5]{->}(-4.0,-1.5)(4,1.5){4in}{2in}%
\psset{dotstyle=Bsquare,fillcolor=lightgray}
\input{CLUST/representative/iris-PC-em-2d-diag-C1}
\psset{fillcolor=white}
\input{CLUST/representative/iris-PC-em-2d-diag-W1}
\psset{dotstyle=Bo,fillcolor=lightgray}
\input{CLUST/representative/iris-PC-em-2d-diag-C2}
\psset{fillcolor=white}
\input{CLUST/representative/iris-PC-em-2d-diag-W2}
\psset{dotstyle=Btriangle,fillcolor=lightgray}
\input{CLUST/representative/iris-PC-em-2d-diag-C3}
\psset{fillcolor=white}
\input{CLUST/representative/iris-PC-em-2d-diag-W3}
\psdot[dotscale=2,dotstyle=Bsquare,fillcolor=black](-2.1,0.28)
\psdot[dotscale=2,dotstyle=Bo,fillcolor=black](2.64,0.19)
\psdot[dotscale=2,dotstyle=Btriangle,fillcolor=black](-0.674,-0.404)
\begin{psclip}{%
\psline[linewidth=1pt](-4,-1.5)(-4,1.5)(4,1.5)(4,-1.5)(-4,-1.5)}
\psset{linewidth=1pt,linestyle=dotted}
\psplotImp[algebraic](-4.5,-2)(4.5,2.5){%
    1.68*(x+2.1)^2+8.93*(y-0.28)^2 + ln(0.15)}
\psplotImp[algebraic](-4.5,-2)(4.5,2.5){%
    20.93*(x-2.64)^2+4.66*(y-0.19)^2 + ln(0.001)}
\psplotImp[algebraic](-4.5,-2)(4.5,2.5){%
    2.04*(x+0.67)^2+9.04*(y+0.404)^2+ ln(0.15)}
\end{psclip}
\endpsgraph
\end{pspicture}
}
}}
\end{figure}

\end{frame}

\begin{frame}[fragile]{K-means as Specialization of EM}

K-means can be considered as a special case of the EM algorithm, as follows:
\begin{align*}
P(\bx_{\!j}|C_{i}) =
\begin{cases}
  1 & \text{if } C_i = \displaystyle \arg\min_{C_a} \lB\{
    \norm{\bx_{\!j} - \bmu_a}^2\rB\}\\
  0 & \mbox{otherwise}
\end{cases}
\end{align*}

The posterior probability $P(C_i|\bx_{\!j})$ is given as
\begin{align*}
P(C_i | \bx_{\!j}) = {P(\bx_{\!j}|C_i)P(C_i) \over
\sum_{a=1}^k P(\bx_{\!j}|C_a)P(C_a)}
\end{align*}
If $P(\bx_{\!j}|C_i) = 0$, then $P(C_i|\bx_{\!j}) = 0$.
Otherwise, if $P(\bx_{\!j}|C_i) = 1$, then $P(\bx_{\!j} | C_a)=0$ 
$ \forall a\ne i$, and thus
$P(C_i|\bx_{\!j}) = {1 \cdot P(C_i) \over 1 \cdot P(C_i)} =
1$. 
\begin{align*}
\tcbhighmath{
    P(C_i | \bx_{\!j}) =
    \begin{cases}
        1 & \text{if } \bx_{\!j} \in C_i, \text{i.e., if }
        C_i = \displaystyle \arg\min_{C_a} \lB\{ \norm{\bx_{\!j} - \bmu_a}^2\rB\}\\
        0 & \text{otherwise}
    \end{cases}
}
\end{align*}
The parameters are $\bmu_i$ and $P(C_i)$ and the covariance matrix is not used.

\end{frame}
