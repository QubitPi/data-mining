\lecture{hierarchical}{hierarchical}

\date{Chapter 14: Hierarchical Clustering}

\begin{frame}
\titlepage
\end{frame}



\begin{frame}{Hierarchical Clustering}

  The goal of hierarchical clustering is to create a sequence of nested
  partitions, which can be conveniently visualized via a tree or
  \hbox{hierarchy} of clusters, also called the cluster {\em
  dendrogram}.  

  \bigskip
The clusters in the hierarchy range from the f\/{i}ne-grained to the
coarse-grained -- the lowest level of the tree (the leaves) consists of
each point in its own cluster, whereas the highest level (the root)
consists of all points in one cluster.  

\bigskip
Agglomerative hierarchical clustering methods work 
in a bottom-up manner. Starting
with each of the $n$ points in a separate cluster, they repeatedly merge
the most similar pair of clusters until all points are members of the
same cluster.  
\end{frame}


\begin{frame}{Hierarchical Clustering: Nested Partitions}
Given a dataset $\bD = \{\bx_1, \ldots, \bx_n\}$,
where $\bx_i \in \setR^d$, a clustering $\cC = \{C_1, \ldots,
C_k\}$ is a partition of $\bD$.

\bigskip A
clustering $\cA = \{A_1, \ldots, A_r\}$ is said to be nested in another
clustering $\cB = \{B_1, \ldots, B_s\}$ if and only if $r > s$, and for
each cluster $A_i \in \cA$, there exists a cluster $B_{j} \in \cB$, such
that $A_i \subseteq B_{j}$. 

\bigskip
Hierarchical clustering yields a sequence of
$n$ nested partitions $\cC_1, \ldots, \cC_n$. 
The clustering $\cC_{t-1}$ is nested in the clustering $\cC_t$.
The cluster dendrogram is a rooted binary tree that captures this nesting
structure, with edges between cluster $C_i \in \cC_{t-1}$ and cluster
$C_{j} \in \cC_{t}$ if $C_i$ is nested in $C_{j}$, that is, if $C_i \subset
C_{j}$. 
\end{frame}


\begin{frame}{Hierarchical Clustering Dendrogram}
  \begin{columns}

	\column{0.5\textwidth}
\begin{figure}
  \centering
  \scalebox{0.8}{
  \psset{unit=1in,radius=0.05}
  \def\dedge{\ncline[linestyle=dotted,dotsep=2pt]}
  \psset{treesep=0.4,levelsep=0.7}
  %\pstree[thislevelsep=0.5]{\TC*}{
  \pstree[]{\TC[edge=\dedge]~[tnpos=r]{$\mathit{ABCDE}$}}{
        \pstree[]{\TC~[tnpos=r]{$\mathit{ABCD}$}}{
            \pstree[]{\TC~[tnpos=r]{$\mathit{AB}$}}{
                \Tcircle{$A$}
                \Tcircle{$B$}
            }
            \pstree[]{\TC~[tnpos=r]{$\mathit{CD}$}}{
                \Tcircle{$C$}
                \Tcircle{$D$}
            }
        }
        \pstree[thislevelsep=1.4]{\Tn}{
            \Tcircle{$E$}
        }
		}}
\end{figure}

	\column{0.5\textwidth}
The
dendrogram represents the following sequence of nested partitions:
\begin{center}
  \scalebox{0.9}{
{\renewcommand{\arraystretch}{1.1}\begin{tabular}{|c|c|}
    \hline
    {\bf Clustering} & {\bf Clusters}\\
    \hline
    $\cC_1$  & $\{\mathit{A}\}, \{\mathit{B}\}, \{\mathit{C}\}, \{\mathit{D}\}, \{\mathit{E}\}$\\
    $\cC_2$  & $\{\mathit{AB}\}, \{\mathit{C}\}, \{\mathit{D}\}, \{\mathit{E}\}$\\
    $\cC_3$  & $\{\mathit{AB}\}, \{\mathit{CD}\}, \{\mathit{E}\}$\\
    $\cC_4$  & $\{\mathit{ABCD}\}, \{\mathit{E}\}$\\
    $\cC_5$ & $\{\mathit{ABCDE}\}$\\
    \hline
  \end{tabular}}}
\end{center}
with $\cC_{t-1} \subset \cC_t$ for $t=2,\ldots,5$. We assume that
$A$ and $B$ are merged before $C$ and $D$.
\end{columns}

\end{frame}


\begin{frame}[fragile]{Number of Hierarchical Clusterings}
The total number of different dendrograms with $n$ leaves is
given as:
\begin{align*}
    \prod_{m=1}^{n-1} (2m-1) = 1 \times 3 \times 5 \times 7 \times \cdots
    \times (2n-3) = (2n-3)!!
\end{align*}

\begin{figure}
  \centering
  \scalebox{0.9}{
  \def\dedge{\ncline[linestyle=dotted,dotsep=2pt]}
  \psset{unit=1in, radius=0.05}
  \psset{treesep=0.4,levelsep=0.7}
  \centerline{
  \scalebox{0.9}{%
  \subfloat[$n=1$]{
  \framebox{
    \begin{tabular}{c}
    \begin{minipage}{0.3in}
    \pstree[thislevelsep=0.5]{\TC*[linecolor=gray]}{
      \Tcircle[edge=\dedge]{$1$}\ncput{\psdot(0,0)}
    }
    \end{minipage}
  \end{tabular}
  }}
  \subfloat[$n=2$]{
  \framebox{
    \begin{tabular}{c}
    \begin{minipage}{0.8in}
    \pstree[thislevelsep=0.5]{\TC*[linecolor=gray]}{
      \pstree{\TC[edge=\dedge]\ncput{\psdot(0,0)}}{
      \Tcircle{$1$}\ncput{\psdot(0,0)}
      \Tcircle{$2$}\ncput{\psdot(0,0)}
      }}
    \end{minipage}
  \end{tabular}
  }}
  \subfloat[$n=3$]{
  \framebox{
   \begin{tabular}{ccc}
    \begin{minipage}{1.15in}
    \pstree[thislevelsep=0.5]{\TC*[linecolor=gray]}{
      \pstree{\TC[edge=\dedge]\ncput{\psdot(0,0)}}{
      \pstree{\TC\ncput{\psdot(0,0)}}{
        \Tcircle{$1$}\ncput{\psdot(0,0)}
        \Tcircle{$3$}\ncput{\psdot(0,0)}
      }
      \Tcircle{$2$}\ncput{\psdot(0,0)}
      }}
    \end{minipage}
  %}
  %\subfloat[$m=3$]{
    \begin{minipage}{1.15in}
    \pstree[thislevelsep=0.5]{\TC*[linecolor=gray]}{
      \pstree{\TC[edge=\dedge]\ncput{\psdot(0,0)}}{
      \Tcircle{$1$}\ncput{\psdot(0,0)}
      \pstree{\TC\ncput{\psdot(0,0)}}{
        \Tcircle{$2$}\ncput{\psdot(0,0)}
        \Tcircle{$3$}\ncput{\psdot(0,0)}
      }
      }
    }
    \end{minipage}
  %}
  %\subfloat[$m=3$]{
    \begin{minipage}{1.15in}
    \pstree[]{\TC*[linecolor=gray]}{
      \pstree{\TC[edge=\dedge]\ncput{\psdot(0,0)}}{
        \pstree{\TC\ncput{\psdot(0,0)}}{
        \Tcircle{$1$}\ncput{\psdot(0,0)}
        \Tcircle{$2$}\ncput{\psdot(0,0)}
        }
        \Tcircle{$3$}\ncput{\psdot(0,0)}
      }
    }
    \end{minipage}
  \end{tabular}
  }}
  }}}
\end{figure}
\end{frame}



\begin{frame}{Agglomerative Hierarchical Clustering}

In agglomerative hierarchical clustering, we begin with each of the $n$ points in a separate cluster. We repeatedly merge the two closest clusters until all
points are members of the same cluster.

\medskip
Given a set of clusters $\cC = \{C_{1}, C_{2},..,C_{m}\}$,
we f\/{i}nd the \emph{closest} pair of
clusters $C_{i}$ and $C_{j}$ and merge them into a new cluster
$C_{ij}=C_{i} \cup C_{j}$. 

\medskip
Next, we update the set
of clusters by removing $C_i$ and $C_{j}$ and adding $C_{ij}$,
as follows $\cC =
\bigl(\cC \setminus \{C_{i}, C_{j}\} \bigr)
\cup \{C_{ij}\}$.

\medskip
We repeat the process until $\cC$ contains only one cluster.
If specif\/{i}ed, we can stop the merging process when there are exactly
$k$ clusters
remaining.
\end{frame}


\newcommand{\agglomerative}{\textsc{AgglomerativeClustering}}
\begin{frame}[fragile]{Agglomerative Hierarchical Clustering Algorithm}
\begin{tightalgo}[H]{\textwidth-18pt}
\SetKwInOut{Algorithm}{\agglomerative ($\bD, k$)}
\Algorithm{}
$\cC \assign \{ C_i = \{\bx_i\} \mid \bx_i \in \bD \}$ \tcp{Each point in
separate cluster}
${\bm \Delta} \assign \{\dist(\bx_i, \bx_{j})\!: \; \bx_i, \bx_{j} \in
\bD \}$ \tcp{Compute distance matrix}
\Repeat{$\card{\cC} = k$}{
  F{i}nd the closest pair of clusters $C_i, C_{j} \in \cC$\;
  $C_{ij} \assign C_i \cup C_{j}$ \tcp{Merge the clusters}
  $\cC \gets \bigl(\cC \setminus \{C_{i}, C_{j}\} \bigr) \cup \{C_{ij}\}$ \tcp{Update the
  clustering}
  Update distance matrix $\bm \Delta$ to reflect new clustering\;
}
\end{tightalgo}
\end{frame}


\begin{frame}{Distance between Clusters}
\framesubtitle{Single, Complete and Average}

A typical distance between two points is 
%typically computed using 
the Euclidean distance or $L_2\text{-}norm$
\begin{align*}
  \norm{\bx - \by}_2 = \Bigl(\sum_{i=1}^d (x_i -
y_i)^2\Bigr)^{1/2}
\end{align*}
%The between-cluster distances are obtained as follows.

\medskip
{\bf Single Link:}
The minimum distance between a point in
$C_i$ and a point in $C_{j}$
\begin{align*}
\tcbhighmath{
  \dist(C_{i},C_{j}) = \min \{\norm{\bx - \by} \mid \bx \in C_{i},
  \by \in C_{j}\}
}
\end{align*}

\medskip
{\bf Complete Link:}
The maximum distance between points
in the two clusters:
\begin{align*}
\tcbhighmath{
  \dist(C_{i},C_{j}) = \max \{\norm{\bx-\by} \mid \bx \in C_{i},
  \by \in C_{j} \}
}
\end{align*}

{\bf Group Average:}
The average pairwise distance between points in
$C_i$ and $C_{j}$:
\begin{align*}
\tcbhighmath{
  \dist(C_{i},C_{j}) = \frac{\sum_{\bx \in C_{i}} \sum_{\by \in
  C_{j}}
  \norm{\bx-\by}}{n_i \cdot n_{j}}
}
\end{align*}
\end{frame}


\begin{frame}{Distance between Clusters: Mean and Ward's}

{\bf Mean Distance:}
The distance between two clusters
is def\/{i}ned as the distance between the means or
centroids of the two clusters:
\begin{align*}
\tcbhighmath{
\dist(C_{i},C_{j}) = \norm{\bmu_i-\bmu_{j}}
}
\end{align*}

\medskip
{\bf Minimum Variance or Ward's Method:}
The distance between two clusters is
def\/{i}ned as the increase in the sum of squared errors (SSE) when the two
clusters are merged, where the SSE for a given cluster $C_i$ is given~as
\begin{align*}
    \dist(C_{i},C_{j}) =
    \Delta SSE_{ij} = SSE_{ij} - SSE_i - SSE_{j}
\end{align*}
where $SSE_i = \sum_{\bx \in C_i} \norm{\bx - \bmu_i}^2$. After
simplification, we get:
\begin{empheq}[box=\tcbhighmath]{align*}
\begin{split}
    \dist(C_{i},C_{j}) 
    & = \lB(n_i n_{j} \over n_i + n_{j}\rB)
    \norm{\bmu_i-\bmu_{j}}^2
\end{split}
\end{empheq}
Ward's measure is therefore a weighted version of
the mean distance measure.
\end{frame}


\begin{frame}[fragile]{Single Link Agglomerative Clustering}
\begin{figure}
\begin{center}
\scalebox{0.7}{
  \psset{unit=1in}
  \psmatrix[colsep=0.25,rowsep=0.3]
  & & & & \TC[name=ABCDE,radius=0.05,tnpos=r]~{$\mathit{ABCDE}$}\\
  \TR{%
    % \psframebox{
    \begin{tabular}{|c||c|}
      \hline
      $\dist$ & $\mathit{E}$\\
      \hline\hline
      $\mathit{ABCD}$ & \Tcircle{3}\\
      \hline
    \end{tabular}}%}%
  & & \TC[name=ABCD,radius=0.05,tnpos=l]~{$\mathit{ABCD}$}\\
  \TR{%
    % \psframebox{
    \begin{tabular}{|c||c|c|}
      \hline
      $\dist$ & $\mathit{CD}$ & $\mathit{E}$\\
      \hline\hline
      $\mathit{AB}$ & \Tcircle{2} & 3\\
      \hline
      $\mathit{CD}$ & & 3\\
      \hline
    \end{tabular}}%}%
    & & & \TC[name=CD,radius=0.05,tnpos=r]~{$\mathit{CD}$}\psspan{2}\\
  \TR{%
    % \psframebox{
    \begin{tabular}{|c||c|c|c|}
      \hline
      $\dist$ & $\mathit{C}$ & $\mathit{D}$ & $\mathit{E}$\\
      \hline\hline
      $\mathit{AB}$ & 3 & 2 & 3\\
      \hline
      $\mathit{C}$ & & \Tcircle{1} & 3\\
      \hline
      $\mathit{D}$ & & & 5\\
      \hline
    \end{tabular}}%}%
   & \TC[name=AB,radius=0.05,tnpos=r]~{$\mathit{AB}$}\psspan{2}\\
  \TR{%
    % \psframebox{
    \begin{tabular}{|c||c|c|c|c|}
      \hline
     $\dist$ & $\mathit{B}$ & $\mathit{C}$ & ${D}$ & $\mathit{E}$\\
      \hline\hline
      $\mathit{A}$ & \Tcircle{1} & 3 & 2 & 4\\
      \hline
      $\mathit{B}$ & & 3 & 2 & 3\\
      \hline
      $\mathit{C}$ & & & 1 & 3\\
      \hline
      $\mathit{D}$ & & & & 5\\
      \hline
    \end{tabular}}%}%
  & \Tcircle[name=A]{$\mathit{A}$} & \Tcircle[name=B]{$\mathit{B}$} & \Tcircle[name=C]{$\mathit{C}$}
  & \Tcircle[name=D]{$\mathit{D}$} & \Tcircle[name=E]{$\mathit{E}$}\\
  \endpsmatrix
  \ncline{A}{AB}\trput{1}
  \ncline{B}{AB}\trput{1}
  \ncline{C}{CD}\trput{1}
  \ncline{D}{CD}\trput{1}
  \ncline{AB}{ABCD}\trput{2}
  \ncline{CD}{ABCD}\trput{2}
  \ncline{ABCD}{ABCDE}\trput{3}
  \ncline{E}{ABCDE}\trput{3}
}
\end{center}
\end{figure}
\end{frame}


\begin{frame}{Lance--Williams Formula}
Whenever two clusters $C_i$ and $C_{j}$ are merged into $C_{ij}$,
we need to update the distance matrix by recomputing the
distances from the newly created cluster $C_{ij}$ to all other
clusters $C_r$ ($r \ne i$ and $r \ne j$).

\medskip
The Lance--Williams formula provides a general equation to
recompute the distances for all of the cluster
proximity measures
\begin{align*}
    \dist(C_{ij},C_r) & =
        \alpha_i \cdot \dist(C_i,C_r) +
        \alpha_{j} \cdot \dist(C_{j},C_r)\; +\nonumber\\
        & \qquad \qquad \beta \cdot \dist(C_i,C_{j}) +
        \gamma \cdot \bigl|\dist(C_i,C_r)-\dist(C_{j},C_r)\bigr|
\end{align*}
The coeff\/{i}cients $\alpha_i, \alpha_{j}, \beta,$ {and} $\gamma$ differ from
one measure to another.
\end{frame}


\begin{frame}{Lance--Williams Formulas for Cluster Proximity}
\renewcommand{\arraystretch}{1.3}
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
Measure & $\alpha_i$ & $\alpha_{j}$ & $\beta$ & $\gamma$ \\ \hline \hline
Single link   &  $\frac{1}{2}$ & $\frac{1}{2}$ & 0 & $-\frac{1}{2}$
\\ \hline
Complete link &  $\frac{1}{2}$ & $\frac{1}{2}$ & 0 & $\frac{1}{2}$
\\ \hline
Group average &  $\frac{n_i}{n_i+n_{j}}$ & $\frac{n_{j}}{n_i+n_{j}}$ &0&0
\\ \hline
Mean distance & $\frac{n_i}{n_i+n_{j}}$ &
$\frac{n_{j}}{n_i+n_{j}}$ &  $\frac{-n_i \cdot n_{j}}{(n_i+n_{j})^2}$ & 0
\\
\hline
Ward's measure        & $\frac{n_i+n_r}{n_i+n_{j}+n_r}$ &
$\frac{n_{j}+n_r}{n_i+n_{j}+n_r}$ &  $\frac{-n_r}{n_i+n_{j}+n_r}$ & 0 \\[2ex]
\hline
\end{tabular}
\end{center}
\end{frame}

\begin{frame}{Lance--Williams Formulas for Cluster Proximity}

\textbf{Single link:} Arithmetical trick to find the minimum.

\begin{align*}
    \dist(C_{ij},C_r) & =
	\frac{\dist(C_i,C_r)}{2} +
	\frac{\dist(C_{j},C_r)}{2} -
	\frac{\bigl|\dist(C_i,C_r)-\dist(C_{j},C_r)\bigr|}{2}
\end{align*}

\textbf{Complete link:} Arithmetical trick to find the maximum.

\begin{align*}
    \dist(C_{ij},C_r) & =
	\frac{\dist(C_i,C_r)}{2} +
	\frac{\dist(C_{j},C_r)}{2} +
	\frac{\bigl|\dist(C_i,C_r)-\dist(C_{j},C_r)\bigr|}{2}
\end{align*}

\textbf{Group average:} Weight the distance by the cluster size.

\begin{align*}
    \dist(C_{ij},C_r) & =
	\frac{n_i}{n_i+n_j} \cdot \dist(C_i,C_r) +
	\frac{n_j}{n_i+n_j} \cdot \dist(C_j,C_r) 
\end{align*}

\end{frame}

\begin{frame}{Lance--Williams Formulas for Cluster Proximity}

\textbf{Mean distance:} The new centroid is in the line defined by $\mu_i$ and $\mu_j$, and its distance to $\mu_r$ has to be adjusted by $\frac{n_i \cdot n_j}{(n_i+n_j)^2}$.

\begin{align*}
    \dist(C_{ij},C_r) & =
	\frac{n_i}{n_i+n_j} \cdot \dist(C_i,C_r) +
	\frac{n_j}{n_i+n_j} \cdot \dist(C_j,C_r) +
	\frac{-n_i \cdot n_j}{(n_i+n_j)^2} \cdot \dist(C_i,C_{j}) 
\end{align*}

\textbf{Ward's measure:} The $\Delta SSE$ of the new cluster is a weigthed sum of the $\Delta SSEs$ of the original clusters, adjusted by the fact that $n_r$ was considered twice.

\begin{align*}
    \dist(C_{ij},C_r) & =
	\frac{n_i+n_r}{n_i+n_j+n_r} \cdot \dist(C_i,C_r) +
	\frac{n_j+n_r}{n_i+n_j+n_r} \cdot \dist(C_j,C_r) +
	\frac{-n_r}{n_i+n_j+n_r} \cdot \dist(C_i,C_{j}) 
\end{align*}

\end{frame}

\readdata{\dataC}{CLUST/hierarchical/figs/iris-PC-complete-link.txt}
\begin{frame}{Iris Dataset: Complete Link Clustering}
\begin{center}
\begin{figure}
%\begin{figure}[!t]
%\vspace{0.1in}
\centerline{
\scalebox{0.7}{
\psset{dotscale=1.5,fillcolor=lightgray,
      arrowscale=2,PointName=none}
\psset{xAxisLabel=$\bu_1$, yAxisLabel= $\bu_2$}
\psgraph[tickstyle=bottom,Ox=-4,Oy=-1.5,Dx=1,Dy=0.5]{->}%
  (-4.0,-1.5)(4.0,1.5){3.5in}{2.25in}%
%original dataset
\listplot[plotstyle=dots,dotstyle=Bo,showpoints=true,
      nStart=101,nEnd=150,plotNo=1,plotNoMax=2]{\dataC}
\listplot[plotstyle=dots,dotstyle=Bsquare,showpoints=true,
          nStart=51,nEnd=100,plotNo=1,plotNoMax=2]{\dataC}
\listplot[plotstyle=dots,dotstyle=Btriangle,showpoints=true,
          nStart=1,nEnd=36,plotNo=1,plotNoMax=2]{\dataC}
%misclustered
 \psset{fillcolor=white}
\listplot[plotstyle=dots,dotstyle=Btriangle,showpoints=true,
          nStart=37,nEnd=37,plotNo=1,plotNoMax=2]{\dataC}
\listplot[plotstyle=dots,dotstyle=Bsquare,showpoints=true,
          nStart=37,nEnd=50,plotNo=1,plotNoMax=2]{\dataC}
\endpsgraph
}}
\end{figure}

Contingency Table:
\begin{tabular}{|c|c|c|c|}
        \hline
         & {\tt iris-setosa} & {\tt iris-virginica} & {\tt
         iris-versicolor}\\
         \hline
        $C_1$ (circle) & 50 & 0 & 0\\
        $C_2$ (triangle) & 0 & 1 & 36\\
        $C_3$ (square) &0 & 49 & 14 \\
        \hline
    \end{tabular}
\end{center}
\end{frame}
