\lecture{fpmeval}{fpmeval}

\date{Chapter 12: Pattern and Rule Assessment}

\begin{frame}
\titlepage
\end{frame}

\newcommand{\lift}{\mathit{lift}}
\newcommand{\leverage}{leverage}
\newcommand{\oddsratio}{oddsratio}
\newcommand{\jaccard}{jaccard}
\newcommand{\conviction}{conv}
\newcommand{\cover}{cover}
\newcommand{\productive}{productive}

\begin{frame}{Rule Assessment Measures: Support and Confidence}

{\bf Support:}
The {\em support} of the rule \index{association rule! support} is def\/{i}ned as the
number of transactions that contain both $X$ and $Y$, that is,
\begin{align*}
\tcbhighmath{
  \supp(X \arule Y) = \supp(\mathit{XY}) = \card{\vT(\mathit{XY})}
}
\end{align*}

\medskip
The {\em relative support} is
the fraction of transactions that contain both $X$
and $Y$, that is, the empirical
joint probability of the items comprising the rule
\begin{align*}
\tcbhighmath{
  \rsupp(X\arule Y) = P(\mathit{XY}) = \rsupp(\mathit{XY}) = \frac{\supp(\mathit{XY})}{\card{\bD}}
}
\end{align*}

\bigskip
{\bf Conf\/{i}dence:}
The {\em conf\/{i}dence} of a rule
is the conditional probability that
a transaction contains the consequent $Y$ given that it contains the
antecedent $X$:
\begin{align*}
\tcbhighmath{
\conff(X \arule Y) = P(Y|X) = \frac{P(\mathit{XY})}{P(X)} =
\frac{\rsupp(\mathit{XY})}{\rsupp (X)} =
\frac{\supp(\mathit{XY})}{\supp (X)}
}
\end{align*}
\end{frame}



\begin{frame}[fragile]{Example Dataset: Support and Confidence}
\begin{columns}
  \column{0.3\textwidth}
\begin{center}
\begin{tabular}{|c|l|}
\hline
Tid & Items \\ \hline
1 & {\it ABDE}\\ \hline
2 & {\it BCE}\\ \hline
3 & {\it ABDE}\\ \hline
4 & {\it ABCE}\\ \hline
5 & {\it ABCDE}\\ \hline
6 & {\it BCD}\\ \hline
\end{tabular}
\end{center}
  \column{0.7\textwidth}
\begin{center}
  \scalebox{0.9}{
\begin{tabular}{|c|c|c|}
  \multicolumn{3}{c}{Frequent itemsets: $\minsup=3$}\\
\hline
$\supp$ & $\rsupp$ & Itemsets \\ \hline
3 & 0.5 & $\mathit{ABD}$, $\mathit{ABDE}$, $\mathit{AD}$,
$\mathit{ADE}$\\
& & $\mathit{BCE}$, $\mathit{BDE}$, $\mathit{CE}$, $\mathit{DE}$  
\\ \hline
4 & $0.67$ & $\mathit{A}$, $\mathit{C}$, $\mathit{D}$, $\mathit{AB}$, $\mathit{ABE}$, $\mathit{AE}$, $\mathit{BC}$, $\mathit{BD}$   \\ \hline
5 & $0.83$ & $\mathit{E}$, $\mathit{BE}$ \\ \hline
6 & $1.0$ & $\mathit{B}$ \\ \hline
\end{tabular}
}
\end{center}

\end{columns}

\begin{columns}
  \column{0.4\textwidth}
\begin{center}
  \scalebox{0.9}{
\begin{tabular}{|r>{\centering}m{0.2in}l|r|}
  \multicolumn{4}{c}{Rule confidence}\\
\hline
\multicolumn{3}{|c|}{Rule} & $\conff$ \\ \hline
$\mathit{A}$ & $\arule$ & $\mathit{E}$ & 1.00 \\ \hline
$\mathit{E}$ & $\arule$ & $\mathit{A}$ & 0.80 \\ \hline \hline
$\mathit{B}$ & $\arule$ & $\mathit{E}$ & 0.83 \\ \hline
$\mathit{E}$ & $\arule$ & $\mathit{B}$ & 1.00 \\ \hline \hline
$\mathit{E}$ & $\arule$ & $\mathit{BC}$ & 0.60 \\ \hline
$\mathit{BC}$ & $\arule$ & $\mathit{E}$ & 0.75 \\ \hline
\end{tabular}
}
\end{center}
  \column{0.6\textwidth}
	\begin{minipage}{0.7\textwidth}
        Confidence should be evaluated considering the support of the rule components. For instance, since $P(BC)=0.67$, the rule $E \arule BC$, with a 60\% confidence, has a deleterious effect on $BC$.
	\end{minipage}
\end{columns}

\end{frame}

\begin{frame}{Rule Assessment Measures: Lift, Leverage and Jaccard}

  {\bf Lift:}
Lift 
is def\/{i}ned as the ratio of the observed joint
probability of $X$ and $Y$ to the expected joint probability if they
were statistically independent, that is,
\begin{align*}
\tcbhighmath{
   \lift (X \arule Y) = \frac{P(\mathit{XY})}{P(X) \cdot P(Y)} =
   \frac{\rsupp(\mathit{XY})}{\rsupp(X)\cdot\rsupp(Y)} = \frac{\conff(X\arule
   Y)}{\rsupp(Y)}
}
\end{align*}

\medskip
{\bf Leverage:}
Leverage 
measures the difference between the
observed and expected joint probability of $XY$ assuming that $X$ and
$Y$ are independent
\begin{align*}
\tcbhighmath{
  \leverage (X \arule Y) = P(\mathit{XY}) - P(X) \cdot P(Y) = \rsupp(\mathit{XY}) -
  \rsupp(X) \cdot \rsupp(Y)
}
\end{align*}

{\bf Jaccard:}
The Jaccard coeff\/{i}cient
measures the similarity
between
two sets. When applied as a rule assessment measure it computes the
similarity between the tidsets of $X$ and $Y$:
\begin{align*}
%\tcbhighmath{
\jaccard (X \arule Y) & =
\frac{\card{\vT(X) \cap \vT(Y)}}{\card{\vT(X) \cup \vT(Y)}}\\
& = \frac{P(\mathit{XY})}{P(X)+P(Y)-P(\mathit{XY})}
%}
\end{align*}


\end{frame}

\begin{frame}{Lift and Leverage}
  \begin{columns}

	\column{0.62\textwidth}
\begin{center}
\begin{tabular}{|r>{\centering}m{0.2in}l|r|}
\hline
\multicolumn{3}{|c|}{Rule} & $\lift$ \\ \hline
$\mathit{AE}$ & $\arule$ & $\mathit{BC}$ & 0.75 \\ \hline
$\mathit{CE}$ & $\arule$ & $\mathit{AB}$ & 1.00 \\ \hline
$\mathit{BE}$ & $\arule$ & $\mathit{AC}$ & 1.20 \\ \hline
\end{tabular}
\end{center}

\column{0.5\textwidth}
\begin{minipage}{0.7\textwidth}
$lift < 1$ indicates the rule support is smaller than expected, while $lift>1$ means the reverse.
\end{minipage}

\end{columns}

\bigskip

\begin{columns}

\column{0.5\textwidth}

\begin{center}
	\scalebox{0.8}{
\begin{tabular}{|r>{\centering}m{0.2in}l|r|r|r|}
\hline
\multicolumn{3}{|c|}{Rule} & $\rsupp$ & $\lift$ & $\leverage$ \\ \hline
$\mathit{ACD}$ & $\arule$ & $\mathit{E}$ & 0.17 & 1.20 & 0.03\\ \hline
$\mathit{AC}$ & $\arule$ & $\mathit{E}$ &  0.33 & 1.20 & 0.06\\ \hline
$\mathit{AB}$ & $\arule$ & $\mathit{D}$ &  0.50 & 1.12 & 0.06\\ \hline
$\mathit{A}$ & $\arule$ & $\mathit{E}$ &   0.67 & 1.20 & 0.11\\ \hline
\end{tabular}
}
\end{center}
	
\column{0.5\textwidth}
\begin{center}
\begin{minipage}{0.7\textwidth}
Lift and leverage must be evaluated together, since the same lift may refer to significantly different leverages.
\end{minipage}
\end{center}
\end{columns}
\end{frame}


\begin{frame}{Lift, Jaccard, and Confidence}
\begin{columns}
\column{0.4\textwidth}
\begin{center}
\begin{tabular}{|r>{\centering}m{0.2in}l|r|r|r|}
\hline
\multicolumn{3}{|c|}{Rule} & $\rsupp$ & $\conff$ & $\lift$ \\ \hline
$\mathit{E}$ & $\arule$ & $\mathit{AC}$ & 0.33 & 0.40 & 1.20\\ \hline
$\mathit{E}$ & $\arule$ & $\mathit{AB}$ & 0.67 & 0.80 & 1.20\\ \hline
$\mathit{B}$ & $\arule$ & $\mathit{E}$ & 0.83 & 0.83 & 1.00\\ \hline
\end{tabular}
\end{center}

\column{0.6\textwidth}
\begin{center}
\begin{minipage}{0.7\textwidth}
Lift and confidence must be evaluated together, to avoid either weak rules
or rules where the antecedent and consequent are independent ($lift=1$).

\end{minipage}
\end{center}

\end{columns}

\bigskip
	
\begin{columns}
\column{0.4\textwidth}
\begin{center}
\begin{tabular}{|r>{\centering}m{0.2in}l|r|r|r|r|}
\hline
\multicolumn{3}{|c|}{Rule} & $\rsupp$ & $\lift$ & $\jaccard$\\ \hline
$\mathit{A}$ & $\arule$ & $\mathit{C}$ & 0.33 & 0.75 & 0.33 \\ \hline
$\mathit{A}$ & $\arule$ & $\mathit{E}$ & 0.67 & 1.20 & 0.80 \\ \hline
$\mathit{A}$ & $\arule$ & $\mathit{B}$ & 0.67 & 1.00 & 0.67 \\ \hline
\end{tabular}
\end{center}

\column{0.6\textwidth}
\begin{center}
\begin{minipage}{0.7\textwidth}
Jaccard and Lift provide similar information, but Jaccard is bounded to the interval $[0,1]$.
\end{minipage}
\end{center}

\end{columns}
\end{frame}



\begin{frame}{Contingency Table for {\it X} and {\it Y}}

We may also define the contingency table for $X$ and $Y$, and exploit their absence, represented by $\neg X$ and $\neg Y$.

\begin{center}
\begin{tabular}{|c|cc||c|}
    \hline
    & $Y$ & $\neg Y$ & \\
    \hline
  $X$ & $\supp(\mathit{XY})$ & $\supp(X\neg Y)$ & $\supp(X)$\\
  $\neg X$ & $\supp(\neg \mathit{XY})$ & $\supp(\neg X\neg Y)$ & $\supp(\neg X)$\\
  \hline\hline
  & $\supp(Y)$ & $\supp(\neg Y)$ & $\card{\bD}$\\
  \hline
  \end{tabular}%}{}
\end{center}
\end{frame}

\begin{frame}{Rule Assessment Measures: Conviction}

Def\/{i}ne $\neg X$ to be the
event that $X$ is not contained in a transaction, that is,  $X
\not\subseteq t\in \cT$, and likewise for $\neg Y$.  There are, in
general, four possible events depending on the occurrence or
non-occurrence of the itemsets $X$ and $Y$ as depicted in the
contingency table.

\medskip
Conviction 
measures the expected error of the rule, that is, how often
$X$ occurs in a transaction where $Y$ does not.
It is thus a measure of the strength of a rule with respect to the
complement of the consequent, def\/{i}ned as
\begin{align*}
%\tcbhighmath{
  \conviction(X \arule Y) &
  = \frac{P(X) \cdot P(\neg Y)}{P(X \neg Y)}
  = \frac{1}{\lift(X \arule \neg Y)} 
	= \frac{1-P(Y)}{1-conf(X \arule Y)}
%}
\end{align*}
If the joint
probability of $X\neg Y$ is less than that expected under independence
of $X$ and $\neg Y$, then
conviction is high, and vice versa. 
\end{frame}


\begin{frame}{Rule Conviction}
\begin{center}
\begin{tabular}{|r>{\centering}m{0.2in}l|r|r|r|r|}
\hline
\multicolumn{3}{|c|}{Rule}     & $\rsupp$ & $\conff$ & $\lift$    & $\conviction$\\ \hline
$\mathit{A}$   & $\arule$ & $\mathit{DE}$   & 0.50    & 0.75       & 1.50    & 2.00     \\ \hline
$\mathit{DE}$  & $\arule$ & $\mathit{A}$    & 0.50    & 1.00      & 1.50    & $\infty$ \\ \hline
$\mathit{E}$   & $\arule$ & $\mathit{C}$    & 0.50    & 0.60       & 0.90    & 0.83 \\ \hline
$\mathit{C}$   & $\arule$ & $\mathit{E}$    & 0.50    & 0.75       & 0.90    & 0.68 \\ \hline
\end{tabular}%}{}
\end{center}

$A \arule DE$ is a strong rule, confirmed by high values of both lift and conviction. 

$DE \arule A$ has 100\% confidence, being a trivial rule. 

$E \arule C$ and $C \arule E$ are weak, but, despite the same support and lift, conviction indicates that the $E \arule C$ is stronger than $C \arule E$, while confidence indicates the reverse.
\end{frame}

\begin{frame}{Rule Assessment Measures: Odds Ratio}
The odds ratio utilizes all
four entries from the contingency table.
Let
us divide the dataset into two groups of transactions -- those
that contain $X$ and those that do not contain $X$. Def\/{i}ne the
odds of $Y$ in these two groups as follows:
\begin{align*}
  odds(Y|X) &= \frac{P(\mathit{XY})/P(X)}{P(X\neg Y)/P(X)}
  =\frac{P(\mathit{XY})}{P(X\neg Y)}\\
  odds(Y|\neg X) &=
  \frac{P(\neg \mathit{XY})/P(\neg X)}{P(\neg X\neg Y)/P(\neg X)}
  =\frac{P(\neg \mathit{XY})}{P(\neg X\neg Y)}
\end{align*}

The odds ratio is then def\/{i}ned as the ratio of these two odds:
\begin{align*}
%\tcbhighmath{
  \oddsratio(X \arule Y) &= \frac{odds(Y|X)}{odds(Y|\neg X)}
   =
   \frac{P(\mathit{XY}) \cdot P(\neg  X \neg Y)}{P(X \neg Y) \cdot P(\neg \mathit{XY})}\\
   & = \frac{\supp(\mathit{XY}) \cdot \supp(\neg X\neg Y)}
    {\supp(X\neg Y)\cdot \supp(\neg \mathit{XY})}
%}
\end{align*}

If $X$ and $Y$ are
independent, then odds ratio has value 1. 
\end{frame}


\begin{frame}{Odds Ratio}
Let us compare the odds ratio for two rules, $C \arule A$ and $D
\arule A$.
The
contingency tables for $A$ and $C$, and for $A$ and $D$, are given
below:

\vspace*{8pt}
\centerline{
\begin{tabular}{ccc}
\begin{tabular}{|c|c|c|}
  \hline
  & $C$ & $\neg C$\\
  \hline
  $A$ & 2 & 2\\
$\neg A$ & 2 & 0\\
\hline
\end{tabular} & &
\begin{tabular}{|c|c|c|}
  \hline
  & $D$ & $\neg D$\\
  \hline
  $A$ & 3 & 1\\
$\neg A$ & 1 & 1\\
\hline
\end{tabular}\\
& &\\
\end{tabular}
}

\noindent The odds ratio values for the two rules are given as
\begin{align*}
   \oddsratio (C \arule A) & =
   \frac{\supp(\mathit{AC}) \cdot \supp(\neg  A \neg C)}{\supp(A \neg C) \cdot
   \supp (\neg \mathit{AC})} = \frac{2 \times 0}{2 \times 2} =
   0\\
   \oddsratio (D \arule A) & =
   \frac{\supp(\mathit{AD}) \cdot \supp(\neg  A \neg D)}{\supp(A \neg D) \cdot
   \supp(\neg \mathit{AD})} = \frac{3 \times 1}{1 \times 1} = 3
\end{align*}

Thus $D \arule A$ is stronger than $C \arule A$, which is also confirmed by lift and confidence.
\end{frame}



\begin{frame}{Example: Association Rules from Iris Data}
\framesubtitle{Discretization of Iris Data}
\begin{center}
\begin{tabular}{|l|c|l|}
\hline
Attribute & Range or value & Label \\ \hline
\multirow{3}{*}{Sepal length} & 4.30--5.55 & $sl_1$ \\
& 5.55--6.15 & $sl_2$ \\
& 6.15--7.90 & $sl_3$ \\ \hline
\multirow{3}{*}{Sepal width}  & 2.00--2.95 & $sw_1$ \\
& 2.95--3.35 & $sw_2$ \\
& 3.35--4.40 & $sw_3$ \\ \hline
\multirow{3}{*}{Petal length} & 1.00--2.45 & $pl_1$ \\
& 2.45--4.75 & $pl_2$ \\
& 4.75--6.90 & $pl_3$ \\ \hline
\multirow{4}{*}{Petal width}  & 0.10--0.80 & $pw_1$ \\
& 0.80--1.75 & $pw_2$ \\
& 1.75--2.50 & $pw_3$ \\ \hline
\multirow{3}{*}{Class} & Iris-setosa  & $c_1$ \\
& Iris-versicolor  & $c_2$ \\
& Iris-virginica  & $c_3$ \\ \hline
\end{tabular}%}{}\vspace*{-6pt}
\end{center}
\end{frame}
  

\captionsetup[subfloat]{captionskip=0.5in}
\begin{frame}[fragile]{Iris: Support vs.\ Conf\/{i}dence, and Conviction
  vs. Lift}
\framesubtitle{$minsup=10$ and $minlift=0.1$ results in 79 rules}
\begin{figure}
\centerline{
        \hspace{0.2in}
\subfloat[Support vs.\ conf\/{i}dence]{
\label{fig:fpm:fpmeval:irissuppconfclass} \scalebox{0.75}{
\readdata{\dataSLWa}{FPM/fpmeval/figs/irissuppconfclass1.dat}
\readdata{\dataSLWb}{FPM/fpmeval/figs/irissuppconfclass2.dat}
\readdata{\dataSLWc}{FPM/fpmeval/figs/irissuppconfclass3.dat}
        \psset{dotstyle=Bo,dotscale=1.5,fillcolor=lightgray,
                                arrowscale=2,PointName=none}
        \psset{xAxisLabel= $\rsupp$, yAxisLabel= $\conff$,
        xAxisLabelPos={c,-0.4in}}%, yAxisLabelPos={-0.5in,c}}
        \psgraph[tickstyle=bottom,Dx=0.1,Dy=0.25,
                 Ox=0,Oy=0,subticks=2]{->}(0.0,0.0)(0.0,0.0)(0.5,1.1){2in}{2in}%
        \dataplot[plotstyle=dots,showpoints=true]{\dataSLWa}
        \dataplot[plotstyle=dots,dotstyle=Bsquare,showpoints=true]{\dataSLWb}
        \dataplot[plotstyle=dots,dotstyle=Btriangle,showpoints=true]{\dataSLWc}
      % legend
        \psdot[showpoints=true](0.18,0.24)
        \rput[l](0.2,0.24){Iris-setosa ($c_1$)}
        \psdot[dotstyle=Bsquare,showpoints=true](0.18,0.15)
        \rput[l](0.2,0.15){Iris-versicolor ($c_2$)}
        \psdot[dotstyle=Btriangle,showpoints=true](0.18,0.06)
        \rput[l](0.2,0.06){Iris-virginica ($c_3$)}
      % best rules
      \psset{dotscale=2}
        \psdot[fillcolor=white](0.333,1.0)
        \psdot[dotstyle=Bsquare,fillcolor=white](0.327,0.91)
        \psdot[dotstyle=Btriangle,fillcolor=white](0.333,0.89)
        \endpsgraph
        }}
        \hspace{0.7in}
\subfloat[Lift vs.\ conviction]{
\label{fig:fpm:fpmeval:irisliftconvclass} \scalebox{0.75}{
\readdata{\dataSLWa}{FPM/fpmeval/figs/irisliftconvclass1.dat}
\readdata{\dataSLWb}{FPM/fpmeval/figs/irisliftconvclass2.dat}
\readdata{\dataSLWc}{FPM/fpmeval/figs/irisliftconvclass3.dat}
        \psset{dotstyle=Bo,dotscale=1.5,fillcolor=lightgray,
                                arrowscale=2,PointName=none}
        \psset{xAxisLabel=$\lift$, yAxisLabel= $\conviction$,
        xAxisLabelPos={c,-0.4in}}%, yAxisLabelPos={-0.5in,c}}
        \psgraph[tickstyle=bottom,Dx=0.5,Dy=5.0,
                 Ox=0,Oy=0,subticks=2]{->}(0.0,0.0)(0.0,0.0)(3.4,34.0){2in}{2in}%
        \dataplot[plotstyle=dots,showpoints=true]{\dataSLWa}
        \dataplot[plotstyle=dots,dotstyle=Bsquare,showpoints=true]{\dataSLWb}
        \dataplot[plotstyle=dots,dotstyle=Btriangle,showpoints=true]{\dataSLWc}
      %legend
        \psdot[showpoints=true](0.25,31)
        \rput[l](0.36,31){Iris-setosa ($c_1$)}
        \psdot[dotstyle=Bsquare,showpoints=true](0.25,28)
        \rput[l](0.36,28){Iris-versicolor ($c_2$)}
        \psdot[dotstyle=Btriangle,showpoints=true](0.25,25)
        \rput[l](0.36,25){Iris-virginica ($c_3$)}
      % best rules
      \psset{dotscale=2}
        \psdot[fillcolor=white](3.0,33.33)
        \psdot[dotstyle=Bsquare,fillcolor=white](2.93,15.00)
        \psdot[dotstyle=Btriangle,fillcolor=white](3.0,24.67)
        \endpsgraph
        }}}
\end{figure}

For each class we select the most specific (i.e., with maximal antecedent) rule with the highest relative support and then confidence, and also those with the highest conviction and then lift.
\end{frame}


\begin{frame}{Iris Data: Best Class-specific Rules}
\begin{center}
\begin{tabular}{|l|r|r|r|r|}
  \multicolumn{5}{c}{Best Rules by Support and Confidence}\\
\hline
Rule & $\rsupp$ & $\conff$ & $\lift$ & $\conviction$ \\ \hline \hline
$\{pl_1, pw_1\} \arule c_1$ & 0.333 & 1.00 & 3.00 & $\infty$ \\ \hline
$pw_2 \arule c_2$           & 0.327 & 0.91 & 2.72 & 6.00 \\ \hline
$pl_3 \arule c_3$           & 0.327 & 0.89 & 2.67 & 5.24 \\ \hline
\end{tabular}%}{}
\end{center}
\vspace{0.15in}
\begin{center}
\begin{tabular}{|l|r|r|r|r|}
  \multicolumn{5}{c}{Best Rules by Lift and Conviction}\\
\hline
Rule & $\rsupp$ & $\conff$ & $\lift$ & $\conviction$ \\ \hline \hline
$\{pl_1, pw_1\} \arule c_1$ & 0.33 & 1.00 & 3.00 & $\infty$ \\ \hline
$\{pl_2, pw_2\} \arule c_2$ & 0.29 & 0.98 & 2.93 & 15.00 \\ \hline
$\{sl_3, pl_3, pw_3\} \arule c_3$ & 0.25 & 1.00 & 3.00 & $\infty$ \\ \hline
\end{tabular}%}{}
\end{center}

Comparing the rules for each criterion, we verify that the best rule for $c_1$ is the same, but the comparison between rules for $c_2$ and $c_3$ suggests a trade-off between support and novelty, represented by lift and conviction.
\end{frame}

\ifdefined\wox \begin{frame} \titlepage \end{frame} \fi

\begin{frame}{Pattern Assessment Measures: Support and Lift}

  {\bf Support:} The most basic measures are support and relative
  support, giving the number and fraction of transactions in $\bD$ that
  contain the itemset $X$:
\begin{align*}
  \supp(X) & = \card{\vT(X)} &
  \rsupp(X) & = \frac{\supp(X)}{\card{\bD}}
\end{align*}

\medskip
{\bf Lift:} The {\em lift} of a $k$-itemset
$X=\{x_1,x_2,\ldots,x_k\}$ is def\/{i}ned as
\begin{align*}
%\tcbhighmath{
  \lift(X, \bD) &= \frac{P(X)}{\prod_{i=1}^k P(x_i)}
  = \frac{\rsupp(X)}{\prod_{i=1}^k \rsupp(x_i)}
%}
\end{align*}

\medskip
{\bf Generalized Lift:} 
Assume that
$\{X_1,X_2,\ldots,X_q\}$ is a $q$-partition of $X$, i.e., a
partitioning of $X$ into $q$ nonempty and disjoint itemsets $X_i$.
Def\/{i}ne
the generalized lift of $X$ over partitions of size $q$ as
follows:
\begin{align*}
%\tcbhighmath{
  \lift_q(X)  & = \min_{X_1,\ldots,X_q}
  \biggl\{ \frac{P(X)}{\prod_{i=1}^q P(X_i)} \biggr\}
%}
\end{align*}
This is, the least value of lift over all $q$-partitions
$X$.
\end{frame}


\begin{frame}{Pattern Assessment Measures: Rule-based Measures}
Let $\Theta$ be some rule assessment measure.
We
generate all possible rules from $X$ of the form
$X_1 \arule X_2$ and $X_2 \arule X_1$, where
the set $\{X_1,X_2\}$ is a 2-partition, or a bipartition, of $X$.

\medskip
We then compute the measure $\Theta$ for each such rule, and
use summary statistics such as the mean, maximum, and minimum to characterize $X$.

\medskip
For example, if $\Theta$ is rule lift, then we can def\/{i}ne the average,
maximum, and minimum lift values for $X$ as follows:
\begin{align*}
  \mathit{AvgLift(X)} & =
  \mathop{\avg}_{X_1,X_2} \Bigl\{ \lift(X_1\arule X_2) \Bigr\}\\
  \mathit{MaxLift(X)} &
  = \max_{X_1,X_2} \Bigl\{ \lift(X_1\arule X_2) \Bigr\}\\
  \mathit{MinLift(X)} &
  = \min_{X_1,X_2} \Bigl\{ \lift(X_1\arule X_2) \Bigr\}
\end{align*}
\end{frame}


\begin{frame}{Iris Data: Support Values for $\{pl_2, pw_2, c_2\}$ and
  its Subsets}

Consider the support and relative support of itemset $X=\{pl_2, pw_2, c_2\}$ and its subsets.
\begin{center}
\begin{tabular}{|l|c|l|}
\hline
Itemset & $\supp$ & $\rsupp$ \\ \hline
$\{pl_2, pw_2, c_2\}$ &  44 & 0.293\\ \hline\hline
$\{pl_2, pw_2\}$ &  45 & 0.300\\ \hline
$\{pl_2, c_2\}$ &  44 & 0.293\\ \hline
$\{pw_2, c_2\}$ &  49 & 0.327\\ \hline
$\{pl_2\}$ &  45 & 0.300\\ \hline
$\{pw_2\}$ &  54 & 0.360\\ \hline
$\{c_2\}$ &  50 & 0.333\\ \hline
\end{tabular}%}{}\vspace*{-3pt}
\end{center}

\[
lift(X) = \frac{rsup(X)}{rsup(pl_2)rsup(pw_2)rsup(c_2)}=\frac{0.293}{0.3*0.36*0.333}=8.16
\]

\end{frame}


\begin{frame}{Rules Generated from $X=\{pl_2, pw_2, c_2\}$}

Consider all rules that may be generated from $X$:
\begin{center}
\begin{tabular}{|c|l|l|l|l|}
\hline
Bipartition & Rule & $\lift$ &
$\leverage$ & $\conff$\\\hline\hline
\multirow{2}{*}{$\Bigl\{\{pl_2\}, \{pw_2, c_2\}\Bigr\}$} &
$pl_2 \arule \{pw_2, c_2\}$ & 2.993 & 0.195 & 0.978\\ \cline{2-5}
& $\{pw_2, c_2\} \arule pl_2$ & 2.993 &0.195 & 0.898  \\ \hline
\multirow{2}{*}{$\Bigl\{\{pw_2\}, \{pl_2, c_2\}\Bigr\}$} &
$pw_2 \arule \{ pl_2, c_2\}$ & 2.778 & 0.188 & 0.815 \\ \cline{2-5}
& $\{pl_2, c_2\} \arule pw_2$ & 2.778 &0.188 & 1.000 \\ \hline
\multirow{2}{*}{$\Bigl\{\{c_2\}, \{pl_2, pw_2\}\Bigr\}$} &
$c_2\arule \{pl_2, pw_2\}$ & 2.933 &0.193 & 0.880\\ \cline{2-5}
& $\{pl_2, pw_2\} \arule c_2$ & 2.933 &0.193 & 0.978\\ \hline
\end{tabular}%}{}\vspace*{-3pt}
\end{center}

We may then calculate $AvgLift(X)$:

\medskip

$AvgLift(X) = avg\{2.993,2.778,2.933\} = 2.901$

\medskip

And also $AvgConf(X)$: 	

\medskip

$AvgConf(X) = avg\{0.978,0.898,0.815,1.0,0.88,0.978\} = 0.925$

\end{frame}


\begin{frame}[fragile]{Iris: Relative Support and Average Lift of Patterns} 
\framesubtitle{306 frequent itemsets with $minsup=1$ and $k\geq2$}
\readdata{\dataSLW}{FPM/fpmeval/figs/irisliftsupp.dat}
\begin{figure}%[!t]\vspace*{16pt}
        \centering
	\scalebox{0.75}{
        \psset{dotstyle=Bo,dotscale=1.5,fillcolor=lightgray,
              arrowscale=2,PointName=none}
        \psset{xAxisLabel=$\rsupp$, yAxisLabel= $\mathit{AvgLift}$,
        xAxisLabelPos={c,-0.4in}}%, yAxisLabelPos={-0.5in,c}}
        \psgraph[tickstyle=bottom,Dx=0.05,Dy=1.0,
                 Ox=0,Oy=0,subticks=2]{->}(0.0,0.0)(0.0,0.0)(0.36,7.5){3.5in}{2.5in}%
        \dataplot[plotstyle=dots,showpoints=true]{\dataSLW}
        \psline[linestyle=dashed](0,2.5)(0.35,2.5)
        \psline[linestyle=dashed](0.1,0)(0.1,7)
        \endpsgraph
	}
\end{figure}

\bigskip

	For sake of analysis, we focus on patterns with high $rsup$ and then high $AvgLift$, such as $X=\{pl_1,pw_1,c_1\}$.
\end{frame}


\begin{frame}{Comparing Itemsets: Maximal Itemsets}
An frequent itemset $X$ is {\em maximal}
\index{itemsets! maximal}
if all of its supersets are not frequent, that is, $X$ is maximal iff
\begin{align*}
  \supp(X) \ge \minsup, \text{ and for all } Y \supset X, \supp(Y) < \minsup
\end{align*}

Given a collection of frequent itemsets, we may choose to retain only
the maximal ones, especially among those that already satisfy some other
constraints on pattern assessment measures like lift or leverage.
\end{frame}


\begin{frame}{Iris: Maximal Patterns for Average~Lift}

We focus on the 37 class-specific itemsets that present $rsup>0.1$ and $AvgLift>2.5$ and select the maximal ones:

\begin{center}
\begin{tabular}{|l|c|}
\hline
Pattern & Avg.\ lift\\ \hline
$\{sl_1, sw_2, pl_1, pw_1, c_1\}$ & 2.90 \\ \hline
$\{sl_1, sw_3, pl_1, pw_1, c_1\}$ & 2.86 \\ \hline
$\{sl_2, sw_1, pl_2, pw_2, c_2\}$ & 2.83 \\ \hline
$\{sl_3, sw_2, pl_3, pw_3, c_3\}$ & 2.88 \\ \hline
$\{sw_1, pl_3, pw_3, c_3\}$ & 2.52 \\ \hline
\end{tabular}%}{}
\end{center}

For instance, for $c_1$, the essential items are $sl_1$, $pl_1$, $pw_1$ and either $sw_2$ or $sw_3$.
\end{frame}


\begin{frame}{Closed Itemsets and Minimal Generators}
An itemset $X$ is {\em closed}
if all of its supersets have strictly
less support, that is,
\begin{align*}
  \supp(X) > \supp(Y), \text{ for all } Y \supset X
\end{align*}

\medskip
An itemset $X$ is a {\em minimal generator}
if all its subsets
have strictly
higher support, that is,
\begin{align*}
  \supp(X) < \supp(Y), \text{ for all } Y \subset X
\end{align*}

\medskip
If an itemset $X$ is not a minimal generator, then
it implies that it has some redundant items, that is,
we can f\/{i}nd some subset $Y \subset X$, which can be replaced
with an
even smaller subset $W \subset Y$ without
changing the support of $X$, that is, there exists a $W \subset Y$, such
that
$$\supp(X) =
\supp(Y \cup (X\setminus Y)) =
\supp(W \cup (X \setminus Y))$$

\medskip
One can show that all subsets of a minimal generator must themselves be
minimal generators.
\end{frame}

\begin{frame}{Closed Itemsets and Minimal Generators}

The support of an itemset $X$ is

\begin{itemize}
\item the maximum support among all closed itemsets that contain $X$.

\item the minimum support among all minimal generators that are subsets of $X$.
\end{itemize}

\begin{center}
\begin{tabular}{|c|l|l|}
\hline
$\supp$ & Closed Itemset & Minimal Generators \\ \hline
3 & $\mathit{ABDE}$ & $\mathit{AD}$, $\mathit{DE}$\\
3 & $\mathit{BCE}$ & $\mathit{CE}$\\
4 & $\mathit{ABE}$ & $A$\\
4 & $\mathit{BC}$ & $C$\\
4 & $\mathit{BD}$ & $D$\\
5 & $\mathit{BE}$ & $E$\\
6 & $B$ & $B$\\ \hline
\end{tabular}%}{}
\end{center}

Consider itemset $AE$:
\medskip

	$sup(AE) = max\{sup(ABE),sup(ABDE)\} = 4$

	\medskip

	$sup(AE) = min\{sup(A),sup(E)\} = 4$

\end{frame}

\ifdefined\wox \begin{frame} \titlepage \end{frame} \fi

\begin{frame}{Comparing Itemsets: Productive Itemsets}
An itemset $X$ is {\em productive}
if its
relative support is higher than the expected
relative support over all of its bipartitions,
assuming they are independent.

More formally, let $\card{X} \ge 2$, and let
$\{X_1,X_2\}$ be a bipartition of $X$.
We say that $X$ is productive provided
\begin{align*}
\tcbhighmath{
\rsupp(X) > \rsupp(X_1) \times \rsupp(X_2),
\text{ for all bipartitions } \{X_1, X_2\} \text{ of } X
}
\end{align*}

\bigskip
This immediately implies that
$X$ is productive if its minimum lift is greater than one, as
\begin{align*}
  \mathit{MinLift}(X) & =
  \min_{X_1,X_2} \biggl\{ \frac{\rsupp(X)}{\rsupp(X_1) \cdot
  \rsupp(X_2)} \biggr\}
  > 1
\end{align*}
In terms of leverage, $X$ is productive if its minimum leverage is above
zero because
\begin{align*}
MinLeverage(X) =
\min_{X_1,X_2} \Bigl\{
\rsupp(X) - \rsupp(X_1) \times \rsupp(X_2)
\Bigr\} > 0
\end{align*}
\end{frame}

\begin{frame}{Comparing Itemsets: Productive Itemsets}

	$ABDE$ is not productive because there is at least a bipartition with $lift=1$. For instance, the bipartition $\{B,ADE\}$:
\begin{align*}
\lift(B \arule \mathit{ADE}) & =
    \frac{\rsupp(\mathit{ABDE})}{\rsupp(B) \cdot \rsupp(\mathit{ADE})}
    = \frac{3/6}{6/6 \cdot 3/6} = 1
\end{align*}

$ADE$, on the other hand, is productive:

\begin{align*}
\lift(A \arule DE) & =
    \frac{\rsupp(\mathit{ADE})}{\rsupp(A) \cdot \rsupp(\mathit{DE})}
    = \frac{3/6}{4/6 \cdot 3/6} = 1.5\\[6pt]
    \lift(D \arule \mathit{AE}) & =
    \frac{\rsupp(\mathit{ADE})}{\rsupp(D) \cdot \rsupp(\mathit{AE})}
    = \frac{3/6}{4/6 \cdot 4/6} = 1.125\\[6pt]
    \lift(E \arule \mathit{AD}) & =
    \frac{\rsupp(\mathit{ADE})}{\rsupp(E) \cdot \rsupp(\mathit{AD})}
    = \frac{3/6}{5/6 \cdot 3/6} = 1.2
\end{align*}

\end{frame}


\begin{frame}{Comparing Rules}
Given two rules $R: X \arule Y$ and $R': W \arule Y$ that have the same
consequent, we say that $R$ is {\em more specif\/{i}c} than $R'$, or
equivalently, that $R'$ is {\em more general} than $R$
provided $W \subset X$.

\medskip
{\bf Nonredundant Rules:} We say that a rule $R: X \arule Y$
is {\em redundant} provided there exists a more general rule
$R': W \arule Y$ that has the same support, that is, $W\subset X$ and $\supp(R) = \supp(R')$.

\medskip
{\bf Improvement and Productive Rules:}
Def\/{i}ne the {\em improvement}
of
a rule $X \arule Y$ as follows:
\begin{align*}
\tcbhighmath{
\textit{imp}(X \arule Y) = \conff (X \arule Y)
- \max_{W \subset X} \Bigl\{\conff (W \arule Y) \Bigr\}
}
\end{align*}

\medskip
A rule $R: X \arule Y$ is {\em productive} if
its improvement is greater than zero, which implies that
for all more general rules $R': W \arule
Y$ we have  $\conff(R) > \conff(R')$. 
\end{frame}

\begin{frame}{Comparing Rules}
Consider
rule $R: \mathit{BE} \arule C$, which has support $3$, and conf\/{i}dence $3/5
= 0.60$.
%lift $0.9$, and leverage $-0.06$.

It has two generalizations, namely
\begin{alignat*}{2}
R'_1: E \arule C,& \quad \supp= 3, \conff = 3/5=0.6\\[-3pt]
R'_2: B \arule C,& \quad \supp= 4, \conff = 4/6 =0.67
\end{alignat*}
Thus, $\mathit{BE} \arule C$ is redundant w.r.t. $E \arule C$ because they
have the same support, that is, $\supp(\mathit{BCE}) = \supp(\mathit{BC})$. 

\medskip

$\mathit{BE} \arule C$ is also unproductive,
since 

\medskip


$imp(\mathit{BE} \arule C) = 0.6 - \max\{0.6, 0.67\} = -0.07$. 

\medskip


It has a
more general rule, namely $R'_2$, with higher conf\/{i}dence.


\end{frame}


\begin{frame}{F{i}sher Exact Test for Productive Rules}
Let $R: X\arule Y$ be an association rule. Consider its
generalization $R': W \arule Y$, where $W = X\setminus Z$ is the
new antecedent formed by removing from $X$ the subset $Z \subseteq
X$. 

\medskip
Given an input dataset $\bD$, conditional on the fact that $W$
occurs, we can create a $2 \times 2$ contingency table between $Z$
and the consequent $Y$
\begin{center}
\begin{tabular}{|c|cc||c|}
    \hline
    $W$ & $Y$ & $\neg Y$ & \\
    \hline
    $Z$ & $a$ & $b$ & $a+b$\\
  $\neg Z$ & $c$ & $d$ & $c+d$\\
  \hline\hline
  & $a+c$ & $b+d$ & $n=\supp(W)$\\
  \hline
  \end{tabular}
\end{center}
where
\begin{alignat*}{3}
  a & = \supp(\mathit{WZY}) = \supp(\mathit{XY}) & \qquad\qquad
  b & = \supp(\mathit{WZ}\neg Y) = \supp(X\neg Y) \\[-3pt]
  c & = \supp(W \neg \mathit{ZY}) & \qquad\qquad
  d & =\supp(W \neg Z \neg Y)
\end{alignat*}
\end{frame}


\begin{frame}{F{i}sher Exact Test for Productive Rules}
Given a contingency table conditional on $W$,
we are interested in the odds ratio obtained by
comparing the presence and absence of $Z$, that is,
\begin{align*}
  oddsratio & = \frac{a/(a+b)}{b/(a+b)} \Biggl/ \frac{c/(c+d)}{d/(c+d)} =
  \frac{ad}{bc}
\end{align*}

Under the null hypothesis $H_0$ that
$Z$ and $Y$ are independent given $W$ the odds ratio is 1. 
If we further assume that the row and column marginals are
f\/{i}xed, then $a$ uniquely determines the other three values $b$, $c$, and
$d$, and the probability mass function of observing the value $a$ in the
contingency table is given by the hypergeometric distribution.  

\begin{align*}
  P\Bigl(a \bigl|\; (a+c), (a+b), n\Bigr) &=
  \frac{(a+b)!\;(c+d)!\;(a+c)!\;(b+d)!}{n!\;a!\;b!\;c!\;d!}
\end{align*}
\end{frame}



\begin{frame}{F{i}sher Exact Test: P-value}

Our aim is to contrast the null hypothesis $H_0$ that $oddsratio =
1$ with the alternative hypothesis $H_a$ that $oddsratio > 1$.

The $\pvalue$ for $a$ is given as
\begin{align*}
  \pvalue(a) & = \sum_{i=0}^{\min(b,c)} P(a+i \mid (a+c),(a+b),n)\\[6pt]
  & =
\tcbhighmath{
  \sum_{i=0}^{\min(b,c)}
  \frac{(a+b)!\;(c+d)!\;(a+c)!\;(b+d)!}{n!\;(a+i)!\;(b-i)!\;(c-i)!\;(d+i)!}
}
\end{align*}
which follows from the fact that when we increase the count of $a$
by $i$, then because the row and column marginals are f\/{i}xed, $b$
and $c$ must decrease by $i$, and $d$ must increase by $i$, as
shown in the table below:
\begin{center} 
\begin{tabular}{|c|cc||c|}
    \hline
    $W$ & $Y$ & $\neg Y$ & \\
    \hline
    $Z$ & $a+i$ & $b-i$ & $a+b$\\
  $\neg Z$ & $c-i$ & $d+i$ & $c+d$\\
  \hline\hline
  & $a+c$ & $b+d$ & $n=\supp(W)$\\
  \hline
  \end{tabular}%}{}
\end{center}
\end{frame}


\begin{frame}{Fisher Exact Test: Example}
  \small
  Consider the rule $R: pw_2 \arule c_2$ obtained from the discretized Iris
  dataset. To test if it is productive, because there is only a single
  item in the antecedent, we compare it only with the default rule
  $\emptyset \arule c_2$. We have
  \begin{align*}
    a & = \supp(pw_2,c_2) = 49 &
    b & = \supp(pw_2,\neg c_2) = 5\\
    c & = \supp(\neg pw_2,c_2) = 1 &
    d & = \supp(\neg pw_2, \neg c_2) = 95
  \end{align*}
  with the contingency table given as

  \begin{center}
{\tabcolsep12pt\renewcommand{\arraystretch}{1.1}  \begin{tabular}{|c|cc|c|}
    \hline
  & $c_2$ & $\neg c_2$ & \\ \hline
  $pw_2$ & 49 & 5 & 54\\
  $\neg pw_2$ & 1 & 95 & 96\\ \hline
  & 50 & 100 & 150\\ \hline
  \end{tabular}}
\end{center}

  Thus the $\pvalue$ is given as
    $\pvalue  = \sum_{i=0}^{\min(b,c)} P(a+i \mid (a+c),(a+b),n) = 
    1.51 \times 10^{-32}$

	\medskip
  Since the $\pvalue$ is extremely small, we can safely reject the null
  \hbox{hypothesis} that the odds ratio is 1. Instead, there is a strong
  relationship between $X=pw_2$ and $Y=c_2$, and we conclude
  that $R:pw_2\arule c_2$
  is a productive rule.
\end{frame}

\begin{frame}{Fisher Exact Test: Example}

  Consider another rule $\{sw_1, pw_2\} \arule c_2$, with
  $X=\{sw_1,pw_2\}$ and $Y=c_2$. Consider its three
  generalizations, and the corresponding contingency tables and
  p-values:
  \begin{center}\vspace*{10pt}
    \begin{tabular}{p{2in}c}
      \begin{tabular}{l}
      $R'_1: pw_2 \arule c_2$\\ \hline
      $Z=\{sw_1\}$ \\
      $W=X\setminus Z = \{pw_2\}$ \\
      $\pvalue=0.84$
      \end{tabular} &
{  \tabcolsep12pt\renewcommand{\arraystretch}{1.1}    \begin{tabular}{|c|cc|c|} \hline
      $W= pw_2$ & $c_2$ & $\neg c_2$ & \\ \hline
      $sw_1$ & 34 & 4 & 38\\
      $\neg sw_1$ & 15 & 1 & 16\\ \hline
       & 49 & 5 & 54\\ \hline
      \end{tabular}}\\
      %
      & \\
      \begin{tabular}{l}
      $R'_2: sw_1 \arule c_2$\\ \hline
      $Z=\{pw_2\}$\\
      $W=X\setminus Z=\{sw_1\}$\\
      $\pvalue=1.39\times10^{-11}$
      \end{tabular} &
{ \tabcolsep12pt\renewcommand{\arraystretch}{1.1}     \begin{tabular}{|c|cc|c|} \hline
      $W= sw_1$ & $c_2$ & $\neg c_2$ & \\ \hline
      $pw_2$ & 34 & 4 & 38\\
      $\neg pw_2$ & 0 & 19 & 19\\ \hline
       & 34 & 23 & 57\\ \hline
      \end{tabular}}\\
       & \\
    \end{tabular}
  \end{center}
\end{frame}

\begin{frame}{Fisher Exact Test: Example}

  \begin{center}
    \hspace*{-1cm}
    \begin{tabular}{p{1.5in}c}
      %
      \begin{tabular}{l}
      $R'_3: \emptyset \arule c_2$\\ \hline
      $Z=\{sw_1,pw_2\}$\\
      $W=X\setminus Z=\emptyset$\\
      $\pvalue=3.55\times10^{-17}$
      \end{tabular} &
{ \tabcolsep12pt\renewcommand{\arraystretch}{1.1}     \begin{tabular}{|c|cc|c|} \hline
      $W= \emptyset$ & $c_2$ & $\neg c_2$ & \\ \hline
      $\{sw_1,pw_2\}$ & 34 & 4 & 38\\
      $\neg\{sw_1,pw_2\}$ & 16 & 96 & 112\\ \hline
       & 50 & 100 & 150\\ \hline
      \end{tabular}}
    \end{tabular}
  \end{center}

  We can see that whereas the $\pvalue$ with respect to $R'_2$ and
  $R'_3$ is small, for $R'_1$ we have $\pvalue=0.84$, which is too
  high and thus we cannot reject the null hypothesis. We
  conclude that $R:\{sw_1, pw_2\} \arule c_2$ is not productive. In
  fact, its generalization $R'_1$ is the one that is productive.

\end{frame}

\begin{frame}{Multiple Hypothesis Testing}

There can be an exponentially large number of rules that need to
be tested to check whether they are productive or not. 

\medskip


\textbf{Multiple hypothesis testing problem:} The sheer number
of hypothesis tests leads to some unproductive rules passing the $\pvalue \le
\alpha$ threshold by random chance.

\medskip


\textbf{Bonferroni correction:} takes into account the number of experiments
performed during the hypothesis testing process.

\medskip

$$\alpha' = \frac{\alpha}{\#r}$$

where $\#r$ is the number of rules to be tested or its estimate.

\medskip

The rule false discovery rate becomes bounded by $\alpha$, where a false discovery is to claim that a rule
is productive when it is not.
\end{frame}

\begin{frame}{Multiple Hypothesis Testing}

  Given the class-specif\/{i}c rules of discretized Iris dataset,
  the maximum number of class-specif\/{i}c rules 
  is given as
\begin{align*}
  \#r = c \times \lB(\sum_{i=1}^4 {4\choose i} b^i\rB)
\end{align*}

where $c$ is the number of Iris classes, $b$
is the maximum number of bins for any other attribute,
$i$ is the antecedent size,  and 
there are $b^i$ possible
combinations for the chosen set of $i$ attributes. 

\medskip

Since $c=3$ and $b=3$, the number of possible
rules is:
\begin{align*}
\#r= 3 \times \lB(\sum_{i=1}^4 {4\choose i} 3^i\rB) = 3(12+54+108+81)=
3\cdot 255= 765
\end{align*}

Given $\alpha=0.01$, $\alpha'=\alpha/\#r = 0.01/765 = 1.31\times10^{-5}$.

\medskip

The rule $pw_2 \arule c_2$ 
has $\pvalue=1.51 \times 10^{-32}$, and thus it remains productive even
when we use $\alpha'$.

\end{frame}

\ifdefined\wox \begin{frame} \titlepage \end{frame} \fi

\begin{frame}{Permutation Test for Significance: Swap Randomization}
A {\em permutation} or {\em randomization} test determines
the distribution of a given test statistic $\Theta$ by randomly
modifying the observed data several times to obtain a random
sample of datasets, which can in turn be used for signif\/{i}cance
testing. 


\medskip
The {\em swap randomization} approach
maintains as invariant the column and row margins for a given dataset,
that is, the permuted datasets preserve the support of each item (the
column margin) as well as the number of items in each transaction (the
row margin).  

\medskip
Given a dataset $\bD$, we randomly create $k$
datasets that have the same row and column margins. We then mine
frequent patterns in $\bD$ and check whether the pattern statistics are different
from those obtained using the randomized datasets. If the differences
are not signif\/{i}cant, we may conclude that the patterns arise solely from
the row and column margins, and not from any interesting properties of
the data.
\end{frame}


\begin{frame}{Swap Randomization}
Given a binary matrix $\bD \subseteq \cT \times \cI$, the swap randomization method exchanges two
nonzero cells of the matrix via a {\em swap} that leaves the row and
column margins unchanged.

\medskip
Consider any two transactions $t_a, t_b
\in \cT$ and any two items $i_a, i_b \in \cI$ such that
$(t_a,i_a), (t_b,i_b) \in \bD$ and $(t_a,i_b), (t_b, i_a) \not\in \bD$,
which corresponds to the
$2 \times 2$ submatrix in $\bD$, given as
$$\bD(t_a,i_a; t_b,i_b)=
\matr{1&0\\ 0&1}$$

\medskip
After a swap operation we obtain the new
  submatrix
$$\bD(t_a,i_b; t_b,i_a)= \matr{0&1\\ 1&0}$$
where we exchange the elements in $\bD$ so that $(t_a, i_b), (t_b,
i_a) \in \bD$, and $(t_a, i_a), (t_b, i_b) \not\in \bD$. We denote
this operation as $Swap(t_a,i_a; t_b, i_b)$. 
\end{frame}


\newcommand{\algswap}{\textsc{SwapRandomization}}
\begin{frame}[fragile]{Algorithm \algswap}
\begin{tightalgo}[H]{\textwidth-18pt}
\SetKwInOut{Algorithm}{\algswap ($t$, $\bD \subseteq \cT \times \cI$)}
\Algorithm{}
\While{$t > 0$}{
  Select pairs $(t_a,i_a), (t_b,i_b) \in \bD$ randomly\;
  \If{$(t_a,i_b) \not\in \bD$ and $(t_b,i_a)\not\in \bD$}{
    $\bD \gets \bD \;\setminus\; \bigl\{ (t_a,i_a), (t_b,i_b)\bigr\}
        \;\cup\; \bigl\{ (t_a,i_b), (t_b,i_a) \bigr\}$\;
  }
  $t = t-1$\;
}
\Return{$\bD$}
\end{tightalgo}
\end{frame}


\begin{frame}[fragile]{Swap Randomization Example}
\setcounter{subfigure}{0}
\begin{table}[H]
\scalebox{0.8}{
{\hspace*{-8pt}\begin{tabular}{c}
  \centerline{
{
\label{tab:fpm:fpmeval:simplemargins}
\centering
\begin{tabular}{|c||c|c|c|c|c||r|}
\hline
\multirow{2}{*}{Tid} & \multicolumn{5}{c||}{Items} &
\multirow{2}{*}{Sum}\\ \cline{2-6}
& $A$ & $B$ & $C$ & $D$ & $E$ & \\\hline\hline
1 & 1&1&0&1&1& 4 \\ \hline
2 & 0&1&1&0&1& 3\\ \hline
3 & 1&1&0&1&1& 4\\ \hline
4 & 1&1&1&0&1& 4\\ \hline
5 & 1&1&1&1&1& 5\\ \hline
6 & 0&1&1&1&0 & 3\\ \hline \hline
Sum&4&6&4&4&5& \\ \hline
\multicolumn{7}{c}{\fontsize{8}{8}\selectfont(a) Input binary data $\bD$}\\
\end{tabular}
}}\\[-6pt]\\
\vspace{0.1in}
\centerline{
{
\label{tab:fpm:fpmeval:swapsimple}
\centering
\begin{tabular}{|c||c|c|c|c|c||c|}
\hline
\multirow{2}{*}{Tid} & \multicolumn{5}{c||}{Items} &
\multirow{2}{*}{Sum}\\ \cline{2-6}
& $A$ & $B$ & $C$ & $D$ & $E$ & \\\hline\hline
1 & 1&1&\cellcolor{gray!50}1&\cellcolor{gray!50}0&1& 4 \\ \hline
2 & 0&1&1&0&1& 3\\ \hline
3 & 1&1&0&1&1& 4\\ \hline
4 & 1&1&\cellcolor{gray!50}0&\cellcolor{gray!50}1&1& 4\\ \hline
5 & 1&1&1&1&1& 5\\ \hline
6 & 0&1&1&1&0 & 3\\ \hline \hline
Sum&4&6&4&4&5& \\ \hline
\multicolumn{7}{c}{\fontsize{8}{8}\selectfont(b) $Swap(1,D; 4,C)$}\\
\end{tabular}
} \hspace{0.2in}
{
\label{tab:fpm:fpmeval:swapsimple2}
\centering
\begin{tabular}{|c||c|c|c|c|c||c|}
\hline
\multirow{2}{*}{Tid} & \multicolumn{5}{c||}{Items} &
\multirow{2}{*}{Sum}\\ \cline{2-6}
& $A$ & $B$ & $C$ & $D$ & $E$ & \\ \hline\hline
1 & 1&1& 1& 0 & 1& 4 \\ \hline
2 & \cellcolor{gray!50}1&1&\cellcolor{gray!50}0 &0 &1& 3\\ \hline
3 & 1&1& 0&1&1& 4\\ \hline
4 & \cellcolor{gray!50}0 &1& \cellcolor{gray!50}1& 1&1& 4\\ \hline
5 & 1&1&1&1&1& 5\\ \hline
6 & 0 &1&1&1&0 & 3\\ \hline \hline
Sum&4&6&4&4&5& \\ \hline
\multicolumn{7}{c}{\fontsize{8}{8}\selectfont(c) $Swap(2,C; 4,A)$}\\
\end{tabular}
}
}
\end{tabular}}{}
}
\end{table}
\end{frame}


\begin{frame}[fragile]{Swap Randomization Example}

We generated $k=100$ swap randomized datasets (150 swaps).

\medskip

Let the test statistic be the total number of
frequent itemsets using $\minsup=3$. 

For $\bD$, we have $\card{\cF} = 19$, and for the
$k=100$ permuted datasets we find:
\begin{align*}
  P\bigl(\card{\cF}=19\bigr) & = 0.67 & P\bigl(\card{\cF} = 17\bigr) = 0.33
\end{align*}


Because $\pvalue(19)=0.67$, we may conclude
that the set of frequent itemsets is essentially
determined by the row and column marginals.

\medskip

	Consider $\mathit{ABDE}$, where
$\supp(\mathit{ABDE})=3$ and the probability
that $\mathit{ABDE}$ is frequent is $17/100 = 0.17$. 
As this probability is not very low,
$\mathit{ABDE}$ is not a statistically signif\/{i}cant pattern.

\medskip

Consider $\mathit{BCD}$, where
$\supp(\mathit{BCD})=2$. The empirical PMF is given as
\begin{align*}
  P(sup=2) & = 0.54 & P(sup = 3) & = 0.44 & P(sup=4) = 0.02
\end{align*}

	Since 54\% indicates $\mathit{BCD}$ is infrequent, we may assume it.

\end{frame}



\readdata{\dISC}{FPM/fpmeval/figs/iris-supp-1.cdf}
\begin{frame}[fragile]{CDF for Number of Frequent Itemsets: Iris}
\begin{center}
\begin{figure}[H]
	\scalebox{0.85}{
        \psset{xAxisLabel=$\minsup$,yAxisLabel=$\hF$,
        xAxisLabelPos={c,-0.4in}}%, yAxisLabelPos={-0.6in,c}}
    \psgraph[tickstyle=bottom,Dy=0.25,dy=0.25,
      Dx=10,dx=10]{->}(0,0)(70,1.25){3in}{2in}
      \dataplot[linewidth=1.5pt]{\dISC}
    \psset{dotsep=2pt}
    \psline[linestyle=dotted](9,0)(9,0.517)
    \psline[linestyle=dotted](0,0.517)(9,0.517)
    \endpsgraph
    }
\end{figure}
\end{center}

\vspace*{0.7cm}

We choose
$\minsup=10$, for which we have $\hF(10) = P(sup < 10) = 0.517$,
that is,
48.3\% of the itemsets that occur at least once are frequent.
\end{frame}


\readdata{\dILR}{FPM/fpmeval/figs/iris-liftrel.cdf}
\begin{frame}[fragile]{CDF for Average Relative Lift: Iris}
  \framesubtitle{$k=100$ swap randomization steps, $140$ frequent itemsets}

The relative lift statistic is
\begin{align*}
  \mathit{rlift}(X,\bD,\bD_i) = \frac{\supp(X,\bD) - \supp(X,\bD_i)}{\supp(X,\bD)}
  = 1 -  \frac{\supp(X,\bD_i)}{\supp(X,\bD)}
\end{align*}

$\bD_i$ is $i$th swap randomized dataset obtained after $k$ steps.

\medskip

\begin{center}
\begin{figure}[H]
        %\vspace{0.2in}
\def\pshlabel#1{\small {$#1$}}
\def\psvlabel#1{\small {$#1$}}
\scalebox{0.8}{
        \centering
        \psset{xAxisLabel=Avg.\ Relative Lift,
          yAxisLabel=$\hF$,
          xAxisLabelPos={c,-0.4in}}%, yAxisLabelPos={-0.6in,c}}
    \psgraph[tickstyle=bottom,Dy=0.25,
        dy=0.25,Ox=-0.6,Dx=0.2,dx=0.2]{->}(-0.6,0)(1.2,1.25){3in}{2in}
          \dataplot[linewidth=1.5pt]{\dILR}
            \psset{dotsep=2pt}
    \psline[linestyle=dotted](-0.2,0)(-0.2,0.05)
    \psline[linestyle=dotted](-0.6,0.05)(-0.2,0.05)
    \psline[linestyle=dashed](0.8,0)(0.8,0.56)
    \psline[linestyle=dashed](-0.6,0.56)(0.8,0.56)
    \endpsgraph
	}
        \vspace{0.35in}
\end{figure}
\end{center}
\end{frame}


\readdata{\dPMF}{FPM/fpmeval/figs/iris-11-42.pmf}
\begin{frame}[fragile]{PMF for Relative Lift: $\{{\it sl}_1, {\it pw}_2\}$}
  \framesubtitle{$k=100$ swap randomization steps}

Its average
relative lift value is $-$0.55, and $\pvalue(-0.2) = 0.069$, which
indicates that the itemset is likely to be disassociative.

\medskip

\begin{center}
\begin{figure}[H]
\scalebox{0.85}{
\def\pshlabel#1{\small {$#1$}}
\def\psvlabel#1{\small {$#1$}}
        \centering
        \psset{xAxisLabel=Relative Lift,
          yAxisLabel=$\hf$,
          xAxisLabelPos={c,-0.3in}}%, yAxisLabelPos={-0.6in,c}}
    \psgraph[tickstyle=bottom,Dy=0.04,
        dy=0.04,Ox=-1.2,Dx=0.2,dx=0.2]{->}(-1.2,0)(0.2,0.2){3in}{2in}
          \listplot[plotstyle=LineToXAxis,linewidth=4pt,linecolor=gray]{\dPMF}
    \endpsgraph
    }
\end{figure}
\end{center}
\end{frame}



\begin{frame}{Bootstrap Sampling for Conf\/{i}dence Interval}

  We can generate $k$ bootstrap samples from $\bD$ using
sampling {\em with replacement}.
Given pattern $X$ or rule $R:X\arule Y$,
we can obtain the value of the test statistic in each of the bootstrap
samples; let $\theta_i$ denote the value in sample $\bD_i$.


\medskip
From these values we can generate the empirical cumulative distribution
function for the statistic
\begin{align*}
  \hF(x) = \hP\lB(\Theta \le x\rB) = \frac{1}{k} \sum_{i=1}^k
  I(\theta_i \le x)
\end{align*}
where $I$ is an indicator variable that takes on the value $1$ when its
argument is true, and $0$ otherwise.

\medskip
Given a desired conf\/{i}dence level
$\alpha$ (e.g., $\alpha=0.95$) we can compute
the interval for
the test statistic by discarding values from the tail ends of $\hF$ on both sides
that encompass $(1-\alpha)/2$ of
the probability mass.
In other words, the interval
$\bigl[v_{1-\alpha/2}, v_{\alpha/2}\bigr]$ encompasses $1-\alpha$
fraction of the probability mass, and therefore it is called the
$100(1-\alpha)$\%
conf\/{i}dence interval for the chosen test statistic $\Theta$.

\end{frame}

\newcommand{\algbootstrapCI}{\textsc{Bootstrap-ConfidenceInterval}}
\begin{frame}[fragile]{Bootstrap Confidence Interval Algorithm}
\begin{tightalgo}[H]{\textwidth-18pt}
\SetKwInOut{Algorithm}{\algbootstrapCI ($X$, $\alpha$, $k$, $\bD$)}
\Algorithm{}
\For{$i \in [1, k]$}{
  $\bD_i \assign$ sample of size $n$ with replacement from $\bD$\;
  $\theta_i \gets $ compute test statistic for $X$ on $\bD_i$\;
}
$\hF(x) = P\lB(\Theta \le x\rB) = \frac{1}{k} \sum_{i=1}^k
  I(\theta_i \le x)$\;
  $v_{(1-\alpha)/2} = \hF^{-1}\bigl((1-\alpha)/2\bigr)$\;
  $v_{(1+\alpha)/2} = \hF^{-1}\bigl((1+\alpha)/2\bigr)$\;
\Return{$[v_{(1-\alpha)/2}, v_{(1+\alpha)/2}\bigr]$}
\end{tightalgo}
\end{frame}

\readdata{\dBSPMF}{FPM/fpmeval/figs/epmf-bootstrap.txt}
\begin{frame}[fragile]{Empirical PMF for RelSupport: $X = \{sw_1, pl_3, pw_3, cl_3\}$}


Given $\rsupp(X, \bD) = 0.113$ (or $\supp(X,\bD)=17$) and 
$k=100$ bootstrap samples, we compute the relative support
of $X$ in each of the samples ($\rsupp(X,\bD_i)$).

\medskip

\begin{center}
\def\pshlabel#1{\small {#1}}
\def\psvlabel#1{\small {#1}}
        \psset{xAxisLabel=$\rsupp$,
          yAxisLabel=$\hf$,
          xAxisLabelPos={c,-0.3in}}%
    \psgraph[tickstyle=bottom,Dy=0.04,
        dy=0.04,Ox=0.04,Dx=0.02,dx=0.02]{->}(0.04,0)(0.2,0.2){3in}{2in}
          \listplot[plotstyle=LineToXAxis,linewidth=4pt,
          linecolor=gray]{\dBSPMF}
            \psset{dotsep=2pt}
          \listplot[linewidth=1pt,linestyle=dotted]{\dBSPMF}
    \endpsgraph
\end{center}
\end{frame}

\readdata{\dBS}{FPM/fpmeval/figs/ecdf-bootstrap.txt}
\def\pshlabel#1{\small {#1}}
\def\psvlabel#1{\small {#1}}
\begin{frame}[fragile]{Empirical CDF for RelSupport: $X = \{sw_1, pl_3, pw_3, cl_3\}$}
\begin{columns}
\column{0.55\textwidth}
\hspace*{0.7cm}
	\scalebox{0.8}{
        \centering
        \psset{xAxisLabel=$\rsupp$,
          yAxisLabel=$\hF$,
          xAxisLabelPos={c,-0.4in}}%
    \psgraph[tickstyle=bottom,Dy=0.25,
        dy=0.25,Ox=0.04,Dx=0.02,dx=0.02]{->}(0.04,0)(0.2,1.25){3in}{2in}
          \dataplot[linewidth=1.5pt]{\dBS}
            \psset{dotsep=2pt}
        \pcline[arrowscale=1.5]{->}(0.073,0.25)(0.073,0.0)
        \uput[u](0.073,0.25){$v_{0.05}$}
        \pcline[arrowscale=1.5]{->}(0.16,1.1)(0.16,0.0)
        \uput[u](0.16,1.1){$v_{0.95}$}
    \endpsgraph
    }

\column{0.45\textwidth}

\begin{minipage}{0.7\textwidth}
Let the conf\/{i}dence level be $1-\alpha = 0.9$, thus $\alpha = 0.1$,
discarding the values that account for $\alpha/2=0.05$:
  \begin{align*}
    v_{1-\alpha/2} = v_{0.95} & = 0.073\\
      v_{\alpha/2} = v_{0.05} & = 0.16 
  \end{align*}

\end{minipage}

\end{columns}

\vspace*{1cm}

The 90\% conf\/{i}dence interval for $\rsupp(X) \in [0.073, 0.16]$, i.e.,  $sup \in [11,24]$.
  
\medskip

Note that $\rsupp(X,\bD)=0.113$, with $\pvalue(0.113) = 0.45$, 
and $\mu_{\rsupp(X)} = 0.115$.
\end{frame}
